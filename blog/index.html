<!doctype html>
    <!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
    <!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
    <!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
    <!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

    
      
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Blogs - Rishijeet Mishra</title>
        <meta name="author" content="Rishijeet Mishra">
        
        <meta name="description" content="Bridging tech and education - Rishijeet Mishra's insights on digital learning">
        
        <meta name="viewport" content="width=device-width">
        <meta name="google-site-verification" content="k3jIYcr9jzBS7xC3F_CC0Eqc-szFtcR-JBr1Wwqnk6w" />
        <link rel="canonical" href="https://rishijeet.github.io/blog">

        <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,400italic' rel='stylesheet' type='text/css'>
        <link href="https://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        
        <link href="/favicon.svg" rel="icon">
        <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet">
        <link href="/stylesheets/style.css" rel="stylesheet">
        <link href="" rel="alternate" title="Rishijeet Mishra" type="application/atom+xml">
    </head>


    <body >

        <header id="header">
    <div class="row">
    <div class="col-xs-12 col-sm-8 col-md-4">
        <a href="/" class="site-title">Rishijeet Mishra</a>
    </div>
    <div class="col-xs-12 col-sm-4 col-md-8">
    <nav>
    <input type="checkbox" id="toggle">
    <label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu"></label>
    <ul class="menu">
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/blog/archives/">Archive</a></li>
</ul>

</nav>

    </div>
</div>

</header>


        <div id="main-content">

            

            

            <div class="row top-xs center-sm center-md center-lg site-wrapper">
                
                <div class="col-xs-12 col-sm-10 col-md-10 col-lg-10">
                
                    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<article class="page">

    
    <header>
        <h1 class="page__title">
            Blogs
        </h1>
    </header>
    <hr class="divider">
    

        
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/generative-ai-in-2025-global-trends-breakthroughs-and-future-horizons/">Generative AI in 2025: Global Trends, Breakthroughs and Future Horizons</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-09-04T12:02:41+05:30" pubdate><i class="fa fa-calendar"></i> Sep 4th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/genai/'>genai</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/ml/'>ml</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>Generative AI (GenAI) has transitioned from an experimental technology to a cornerstone of global innovation by 2025,
reshaping industries, economies, and societal norms. This comprehensive overview draws on recent reports, surveys,
and developments to explore the latest happenings in the GenAI space worldwide, while projecting likely future
trajectories.</p>

<p>From surging investments and enterprise adoption to ethical dilemmas and regulatory frameworks,
GenAI&rsquo;s evolution reflects a blend of unprecedented potential and persistent challenges. We&rsquo;ll examine key trends,
regional variations, technological breakthroughs, and forward-looking predictions, incorporating data from authoritative sources like Stanford&rsquo;s AI Index, McKinsey and Gartner.</p>

<p>In 2025, GenAI has seen explosive growth in enterprise adoption, particularly in functions like marketing, product development, and software engineering. Companies are investing heavily, with traffic surging 890% and budgets growing 60% through 2027. Breakthroughs include multimodal AI, where models process text, images, video, and audio, enabling applications in public sectors for better data search and citizen services. However, issues like AI-generated ransomware and deepfakes are rising, prompting global regulatory responses.</p>

<p><img src="/images/2025/gen_ai_usage.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>


        <a class="btn pull-right" href="/blog/generative-ai-in-2025-global-trends-breakthroughs-and-future-horizons/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/quantum-computing-the-next-leap-beyond-classical-machines/">Quantum Computing: The Next Leap Beyond Classical Machines</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-09-01T20:56:37+05:30" pubdate><i class="fa fa-calendar"></i> Sep 1st, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/quantum/'>quantum</a></li>
            
                <li><a class='category' href='/blog/categories/quantum-computing/'>quantum_computing</a></li>
            
                <li><a class='category' href='/blog/categories/quantum-leap/'>quantum_leap</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>For decades, classical computers have been the backbone of innovation, powering everything from banking systems to spacecraft navigation. But as we continue to push the boundaries of science—whether simulating molecules for drug discovery, cracking complex optimization problems, or modeling the cosmos—classical computing starts hitting hard physical and mathematical walls.</p>

<p>This is where <strong>quantum computing</strong> steps in: a paradigm that doesn’t just speed things up, but fundamentally changes <em>how</em> we compute.</p>

<p><img src="/images/2025/qc.jpg" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="What-Exactly-is-Quantum-Computing-3f-"></a>
<h2>What Exactly is Quantum Computing?</h2>

<p>Quantum computing is a computational model that leverages the principles of <strong>quantum mechanics</strong>—the physics governing particles at atomic and subatomic scales. Unlike classical computers that process data in <strong>bits</strong> (0 or 1), quantum computers use <strong>quantum bits (qubits)</strong>, which can exist as:</p>

<ul>
<li><strong>0</strong></li>
<li><strong>1</strong></li>
<li><strong>or both 0 and 1 simultaneously (superposition)</strong></li>
</ul>


<p>This property enables quantum machines to process exponentially more information than classical systems.</p>


        <a class="btn pull-right" href="/blog/quantum-computing-the-next-leap-beyond-classical-machines/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/supercharge-reasoning-in-ai-hands-on-chain-of-thought-builds/">Supercharge Reasoning in AI: Hands-On Chain of Thought Builds</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-29T13:26:07+05:30" pubdate><i class="fa fa-calendar"></i> Aug 29th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/cot/'>cot</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/ml/'>ml</a></li>
            
                <li><a class='category' href='/blog/categories/rag/'>rag</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>Chain of Thought (CoT) is a prompting technique introduced in a 2022 paper by Google researchers (Wei et al., &ldquo;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&rdquo;). The core idea is simple: instead of asking an LLM for a direct answer, you instruct it to <strong>reason step by step</strong>. This elicits better performance on tasks requiring logic, math, commonsense, or multi-step planning.</p>

<p><img src="/images/2025/cot.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>For example:</p>

<ul>
<li><strong>Direct Prompt</strong>: &ldquo;What is 15% of 200?&rdquo;</li>
<li><strong>CoT Prompt</strong>: &ldquo;What is 15% of 200? Let&rsquo;s think step by step.&rdquo;</li>
</ul>


<p>The LLM might respond:</p>

<ul>
<li>&ldquo;Step 1: 15% means 15 per 100, so 15/100 = 0.15.</li>
<li>Step 2: Multiply by 200: 0.15 * 200 = 30. So, the answer is 30.&#8221;</li>
</ul>



        <a class="btn pull-right" href="/blog/supercharge-reasoning-in-ai-hands-on-chain-of-thought-builds/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/understanding-react-in-large-language-models/">Understanding ReAct in Large Language Models</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-28T08:48:16+05:30" pubdate><i class="fa fa-calendar"></i> Aug 28th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/mcp/'>mcp</a></li>
            
                <li><a class='category' href='/blog/categories/a2a/'>a2a</a></li>
            
                <li><a class='category' href='/blog/categories/rag/'>rag</a></li>
            
                <li><a class='category' href='/blog/categories/cot/'>cot</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>ReAct, short for Reasoning and Acting, is a paradigm for enhancing large language models (LLMs) by integrating verbal reasoning traces with task-specific actions. Introduced in a 2022 paper, it addresses limitations in chain-of-thought (CoT) prompting by allowing models to interact with external environments, such as APIs or databases, to gather real-time data. This makes LLMs more reliable for tasks requiring factual accuracy or multi-step planning.</p>

<p>In the evolving field of artificial intelligence, large language models (LLMs) have transformed how we approach problem-solving, but they often struggle with hallucinations—generating plausible but incorrect information—or handling tasks requiring real-world interaction. Enter ReAct (Reasoning and Acting), a prompting framework that synergizes reasoning traces with actionable steps, enabling LLMs to behave more like intelligent agents. This detailed blog explores ReAct&rsquo;s foundations, mechanics, advantages, and practical implementation, culminating in a sample Python application using LangChain. We&rsquo;ll draw on established research and code examples to provide a comprehensive guide, updated with insights as of 2025.</p>

<a name="How-ReAct-Works"></a>
<h2>How ReAct Works</h2>

<p>In ReAct, the LLM generates a &ldquo;thought&rdquo; to plan, selects an &ldquo;action&rdquo; from available tools, observes the outcome, and iterates. This loop continues until the model outputs a final answer. For example, answering &ldquo;What is Olivia Wilde&rsquo;s boyfriend&rsquo;s age raised to the 0.23 power?&rdquo; might involve searching for the boyfriend, then calculating the power.</p>

<p><img src="/images/2025/reAct.gif" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Key-Points"></a>
<h2>Key Points</h2>

<ul>
<li><strong>ReAct Framework</strong>: It seems likely that ReAct is a prompting technique enabling LLMs to alternate between reasoning (thinking step-by-step) and acting (using tools like searches or calculations), improving accuracy on complex tasks by reducing hallucinations and incorporating external information.</li>
<li><strong>Core Process</strong>: Evidence leans toward a loop of Thought (reasoning), Action (tool invocation), Observation (results), repeating until a final answer, mimicking human problem-solving.</li>
<li><strong>Benefits and Limitations</strong>: Research suggests ReAct enhances interpretability and performance on knowledge-intensive and decision-making tasks, though it may increase computational costs and rely on well-defined tools; it&rsquo;s particularly useful for dynamic environments but less so for simple queries.</li>
</ul>



        <a class="btn pull-right" href="/blog/understanding-react-in-large-language-models/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/deep-dive-into-context-mcp-a2a-and-rag/">Deep Dive into Context: MCP, A2A and RAG</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-25T08:48:32+05:30" pubdate><i class="fa fa-calendar"></i> Aug 25th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/rag/'>rag</a></li>
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/a2a/'>a2a</a></li>
            
                <li><a class='category' href='/blog/categories/mcp/'>mcp</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>RAG combines retrieval from external sources with LLM generation to produce informed responses. For instance, it
retrieves documents from a vector store before prompting the model.</p>

<p>MCP, introduced by Anthropic, acts as a &ldquo;USB-C for AI,&rdquo; allowing models to dynamically access tools and data via a client-server model. It supports prompts, resources, and tools for contextual enhancement.</p>

<p>A2A, developed by Google, enables agents to exchange tasks and results over HTTP, using Agent Cards for discovery. It&rsquo;s modality-agnostic, supporting text, images, and more.</p>

<p>Related terms include ReAct (reasoning + acting loop for decision-making) and ACP (local-first agent coordination, differing from A2A&rsquo;s web-native focus).</p>

<p><img src="/images/2025/mcp_rag_a2a.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>


        <a class="btn pull-right" href="/blog/deep-dive-into-context-mcp-a2a-and-rag/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    

    <div class="pagination">
    
        <a class="btn pull-left" href="/blog/2">&larr; Older</a>
    

    
    </div>


</article>

                </div>

                
            </div>
        </div>

        

    
    




<footer class="footer">
    <div class="row middle-xs">
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <p class="footer__copyright">
    Copyright &copy; 2014 - 2025 - Rishijeet Mishra
</p>

        </div>
        
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <div>
    



    




<div class="hire hire--unavailable">
    
        
    
</div>

</div>
        </div>
        
    </div>
</footer>


        
<!--Adding the Mathjax support -->
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>

<script src="/javascripts/md5.js"></script>

<!--Octopress JS added to the site -->
<script defer src="/javascripts/octopress.js"></script>

<!--Ad thingy added by Rishi -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6086670860734956"
     crossorigin="anonymous"></script>


<!--Some analytics -->

<script>
    var _gaq=[['_setAccount','G-1P58V2BBV4'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1P58V2BBV4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1P58V2BBV4');
</script>



<!--DisQus thingy -->

<script>
    var disqus_shortname = 'rishijeet';
    
        
        // var disqus_developer = 1;
        var disqus_identifier = 'https://rishijeet.github.io/blog/index.html';
        var disqus_url = 'https://rishijeet.github.io/blog/index.html';
        var disqus_script = 'embed.js';
    
    (function () {
        // Only if disqus_thread id is defined load the embed script
        if (document.getElementById('disqus_thread')) {
        var your_sub_domain = ''; // Here goes your subdomain
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }
    })();
</script>




	<!-- 1. Add latest jQuery and fancyBox files -->
<!--Migrated to Fancybox 3 - -->

<script src="//code.jquery.com/jquery-3.2.1.min.js"></script>

<link rel="stylesheet" href="/css/jquery.fancybox.min.css" />
<script src="/javascripts/jquery.fancybox.min.js"></script>

<script type="text/javascript">
	$("[data-fancybox]").fancybox({
		// Options will go here
		image : {
		protect: true
				}
	});
</script>
<!--Adding some more restriction on photos-->
  <script type="text/javascript">
      document.addEventListener("contextmenu", (event) => {
         event.preventDefault();
      });
  </script> 
    </body>

</html>
