<!doctype html>
    <!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
    <!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
    <!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
    <!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

    
      
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Blogs - Rishijeet Mishra</title>
        <meta name="author" content="Rishijeet Mishra">
        
        <meta name="description" content="Rishijeet Mishra's Blog Site">
        
        <meta name="viewport" content="width=device-width">
        <meta name="google-site-verification" content="k3jIYcr9jzBS7xC3F_CC0Eqc-szFtcR-JBr1Wwqnk6w" />
        <link rel="canonical" href="https://rishijeet.github.io/blog">

        <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,400italic' rel='stylesheet' type='text/css'>
        <link href="https://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        
        <link href="/favicon.svg" rel="icon">
        <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet">
        <link href="/stylesheets/style.css" rel="stylesheet">
        <link href="" rel="alternate" title="Rishijeet Mishra" type="application/atom+xml">
    </head>


    <body >

        <header id="header">
    <div class="row">
    <div class="col-xs-12 col-sm-8 col-md-4">
        <a href="/" class="site-title">Rishijeet Mishra</a>
    </div>
    <div class="col-xs-12 col-sm-4 col-md-8">
    <nav>
    <input type="checkbox" id="toggle">
    <label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu"></label>
    <ul class="menu">
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/blog/archives/">Archive</a></li>
    <li><a href="/lenswork/">Photography</a></li>
    <li><a href="/about/">About</a></li>
</ul>

</nav>

    </div>
</div>

</header>


        <div id="main-content">

            

            

            <div class="row top-xs center-sm center-md center-lg site-wrapper">
                
                <div class="col-xs-12 col-sm-10 col-md-10 col-lg-10">
                
                    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<article class="page">

    
    <header>
        <h1 class="page__title">
            Blogs
        </h1>
    </header>
    <hr class="divider">
    

        
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/understanding-react-in-large-language-models/">Understanding ReAct in Large Language Models</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-28T08:48:16+05:30" pubdate><i class="fa fa-calendar"></i> Aug 28th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/mcp/'>mcp</a></li>
            
                <li><a class='category' href='/blog/categories/a2a/'>a2a</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>ReAct, short for Reasoning and Acting, is a paradigm for enhancing large language models (LLMs) by integrating verbal reasoning traces with task-specific actions. Introduced in a 2022 paper, it addresses limitations in chain-of-thought (CoT) prompting by allowing models to interact with external environments, such as APIs or databases, to gather real-time data. This makes LLMs more reliable for tasks requiring factual accuracy or multi-step planning.</p>

<p>In the evolving field of artificial intelligence, large language models (LLMs) have transformed how we approach problem-solving, but they often struggle with hallucinations—generating plausible but incorrect information—or handling tasks requiring real-world interaction. Enter ReAct (Reasoning and Acting), a prompting framework that synergizes reasoning traces with actionable steps, enabling LLMs to behave more like intelligent agents. This detailed blog explores ReAct&rsquo;s foundations, mechanics, advantages, and practical implementation, culminating in a sample Python application using LangChain. We&rsquo;ll draw on established research and code examples to provide a comprehensive guide, updated with insights as of 2025.</p>

<a name="How-ReAct-Works"></a>
<h2>How ReAct Works</h2>

<p>In ReAct, the LLM generates a &ldquo;thought&rdquo; to plan, selects an &ldquo;action&rdquo; from available tools, observes the outcome, and iterates. This loop continues until the model outputs a final answer. For example, answering &ldquo;What is Olivia Wilde&rsquo;s boyfriend&rsquo;s age raised to the 0.23 power?&rdquo; might involve searching for the boyfriend, then calculating the power.</p>

<p><img src="/images/2025/reAct.gif" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Key-Points"></a>
<h2>Key Points</h2>

<ul>
<li><strong>ReAct Framework</strong>: It seems likely that ReAct is a prompting technique enabling LLMs to alternate between reasoning (thinking step-by-step) and acting (using tools like searches or calculations), improving accuracy on complex tasks by reducing hallucinations and incorporating external information.</li>
<li><strong>Core Process</strong>: Evidence leans toward a loop of Thought (reasoning), Action (tool invocation), Observation (results), repeating until a final answer, mimicking human problem-solving.</li>
<li><strong>Benefits and Limitations</strong>: Research suggests ReAct enhances interpretability and performance on knowledge-intensive and decision-making tasks, though it may increase computational costs and rely on well-defined tools; it&rsquo;s particularly useful for dynamic environments but less so for simple queries.</li>
</ul>



        <a class="btn pull-right" href="/blog/understanding-react-in-large-language-models/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/deep-dive-into-context-mcp-a2a-and-rag/">Deep Dive into Context: MCP, A2A and RAG</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-25T08:48:32+05:30" pubdate><i class="fa fa-calendar"></i> Aug 25th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/rag/'>rag</a></li>
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/a2a/'>a2a</a></li>
            
                <li><a class='category' href='/blog/categories/mcp/'>mcp</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>RAG combines retrieval from external sources with LLM generation to produce informed responses. For instance, it
retrieves documents from a vector store before prompting the model.</p>

<p>MCP, introduced by Anthropic, acts as a &ldquo;USB-C for AI,&rdquo; allowing models to dynamically access tools and data via a client-server model. It supports prompts, resources, and tools for contextual enhancement.</p>

<p>A2A, developed by Google, enables agents to exchange tasks and results over HTTP, using Agent Cards for discovery. It&rsquo;s modality-agnostic, supporting text, images, and more.</p>

<p>Related terms include ReAct (reasoning + acting loop for decision-making) and ACP (local-first agent coordination, differing from A2A&rsquo;s web-native focus).</p>

<p><img src="/images/2025/mcp_rag_a2a.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>


        <a class="btn pull-right" href="/blog/deep-dive-into-context-mcp-a2a-and-rag/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora/">Efficient Fine-Tuning of Large Language Models: A Deep Dive into LoRA and QLoRA</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-17T18:27:01+05:30" pubdate><i class="fa fa-calendar"></i> Aug 17th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/ml/'>ml</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>In the era of large language models (LLMs) like GPT-3 and Llama, fine-tuning these behemoths for specific tasks has become a cornerstone of AI development. However, traditional full fine-tuning demands enormous computational resources, often requiring hundreds of GBs of GPU memory and extensive training time. This is where parameter-efficient fine-tuning (PEFT) techniques shine, allowing us to adapt massive models with minimal overhead. Among these, Low-Rank Adaptation (LoRA) and its quantized variant, Quantized LoRA (QLoRA), stand out for their efficiency and effectiveness. In this technical blog, we&rsquo;ll explore the mechanics, mathematics, advantages, and practical implementations of LoRA and QLoRA, drawing from foundational research and real-world applications.</p>

<a name="Understanding-Fine-2d-Tuning-Challenges"></a>
<h2>Understanding Fine-Tuning Challenges</h2>

<p>Full fine-tuning involves updating all parameters of a pre-trained model on a downstream dataset, which maximizes performance but at a steep cost. For instance, fine-tuning a 175B-parameter model like GPT-3 requires retraining every weight, leading to high memory usage and deployment challenges. PEFT methods mitigate this by updating only a subset of parameters or adding lightweight adapters, reducing trainable parameters by orders of magnitude while preserving model quality.</p>

<p><img src="/images/2025/lora_qlora.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>


        <a class="btn pull-right" href="/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/data-centers-in-the-united-states-and-ai-driven-developments/">Data Centers in the United States &amp; AI-Driven Developments</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-07-27T23:25:21+05:30" pubdate><i class="fa fa-calendar"></i> Jul 27th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/energy/'>energy</a></li>
            
                <li><a class='category' href='/blog/categories/data-center/'>data_center</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>Data centers are the backbone of the digital economy, housing the servers, storage systems, and networking equipment
that power cloud computing, web services, and data-intensive applications. In the United States, data centers are strategically located to meet the demands of businesses, governments, and consumers. The rise of artificial intelligence (AI) has further amplified the importance of data centers, requiring specialized infrastructure to handle complex computational workloads. This article explores the primary locations of data centers in the US, the reasons behind their selection, and recent developments driven by AI.</p>

<p><img src="/images/2025/data_center.png" height="300" width="900" alt="Alt text" /></p>

<a name="Major-Data-Center-Locations-in-the-United-States"></a>
<h2>Major Data Center Locations in the United States</h2>

<p>The US hosts approximately 5,381 data centers, with significant concentrations in specific regions that offer optimal conditions for operation. The top data center markets include:</p>


        <a class="btn pull-right" href="/blog/data-centers-in-the-united-states-and-ai-driven-developments/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    
        
        <article class="article article--index">
            <header class="article__header">
    
        <h1 class="h2 article__title">
            <a href="/blog/energy-requirements-for-ai-infrastructure-current-and-future-impacts/">Energy Requirements for AI Infrastructure: Current and Future Impacts</a>
        </h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-07-26T21:16:34+05:30" pubdate><i class="fa fa-calendar"></i> Jul 26th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        
            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/energy/'>energy</a></li>
            
                <li><a class='category' href='/blog/categories/data-center/'>data_center</a></li>
            
            
        </ul>
    </div>


            
        </div>
    
</header>




    <div class="article__content clearfix">
        <p>The rapid expansion of artificial intelligence (AI), particularly large language models (LLMs) and generative AI, has driven an unprecedented surge in energy demand due to the computational intensity of training and operating these systems. Eric Schmidt, former Google CEO, has highlighted electricity as the primary limiter of AI growth, estimating that the U.S. will require an additional 92 gigawatts (GW) of power—equivalent to the output of 92 nuclear power plants—to sustain the AI revolution. This analysis explores the current energy consumption of major companies’ AI infrastructure, projects future energy needs through 2035, and examines how these demands will reshape the energy sector, drawing on available data from web sources and posts on X.</p>

<a name="Current-Energy-Consumption-by-Major-Companies"></a>
<h2>Current Energy Consumption by Major Companies</h2>

<a name="Overview"></a>
<h3>Overview</h3>

<p>Major tech companies, or “hyperscalers” (e.g., Microsoft, Google, Meta, Amazon, OpenAI), are the primary drivers of AI infrastructure energy demand, operating massive data centers for training and inference of AI models. Training a single state-of-the-art AI model, such as OpenAI’s GPT-4, can consume 50 gigawatt-hours (GWh) of electricity, equivalent to the annual energy use of 4,800 U.S. households. Inference (running AI models for user queries) is also energy-intensive, with a single ChatGPT query requiring approximately 2.9 watt-hours, nearly 10 times that of a Google search (0.3 watt-hours). Below is an overview of key players’ energy footprints based on available data:</p>

<p><img src="/images/2025/Screenshot%202025-07-26%20at%208.43.24%E2%80%AFPM.png" height="300" width="900" alt="Alt text" /></p>


        <a class="btn pull-right" href="/blog/energy-requirements-for-ai-infrastructure-current-and-future-impacts/">Read on &rarr;</a>
    </div>


        </article>
        <hr>
    

    <div class="pagination">
    
        <a class="btn pull-left" href="/blog/2">&larr; Older</a>
    

    
    </div>


</article>

                </div>

                
            </div>
        </div>

        

    
    




<footer class="footer">
    <div class="row middle-xs">
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <p class="footer__copyright">
    Copyright &copy; 2014 - 2025 - Rishijeet Mishra
</p>

        </div>
        
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <div>
    



    




<div class="hire hire--unavailable">
    
        
    
</div>

</div>
        </div>
        
    </div>
</footer>


        
<!--Adding the Mathjax support -->
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>

<script src="/javascripts/md5.js"></script>

<!--Octopress JS added to the site -->
<script defer src="/javascripts/octopress.js"></script>

<!--Ad thingy added by Rishi -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6086670860734956"
     crossorigin="anonymous"></script>


<!--Some analytics -->

<script>
    var _gaq=[['_setAccount','G-1P58V2BBV4'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1P58V2BBV4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1P58V2BBV4');
</script>



<!--DisQus thingy -->

<script>
    var disqus_shortname = 'rishijeet';
    
        
        // var disqus_developer = 1;
        var disqus_identifier = 'https://rishijeet.github.io/blog/index.html';
        var disqus_url = 'https://rishijeet.github.io/blog/index.html';
        var disqus_script = 'embed.js';
    
    (function () {
        // Only if disqus_thread id is defined load the embed script
        if (document.getElementById('disqus_thread')) {
        var your_sub_domain = ''; // Here goes your subdomain
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }
    })();
</script>




	<!-- 1. Add latest jQuery and fancyBox files -->
<!--Migrated to Fancybox 3 - -->

<script src="//code.jquery.com/jquery-3.2.1.min.js"></script>

<link rel="stylesheet" href="/css/jquery.fancybox.min.css" />
<script src="/javascripts/jquery.fancybox.min.js"></script>

<script type="text/javascript">
	$("[data-fancybox]").fancybox({
		// Options will go here
		image : {
		protect: true
				}
	});
</script>
<!--Adding some more restriction on photos-->
  <script type="text/javascript">
      document.addEventListener("contextmenu", (event) => {
         event.preventDefault();
      });
  </script> 
    </body>

</html>
