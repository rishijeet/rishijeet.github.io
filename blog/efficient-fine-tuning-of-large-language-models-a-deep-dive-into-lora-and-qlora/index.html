<!doctype html>
    <!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
    <!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
    <!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
    <!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

    
      
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Efficient Fine-Tuning of Large Language Models: A Deep Dive into LoRA and QLoRA - Rishijeet Mishra</title>
        <meta name="author" content="Rishijeet Mishra">
        
        <meta name="description" content="Bridging tech and education - Rishijeet Mishra's insights on digital learning">
        
        <meta name="viewport" content="width=device-width">
        <meta name="google-site-verification" content="k3jIYcr9jzBS7xC3F_CC0Eqc-szFtcR-JBr1Wwqnk6w" />
        <link rel="canonical" href="https://rishijeet.github.io/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora">

        <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,400italic' rel='stylesheet' type='text/css'>
        <link href="https://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        
        <link href="/favicon.svg" rel="icon">
        <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet">
        <link href="/stylesheets/style.css" rel="stylesheet">
        <link href="" rel="alternate" title="Rishijeet Mishra" type="application/atom+xml">
    </head>


    <body >

        <header id="header">
    <div class="row">
    <div class="col-xs-12 col-sm-8 col-md-4">
        <a href="/" class="site-title">Rishijeet Mishra</a>
    </div>
    <div class="col-xs-12 col-sm-4 col-md-8">
    <nav>
    <input type="checkbox" id="toggle">
    <label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu"></label>
    <ul class="menu">
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/blog/archives/">Archive</a></li>
</ul>

</nav>

    </div>
</div>

</header>


        <div id="main-content">

            

            

            <div class="row top-xs center-sm center-md center-lg site-wrapper">
                
                <div class="col-xs-12 col-lg-10">
                
                    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet" type='text/css'>
<article class="article article--single">
    <header class="article__header">
    
        <h1 class="article__title">Efficient Fine-Tuning of Large Language Models: A Deep Dive into LoRA and QLoRA</h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-08-17T18:27:01+05:30" pubdate><i class="fa fa-calendar"></i> Aug 17th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        

            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
                <li><a class='category' href='/blog/categories/ml/'>ml</a></li>
            
        
        </ul>
    </div>


            
        </div>
    
</header>




    <p>In the era of large language models (LLMs) like GPT-3 and Llama, fine-tuning these behemoths for specific tasks has become a cornerstone of AI development. However, traditional full fine-tuning demands enormous computational resources, often requiring hundreds of GBs of GPU memory and extensive training time. This is where parameter-efficient fine-tuning (PEFT) techniques shine, allowing us to adapt massive models with minimal overhead. Among these, Low-Rank Adaptation (LoRA) and its quantized variant, Quantized LoRA (QLoRA), stand out for their efficiency and effectiveness. In this technical blog, we&rsquo;ll explore the mechanics, mathematics, advantages, and practical implementations of LoRA and QLoRA, drawing from foundational research and real-world applications.</p>

<a name="Understanding-Fine-2d-Tuning-Challenges"></a>
<h2>Understanding Fine-Tuning Challenges</h2>

<p>Full fine-tuning involves updating all parameters of a pre-trained model on a downstream dataset, which maximizes performance but at a steep cost. For instance, fine-tuning a 175B-parameter model like GPT-3 requires retraining every weight, leading to high memory usage and deployment challenges. PEFT methods mitigate this by updating only a subset of parameters or adding lightweight adapters, reducing trainable parameters by orders of magnitude while preserving model quality.</p>

<p><img src="/images/2025/lora_qlora.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<!--more-->


<a name="What-is-LoRA-3f-"></a>
<h2>What is LoRA?</h2>

<p>LoRA, or Low-Rank Adaptation, was introduced by Microsoft researchers in 2021 as a method to fine-tune LLMs by injecting low-rank trainable matrices into the model&rsquo;s layers without altering the original weights. The core idea stems from the observation that weight updates during fine-tuning often lie in a low-dimensional subspace—meaning the changes to the model&rsquo;s weights have a low &ldquo;intrinsic rank.&rdquo; Instead of updating the full weight matrix, LoRA decomposes these updates into smaller, low-rank factors.</p>

<a name="How-LoRA-Works"></a>
<h3>How LoRA Works</h3>

<p>Consider a pre-trained weight matrix \( W_0 \in \mathbb{R}^{d \times k} \) in a Transformer layer (e.g., attention weights like \( W_q, W_k, W_v, W_o \)). During fine-tuning, the update is constrained as \( \Delta W = BA \), where \( B \in \mathbb{R}^{d \times r} \), \( A \in \mathbb{R}^{r \times k} \), and \( r \ll \min(d, k) \) is the rank (typically 1-64). The original weights \( W_0 \) remain frozen, and only \( A \) and \( B \) are trained.</p>

<p>The forward pass becomes:
\[
h = W_0 x + \frac{\alpha}{r} BA x
\]
where \( \alpha \) is a scaling factor (often set to twice the rank for stability), and the division by \( r \) normalizes the update magnitude. Initialization is key: \( A \) starts with random Gaussian values, while \( B \) is zeroed to ensure no initial change.</p>

<p>LoRA is typically applied to attention layers in Transformers, as empirical studies show this yields the best results with fewer parameters. For a model like GPT-3 175B, this reduces trainable parameters from billions to just thousands (e.g., 0.3M for r=8), slashing GPU memory needs by 3x and trainable parameters by 10,000x compared to full fine-tuning.</p>

<a name="Advantages-of-LoRA"></a>
<h3>Advantages of LoRA</h3>

<ul>
<li><strong>Efficiency</strong>: Training throughput increases due to fewer gradients and optimizer states. For example, on GPT-3, LoRA matches or exceeds full fine-tuning quality on benchmarks like RoBERTa and DeBERTa while using far less memory.</li>
<li><strong>Deployment Flexibility</strong>: Post-training, \( BA \) can be merged into \( W_0 \), incurring no inference latency. Multiple LoRA adapters can share the base model, enabling quick task-switching.</li>
<li><strong>Hyperparameter Tips</strong>: Common ranks are 4-32; alpha is often 2x rank. Libraries like Hugging Face&rsquo;s PEFT make integration seamless.</li>
</ul>


<a name="Introducing-QLoRA:-Quantization-Meets-LoRA"></a>
<h2>Introducing QLoRA: Quantization Meets LoRA</h2>

<p>While LoRA is efficient, fine-tuning still requires loading the full model in high-precision formats (e.g., FP16), which can exceed single-GPU limits for models over 30B parameters. QLoRA, proposed in 2023, extends LoRA by quantizing the base model to 4 bits, enabling fine-tuning of 65B+ models on a single 48GB GPU without performance loss.</p>

<a name="How-QLoRA-Builds-on-LoRA"></a>
<h3>How QLoRA Builds on LoRA</h3>

<p>QLoRA freezes a 4-bit quantized version of the pre-trained model and backpropagates gradients through it into LoRA adapters. Key innovations include:</p>

<ul>
<li><strong>4-bit NormalFloat (NF4) Quantization</strong>: An information-theoretically optimal data type for normally distributed weights. Weights are normalized to [-1, 1] and quantized into bins with equal expected values from a N(0,1) distribution, avoiding outliers.</li>
<li><strong>Double Quantization</strong>: Quantizes the quantization constants themselves (e.g., to 8-bit), saving ~0.37 bits per parameter by reducing constants&#8217; memory footprint.</li>
<li><strong>Paged Optimizers</strong>: Uses NVIDIA unified memory to page optimizer states to CPU RAM during spikes, preventing OOM errors.</li>
</ul>


<p>Mathematically, for a linear layer:</p>

<div class="math-wrap">
&#92;[
Y_{&#92;text{BF16}} = X_{&#92;text{BF16}} &#92;cdot &#92;text{doubleDequant}(c_{&#92;text{FP32}_1}, c_{k&#92;text{-bit}_2}, W_{&#92;text{NF4}})
+ X_{&#92;text{BF16}} &#92;cdot L_{&#92;text{BF16}_1} &#92;cdot L_{&#92;text{BF16}_2}
&#92;]
</div>


<p>Gradients are computed in BF16 for LoRA params (\( L_1, L_2 \)) only, while the quantized base remains frozen.</p>

<p>QLoRA matches 16-bit full fine-tuning on benchmarks like Vicuna, with models like Guanaco achieving 99.3% of ChatGPT&rsquo;s performance after 24 hours on one GPU.</p>

<a name="Advantages-of-QLoRA-Over-LoRA"></a>
<h3>Advantages of QLoRA Over LoRA</h3>

<ul>
<li><strong>Memory Savings</strong>: Reduces footprint from >780GB to &lt;48GB for 65B models, democratizing access.</li>
<li><strong>No Performance Trade-off</strong>: Unlike naive quantization, QLoRA preserves accuracy by fine-tuning adapters on high-quality data.</li>
<li><strong>Scalability</strong>: Enables fine-tuning on consumer hardware, with extensions like 8-bit integration in Hugging Face for even broader use.</li>
</ul>


<a name="Practical-Implementation-with-Hugging-Face"></a>
<h2>Practical Implementation with Hugging Face</h2>

<p>Hugging Face&rsquo;s libraries (Transformers, PEFT, Diffusers) simplify LoRA/QLoRA usage. For LoRA fine-tuning of Stable Diffusion:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><pre><code class="python"><span class='line'><span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">StableDiffusionPipeline</span>
</span><span class='line'><span class="n">pipe</span> <span class="o">=</span> <span class="n">StableDiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">&quot;runwayml/stable-diffusion-v1-5&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">pipe</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  <span class="c"># Enable LoRA with rank 4</span>
</span><span class='line'><span class="c"># Train on dataset, then merge and infer</span>
</span></code></pre></div></figure>


<p>For QLoRA with Gemma:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><pre><code class="python"><span class='line'><span class="kn">from</span> <span class="nn">keras_nlp.models</span> <span class="kn">import</span> <span class="n">GemmaLM</span>
</span><span class='line'><span class="n">gemma_lm</span> <span class="o">=</span> <span class="n">GemmaLM</span><span class="o">.</span><span class="n">from_preset</span><span class="p">(</span><span class="s">&quot;gemma_2b_en&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">gemma_lm</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="s">&quot;int8&quot;</span><span class="p">)</span>  <span class="c"># Quantize to 8-bit (extendable to 4-bit)</span>
</span><span class='line'><span class="n">gemma_lm</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span><span class='line'><span class="c"># Fine-tune and evaluate</span>
</span></code></pre></div></figure>


<p>Recent tutorials emphasize dataset preparation and evaluation for vision-language models like QWEN2.5VL.</p>

<a name="Applications-and-Case-Studies"></a>
<h2>Applications and Case Studies</h2>

<p>LoRA/QLoRA power domain-specific adaptations, from chatbots (e.g., Guanaco) to image generation (e.g., Pokémon fine-tuning). In production, they&rsquo;ve enabled zero-shot learning via hypernetworks and optimized LLMs for edge devices. Studies from Lightning AI show QLoRA excelling in memory-constrained environments across hundreds of experiments.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>LoRA and QLoRA revolutionize LLM fine-tuning by making it accessible, efficient, and scalable. LoRA&rsquo;s low-rank decomposition minimizes parameters, while QLoRA&rsquo;s quantization pushes boundaries further, enabling massive models on modest hardware. As AI evolves, these techniques will be pivotal for customizing foundation models. Experiment with Hugging Face tools to harness their power in your projects.</p>



    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6086670860734956"
     crossorigin="anonymous"></script>
</article>


<section id="disqus">
    <h1 class="disqus__title">Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>


                </div>

                
            </div>
        </div>

        

    
    




<footer class="footer">
    <div class="row middle-xs">
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <p class="footer__copyright">
    Copyright &copy; 2014 - 2025 - Rishijeet Mishra
</p>

        </div>
        
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <div>
    



    




<div class="hire hire--unavailable">
    
        
    
</div>

</div>
        </div>
        
    </div>
</footer>


        
<!--Adding the Mathjax support -->
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>

<script src="/javascripts/md5.js"></script>

<!--Octopress JS added to the site -->
<script defer src="/javascripts/octopress.js"></script>

<!--Ad thingy added by Rishi -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6086670860734956"
     crossorigin="anonymous"></script>


<!--Some analytics -->

<script>
    var _gaq=[['_setAccount','G-1P58V2BBV4'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1P58V2BBV4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1P58V2BBV4');
</script>



<!--DisQus thingy -->

<script>
    var disqus_shortname = 'rishijeet';
    
        
        // var disqus_developer = 1;
        var disqus_identifier = 'https://rishijeet.github.io/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora/';
        var disqus_url = 'https://rishijeet.github.io/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora/';
        var disqus_script = 'embed.js';
    
    (function () {
        // Only if disqus_thread id is defined load the embed script
        if (document.getElementById('disqus_thread')) {
        var your_sub_domain = ''; // Here goes your subdomain
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }
    })();
</script>




	<!-- 1. Add latest jQuery and fancyBox files -->
<!--Migrated to Fancybox 3 - -->

<script src="//code.jquery.com/jquery-3.2.1.min.js"></script>

<link rel="stylesheet" href="/css/jquery.fancybox.min.css" />
<script src="/javascripts/jquery.fancybox.min.js"></script>

<script type="text/javascript">
	$("[data-fancybox]").fancybox({
		// Options will go here
		image : {
		protect: true
				}
	});
</script>
<!--Adding some more restriction on photos-->
  <script type="text/javascript">
      document.addEventListener("contextmenu", (event) => {
         event.preventDefault();
      });
  </script> 
    </body>

</html>
