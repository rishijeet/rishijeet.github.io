<!doctype html>
    <!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
    <!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
    <!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
    <!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

    
      
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>From Text to Tokens: The Complete Guide to Tokenization in LLMs - Rishijeet Mishra | Technologist | Tech Trends & Development Blog</title>
        <meta name="author" content="Rishijeet Mishra">
        
        <meta name="description" content="Bridging tech and education - Rishijeet Mishra's insights on digital learning">
        
        <meta name="viewport" content="width=device-width">
        <meta name="google-site-verification" content="k3jIYcr9jzBS7xC3F_CC0Eqc-szFtcR-JBr1Wwqnk6w" />
        <link rel="canonical" href="https://rishijeet.github.io/blog/from-text-to-tokens-the-complete-guide-to-tokenization-in-llms">

        <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,400italic' rel='stylesheet' type='text/css'>
        <link href="https://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        
        <link href="/favicon.svg" rel="icon">
        <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet">
        <link href="/stylesheets/style.css" rel="stylesheet">
        <link href="" rel="alternate" title="Rishijeet Mishra | Technologist | Tech Trends & Development Blog" type="application/atom+xml">
    </head>


    <body >

        <header id="header">
    <div class="row">
    <div class="col-xs-12 col-sm-8 col-md-4">
        <a href="/" class="site-title">Rishijeet Mishra</a>
    </div>
    <div class="col-xs-12 col-sm-4 col-md-8">
    <nav>
    <input type="checkbox" id="toggle">
    <label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu"></label>
    <ul class="menu">
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/blog/archives/">Archive</a></li>
</ul>

</nav>

    </div>
</div>

</header>


        <div id="main-content">

            

            

            <div class="row top-xs center-sm center-md center-lg site-wrapper">
                
                <div class="col-xs-12 col-lg-10">
                
                    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet" type='text/css'>
<article class="article article--single">
    <header class="article__header">
    
        <h1 class="article__title">From Text to Tokens: The Complete Guide to Tokenization in LLMs</h1>
    

    
        <div class="article__meta clearfix">
            






    <time class="article__date pull-left" datetime="2025-06-28T08:55:51+05:30" pubdate><i class="fa fa-calendar"></i> Jun 28th, 2025</time>




            

    <div class="article__tags pull-left">
        <i class="fa fa-tags"></i>
        <ul class="unstyled">
        

            
                <li><a class='category' href='/blog/categories/llm/'>llm</a></li>
            
                <li><a class='category' href='/blog/categories/ai/'>ai</a></li>
            
        
        </ul>
    </div>


            
        </div>
    
</header>




    <p>In the ever-evolving field of artificial intelligence, large language models (LLMs) like GPT-4, Claude, Gemini, and LLaMA have reshaped how machines understand and generate human language. Behind the impressive capabilities of these models lies a deceptively simple but foundational step: <strong>tokenization</strong>.</p>

<p>In this blog, we will dive deep into the concept of tokenization, understand its types, why it&rsquo;s needed, the challenges it solves, how it works under the hood, and where it‚Äôs headed in the future. This is a one-stop technical deep-dive for anyone looking to fully grasp the backbone of language understanding in LLMs.</p>

<hr />

<a name="L-3c-strong-3e-What-is-Tokenization-3f--3c--2f-strong-3e-"></a>
<h2><strong>What is Tokenization?</strong></h2>

<p>At its core, tokenization is the process of converting raw text into smaller units called <strong>tokens</strong> that a language model can understand and process. These tokens can be:</p>

<ul>
<li>Characters</li>
<li>Words</li>
<li>Subwords</li>
<li>Byte-pair sequences</li>
<li>WordPieces</li>
<li>SentencePieces</li>
<li>Byte-level representations</li>
</ul>


<p>Each model has its own strategy, depending on design goals like efficiency, vocabulary size, multilingual handling, and memory constraints.</p>

<!--more-->


<p>For example, the sentence:</p>

<figure class='code'><div class="highlight"><pre><code class=""><span class='line'>"Tokenization is crucial for LLMs."</span></code></pre></div></figure>


<p>May be tokenized as:</p>

<ul>
<li>Word-level: <code>["Tokenization", "is", "crucial", "for", "LLMs", "."]</code></li>
<li>Character-level: <code>["T", "o", "k", ..., "L", "L", "M", "s", "."]</code></li>
<li>Subword (BPE): <code>["Token", "ization", "is", "cru", "cial", "for", "LL", "Ms", "."]</code></li>
</ul>


<a name="Tokens"></a>
<h4>Tokens</h4>

<p><img src="/images/2025/token.png" height="300" width="900" alt="Alt text" /></p>

<a name="Token-IDs"></a>
<h4>Token IDs</h4>

<p><img src="/images/2025/tokenid.png" height="300" width="900" alt="Alt text" /></p>

<hr />

<a name="L-3c-strong-3e-Why-Tokenization-is-Needed-in-LLMs-3c--2f-strong-3e-"></a>
<h2><strong>Why Tokenization is Needed in LLMs</strong></h2>

<p>Language models operate over numbers (tensors), not raw strings. Before any neural network processes your prompt, the words must be:</p>

<ol>
<li><strong>Split into atomic units (tokens)</strong></li>
<li><strong>Mapped to numerical IDs (vocabulary embedding)</strong></li>
<li><strong>Fed into the model as vectors</strong></li>
</ol>


<p>Without tokenization:</p>

<ul>
<li>Models would struggle with infinite vocabulary.</li>
<li>Multilingual text and compound words would explode the vocabulary.</li>
<li>There would be no efficient way to control sequence length or positional encoding.</li>
</ul>


<hr />

<a name="L-3c-strong-3e-Types-of-Tokenization-Strategies-3c--2f-strong-3e-"></a>
<h2><strong>Types of Tokenization Strategies</strong></h2>

<a name="L-3c-strong-3e-Word-2d-Level-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Word-Level Tokenization</strong></h3>

<p>Each word is a token. Simple but inefficient for:</p>

<ul>
<li>Unknown words (out-of-vocabulary issues)</li>
<li>Morphologically rich languages</li>
<li>Compound words</li>
</ul>


<p><strong>Example:</strong>
&ldquo;unhappiness&rdquo; ‚Üí 1 token ‚Üí [‚Äúunhappiness‚Äù]
If unseen during training, this is a problem.</p>

<hr />

<a name="L-3c-strong-3e-Character-2d-Level-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Character-Level Tokenization</strong></h3>

<p>Each character is a token. Solves OOV issues but leads to longer sequences and loss of semantic granularity.</p>

<p><strong>Example:</strong>
&ldquo;unhappiness&rdquo; ‚Üí [‚Äúu‚Äù, ‚Äún‚Äù, ‚Äúh‚Äù, ‚Äúa‚Äù, ‚Äúp‚Äù, ‚Ä¶]</p>

<hr />

<a name="L-3c-strong-3e-Subword-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Subword Tokenization</strong></h3>

<p>Breaks words into frequent subword units using statistical techniques like:</p>

<ul>
<li><strong>Byte Pair Encoding (BPE)</strong> ‚Äì used by GPT-2, GPT-3</li>
<li><strong>WordPiece</strong> ‚Äì used by BERT</li>
<li><strong>Unigram Language Model</strong> ‚Äì used by SentencePiece (T5, LLaMA)</li>
</ul>


<p><strong>Example (BPE):</strong>
&ldquo;unhappiness&rdquo; ‚Üí [‚Äúun‚Äù, ‚Äúhappi‚Äù, ‚Äúness‚Äù]</p>

<p><strong>Benefits:</strong></p>

<ul>
<li>Handles unknown words gracefully</li>
<li>Reduces vocabulary size</li>
<li>Efficient for multilingual models</li>
</ul>


<hr />

<a name="L-3c-strong-3e-Byte-2d-Level-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Byte-Level Tokenization</strong></h3>

<p>Tokenizes text at the byte level, including UTF-8 encodings.</p>

<p>Used by models like GPT-3.5/4 to handle raw binary inputs and emojis robustly.</p>

<p><strong>Example:</strong>
‚Äúüî•‚Äù ‚Üí byte sequence ‚Üí [240, 159, 148, 165]</p>

<hr />

<a name="L-3c-strong-3e-SentencePiece-3c--2f-strong-3e-"></a>
<h3><strong>SentencePiece</strong></h3>

<p>A library that trains subword models using BPE or Unigram LM on raw text. Used in multilingual LLMs like T5, mT5.</p>

<p>It allows training on raw text without pre-tokenization (no need for whitespace-based splitting).</p>

<hr />

<a name="L-3c-strong-3e-How-Tokenization-Works:-Under-the-Hood-3c--2f-strong-3e-"></a>
<h2><strong>How Tokenization Works: Under the Hood</strong></h2>

<a name="L-3c-strong-3e-Training-a-Tokenizer-3c--2f-strong-3e-"></a>
<h3><strong>Training a Tokenizer</strong></h3>

<p>During tokenizer training, the process involves:</p>

<ul>
<li>Reading a large corpus</li>
<li>Building frequency tables of substrings</li>
<li>Iteratively merging the most frequent substrings</li>
<li>Forming a vocabulary of tokens</li>
<li>Saving a tokenizer model (vocab + merge rules)</li>
</ul>


<a name="L-3c-strong-3e-Encoding-3c--2f-strong-3e-"></a>
<h3><strong>Encoding</strong></h3>

<p>At inference or training:</p>

<ul>
<li>Input string ‚Üí split into substrings based on learned merges</li>
<li>Tokens ‚Üí mapped to numerical IDs via the vocabulary</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><pre><code class="python"><span class='line'><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>
</span><span class='line'>
</span><span class='line'><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">&quot;gpt2&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&quot;Tokenization is powerful.&quot;</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span><span class='line'><span class="c"># Output: [&#39;Token&#39;, &#39;ization&#39;, &#39;ƒ†is&#39;, &#39;ƒ†powerful&#39;, &#39;.&#39;]</span>
</span></code></pre></div></figure>


<p><code>ƒ†</code> indicates a space in GPT-2 tokenizers.</p>

<hr />

<a name="L-3c-strong-3e-Tokenization-and-Model-Limits-3c--2f-strong-3e-"></a>
<h2><strong>Tokenization and Model Limits</strong></h2>

<p>Most LLMs have a context window defined in <strong>tokens</strong>, not characters. For instance:</p>

<ul>
<li>GPT-3.5: 4,096 tokens</li>
<li>GPT-4 (o4): 128,000 tokens</li>
<li>Claude 3 Opus: \~200,000 tokens</li>
</ul>


<p>So, 1000 words of English ‚âà 750 tokens.</p>

<p>This is crucial for prompt design, summarization, RAG (Retrieval Augmented Generation), and efficient inference.</p>

<a name="L-3c-strong-3e-Challenges-and-Trade-2d-offs-3c--2f-strong-3e-"></a>
<h2><strong>Challenges and Trade-offs</strong></h2>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Challenge</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>OOV (Out-of-Vocabulary)</td>
        <td>Especially for word-level tokenization</td>
      </tr>
      <tr>
        <td>Token inflation</td>
        <td>Some languages (e.g., Chinese, Japanese) produce more tokens</td>
      </tr>
      <tr>
        <td>Inconsistency</td>
        <td>Subword boundaries may not align with morphemes</td>
      </tr>
      <tr>
        <td>Efficiency vs Accuracy</td>
        <td>Smaller tokens = longer sequences = more compute</td>
      </tr>
      <tr>
        <td>Encoding Bias</td>
        <td>Tokenizers trained on certain scripts or corpora may underperform on others</td>
      </tr>
    </tbody>
  </table>
</div>


<a name="L-3c-strong-3e-Tokenization-in-Multilingual-and-Code-Models-3c--2f-strong-3e-"></a>
<h2><strong>Tokenization in Multilingual and Code Models</strong></h2>

<a name="L-3c-strong-3e-Multilingual-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Multilingual Tokenization</strong></h3>

<ul>
<li>Unicode-aware models must handle multiple scripts (Latin, Devanagari, Arabic, etc.)</li>
<li>Token inflation can disadvantage languages like Hindi and Tamil</li>
<li>SentencePiece helps standardize across languages</li>
</ul>


<a name="L-3c-strong-3e-Code-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Code Tokenization</strong></h3>

<ul>
<li>Code models (e.g., Codex, CodeBERT) often use language-specific tokenizers</li>
<li>Must preserve syntax, spacing, indentation, and even comments</li>
</ul>


<p>Example:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><pre><code class="python"><span class='line'><span class="k">def</span> <span class="nf">say_hello</span><span class="p">():</span>
</span><span class='line'>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Hello&quot;</span><span class="p">)</span>
</span></code></pre></div></figure>


<p>‚Üí <code>[‚Äòdef‚Äô, ‚Äòƒ†say‚Äô, ‚Äò_‚Äô, ‚Äòhello‚Äô, ‚Äò()‚Äô, ‚Äò:‚Äô, ‚Äòƒ†‚Äô, ‚Äòprint‚Äô, ‚Äò(‚Äù, ‚ÄòHello‚Äô, ‚Äò‚Äù)‚Äô]</code></p>

<hr />

<a name="L-3c-strong-3e-Compression-2c--Prompt-Engineering-2c--and-Token-Optimization-3c--2f-strong-3e-"></a>
<h2><strong>Compression, Prompt Engineering, and Token Optimization</strong></h2>

<p>Tokenization also directly affects:</p>

<ul>
<li><strong>Prompt length limits</strong> (compressed prompts ‚Üí more room for data)</li>
<li><strong>Token cost in inference/billing</strong></li>
<li><strong>RAG performance</strong> (chunking based on tokens)</li>
<li><strong>Training data deduplication</strong> (token-based hashing)</li>
</ul>


<p>Optimizing prompts for fewer tokens can reduce cost and latency.</p>

<hr />

<a name="L-3c-strong-3e-Tokenization-vs.-Embeddings-3c--2f-strong-3e-"></a>
<h2><strong>Tokenization vs. Embeddings</strong></h2>

<p>It‚Äôs important to note:</p>

<ul>
<li><strong>Tokenization</strong> comes before embedding.</li>
<li>Token ‚Üí token ID ‚Üí embedding vector</li>
</ul>


<p>A poor tokenization scheme = noisy embeddings = reduced model performance.</p>

<hr />

<a name="L-3c-strong-3e-The-Future-of-Tokenization-in-LLMs-3c--2f-strong-3e-"></a>
<h2><strong>The Future of Tokenization in LLMs</strong></h2>

<a name="L-3c-strong-3e-Token-2d-Free-Models-3c--2f-strong-3e-"></a>
<h3><strong>Token-Free Models</strong></h3>

<p>Efforts like <strong>Charformer</strong> and <strong>Byte-level transformers</strong> aim to bypass static tokenization and learn from raw bytes or characters.</p>

<a name="L-3c-strong-3e-Neural-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Neural Tokenization</strong></h3>

<p>Trainable tokenizers using neural nets to learn optimal segmentation dynamically.</p>

<a name="L-3c-strong-3e-Universal-Tokenizers-3c--2f-strong-3e-"></a>
<h3><strong>Universal Tokenizers</strong></h3>

<p>Tokenizers trained across modalities (text, image, code) using a common vocabulary to unify multimodal models.</p>

<a name="L-3c-strong-3e-Efficient-Context-Windows-3c--2f-strong-3e-"></a>
<h3><strong>Efficient Context Windows</strong></h3>

<p>With sliding-window and compression-based methods (e.g., Mamba, Hyena), token overhead may reduce for long contexts.</p>

<hr />

<a name="L-3c-strong-3e-Major-LLMs-and-Their-Tokenization-3c--2f-strong-3e-"></a>
<h2><strong>Major LLMs and Their Tokenization</strong></h2>

<div class="scrollable-table-container">
  <h2>Model Tokenizers</h2>
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Model</th>
        <th>Tokenizer</th>
        <th>Type</th>
        <th>Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>GPT-3/4</td>
        <td>GPT2Tokenizer</td>
        <td>BPE + byte</td>
        <td>Handles Unicode well</td>
      </tr>
      <tr>
        <td>BERT</td>
        <td>WordPiece</td>
        <td>Subword</td>
        <td>Requires pre-tokenization</td>
      </tr>
      <tr>
        <td>RoBERTa</td>
        <td>BPE (FairSeq)</td>
        <td>Subword</td>
        <td>Custom vocabulary</td>
      </tr>
      <tr>
        <td>T5</td>
        <td>SentencePiece</td>
        <td>Unigram LM</td>
        <td>Whitespace-free tokenization</td>
      </tr>
      <tr>
        <td>LLaMA 2/3/4</td>
        <td>SentencePiece</td>
        <td>Unigram LM</td>
        <td>Supports multiple languages</td>
      </tr>
      <tr>
        <td>Claude</td>
        <td>Byte-level BPE</td>
        <td>Proprietary</td>
        <td>Handles emojis and long context</td>
      </tr>
    </tbody>
  </table>
</div>


<hr />

<a name="L-3c-strong-3e-Conclusion-3c--2f-strong-3e-"></a>
<h2><strong>Conclusion</strong></h2>

<p>Tokenization may appear trivial at first glance, but it&rsquo;s the hidden workhorse powering the language capabilities of every modern LLM. From enabling multilingual understanding to compressing long documents into tight prompts, tokenization determines what the model sees, learns, and generates.</p>

<p>As we step into an era of 1M+ token context windows, modality fusion, and instruction-following agents, tokenization will either evolve or be replaced by more fluid, learned representations.</p>

<p>But for now‚Äîand the foreseeable future‚Äîit remains a vital piece of the LLM puzzle.</p>



    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6086670860734956"
     crossorigin="anonymous"></script>
</article>


<section id="disqus">
    <h1 class="disqus__title">Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>


                </div>

                
            </div>
        </div>

        

    
    




<footer class="footer">
    <div class="row middle-xs">
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <p class="footer__copyright">
    Copyright &copy; 2014 - 2025 - Rishijeet Mishra
</p>

        </div>
        
        
        <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
            <div>
    



    




<div class="hire hire--unavailable">
    
        
    
</div>

</div>
        </div>
        
    </div>
</footer>


        
<!--Adding the Mathjax support -->
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>

<script src="/javascripts/md5.js"></script>

<!--Octopress JS added to the site -->
<script defer src="/javascripts/octopress.js"></script>

<!--Ad thingy added by Rishi -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6086670860734956"
     crossorigin="anonymous"></script>


<!--Some analytics -->

<script>
    var _gaq=[['_setAccount','G-1P58V2BBV4'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1P58V2BBV4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1P58V2BBV4');
</script>



<!--DisQus thingy -->

<script>
    var disqus_shortname = 'rishijeet';
    
        
        // var disqus_developer = 1;
        var disqus_identifier = 'https://rishijeet.github.io/blog/from-text-to-tokens-the-complete-guide-to-tokenization-in-llms/';
        var disqus_url = 'https://rishijeet.github.io/blog/from-text-to-tokens-the-complete-guide-to-tokenization-in-llms/';
        var disqus_script = 'embed.js';
    
    (function () {
        // Only if disqus_thread id is defined load the embed script
        if (document.getElementById('disqus_thread')) {
        var your_sub_domain = ''; // Here goes your subdomain
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }
    })();
</script>




	<!-- 1. Add latest jQuery and fancyBox files -->
<!--Migrated to Fancybox 3 - -->

<script src="//code.jquery.com/jquery-3.2.1.min.js"></script>

<link rel="stylesheet" href="/css/jquery.fancybox.min.css" />
<script src="/javascripts/jquery.fancybox.min.js"></script>

<script type="text/javascript">
	$("[data-fancybox]").fancybox({
		// Options will go here
		image : {
		protect: true
				}
	});
</script>
<!--Adding some more restriction on photos-->
  <script type="text/javascript">
      document.addEventListener("contextmenu", (event) => {
         event.preventDefault();
      });
  </script> 
    </body>

</html>
