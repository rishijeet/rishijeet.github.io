<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: database | Rishijeet Mishra]]></title>
    <link href="https://rishijeet.github.io/blog/categories/database/atom.xml" rel="self"/>
    <link href="https://rishijeet.github.io/"/>
    <updated>2024-06-26T09:46:07+05:30</updated>
    <id>https://rishijeet.github.io/</id>
    <author>
        <name><![CDATA[Rishijeet Mishra]]></name>
        <email><![CDATA[rishijeet@gmail.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Cassandra - Under the hood]]></title>
        <link href="https://rishijeet.github.io/blog/cassandra-under-the-hood/"/>
        <updated>2024-05-22T23:48:44+05:30</updated>
        <id>https://rishijeet.github.io/blog/cassandra-under-the-hood</id>
        <content type="html"><![CDATA[<p>Apache Cassandra is designed to handle large amounts of data across many commodity servers without any single point of failure. This architecture allows it to provide high availability and fault tolerance, making it an excellent choice for large-scale, mission-critical applications. Below, we&rsquo;ll delve into the key components and architecture of Cassandra.</p>

<a name="Key-Components"></a>
<h3>Key Components</h3>

<ul>
<li><strong>Nodes</strong>: Individual machines running Cassandra.</li>
<li><strong>Clusters</strong>: A collection of nodes that work together.</li>
<li><strong>Data Centers</strong>: Groupings of nodes within a cluster, typically corresponding to physical or logical locations.</li>
<li><strong>Keyspace</strong>: A namespace for tables, analogous to a database in SQL.</li>
<li><strong>Tables</strong>: Collections of rows, each row containing columns, similar to tables in an RDBMS.</li>
<li><strong>Commit Log</strong>: A log of all write operations, used for crash recovery.</li>
<li><strong>Memtable</strong>: An in-memory structure where data is first written.</li>
<li><strong>SSTable</strong>: Immutable on-disk storage files created from flushed Memtables.</li>
<li><strong>Bloom Filters</strong>: Probabilistic data structures that help determine whether an SSTable might contain a requested row.</li>
</ul>


<a name="Architecture-Overview"></a>
<h3>Architecture Overview</h3>

<a name="Cluster-Management"></a>
<h4>Cluster Management</h4>

<p>Cassandra&rsquo;s cluster architecture ensures high availability and fault tolerance. The cluster is a set of nodes, and data is distributed among these nodes using consistent hashing. Key features include:</p>

<ul>
<li><strong>Gossip Protocol</strong>: Nodes communicate with each other using a peer-to-peer gossip protocol to share state information.</li>
<li><strong>Snitches</strong>: Determine the relative distance between nodes to route requests efficiently.</li>
<li><strong>Replication</strong>: Data is replicated across multiple nodes. The replication strategy and factor determine how and where data is replicated.</li>
</ul>


<!--more-->


<a name="Data-Distribution"></a>
<h4>Data Distribution</h4>

<p>Cassandra uses a consistent hashing algorithm to distribute data across nodes. Key features include:</p>

<ul>
<li><strong>Partitioners</strong>: Determine the node placement of data based on the primary key.</li>
<li><strong>Token Ring</strong>: Each node in the cluster is assigned a range of tokens. Data is distributed based on these tokens.</li>
<li><strong>Replication Factor</strong>: The number of copies of data stored in the cluster.</li>
</ul>


<a name="Write-Path"></a>
<h4>Write Path</h4>

<p>The write path in Cassandra ensures durability and high availability:</p>

<ul>
<li><strong>Commit Log</strong>: Each write operation is recorded in the commit log for durability.</li>
<li><strong>Memtable</strong>: The data is written to an in-memory structure called the Memtable.</li>
<li><strong>SSTable</strong>: Once the Memtable is full, data is flushed to disk into an SSTable.</li>
<li><strong>Compaction</strong>: Over time, SSTables are compacted to merge and purge deleted data.</li>
</ul>


<a name="Read-Path"></a>
<h4>Read Path</h4>

<p>The read path in Cassandra is optimized for speed:</p>

<ul>
<li><strong>Read Request</strong>: A read request is routed to the appropriate nodes.</li>
<li><strong>Bloom Filter</strong>: Checks if the SSTable might contain the requested row.</li>
<li><strong>Key Cache</strong>: Quickly locates the row key in the SSTable.</li>
<li><strong>Row Cache</strong>: Caches the entire row to speed up frequent queries.</li>
<li><strong>Memtable and SSTable</strong>: Data is read from Memtables and SSTables, and results are merged.</li>
</ul>


<a name="Fault-Tolerance"></a>
<h4>Fault Tolerance</h4>

<p>Cassandra is designed to be highly fault-tolerant:</p>

<ul>
<li><strong>Data Replication</strong>: Multiple copies of data are stored across different nodes.</li>
<li><strong>Hinted Handoff</strong>: If a node is down, writes are stored temporarily on another node and delivered when the target node is available.</li>
<li><strong>Read Repair</strong>: During reads, inconsistencies are repaired by comparing data across replicas.</li>
<li><strong>Anti-Entropy Repair</strong>: Regularly scheduled repairs ensure all replicas are consistent.</li>
</ul>


<a name="Diagram"></a>
<h3>Diagram</h3>

<p>Here’s a simplified diagram of Cassandra’s architecture:</p>

<pre><code>                        +-----------------------------+
                        |         Cassandra Cluster   |
                        +-----------------------------+
                                      |
      +-------------------------------+-------------------------------+
      |                               |                               |
+------------+                 +------------+                 +------------+
| Data Center 1 |               | Data Center 2 |               | Data Center 3 |
+------------+                 +------------+                 +------------+
      |                               |                               |
+------+--------+          +------+--------+          +------+--------+
| Node 1       |          | Node 1       |          | Node 1       |
+--------------+          +--------------+          +--------------+
| Commit Log   |          | Commit Log   |          | Commit Log   |
| Memtable     |          | Memtable     |          | Memtable     |
| SSTable      |          | SSTable      |          | SSTable      |
+--------------+          +--------------+          +--------------+
</code></pre>

<p>To illustrate, let&rsquo;s consider setting up a Cassandra cluster and creating a keyspace</p>

<a name="Configuration"></a>
<h4>Configuration</h4>

<p><strong>cassandra.yaml</strong>: The primary configuration file for Cassandra.</p>

<pre><code class="yaml">cluster_name: 'MyCluster'
num_tokens: 256
seed_provider:
  - class_name: org.apache.cassandra.locator.SimpleSeedProvider
    parameters:
      - seeds: "127.0.0.1"

storage_port: 7000
listen_address: localhost

rpc_port: 9042
rpc_address: localhost

endpoint_snitch: SimpleSnitch
</code></pre>

<a name="Keyspace-and-Table-Creation"></a>
<h4>Keyspace and Table Creation</h4>

<pre><code class="sql">-- Create a keyspace
CREATE KEYSPACE mykeyspace WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': 3
};

-- Use the keyspace
USE mykeyspace;

-- Create a table
CREATE TABLE users (
  user_id UUID PRIMARY KEY,
  name TEXT,
  email TEXT,
  age INT
);
</code></pre>

<a name="Why-is-Cassandra-fast-in-writes-3f-"></a>
<h2>Why is Cassandra fast in writes?</h2>

<a name="Log-2d-Structured-Storage"></a>
<h3>Log-Structured Storage</h3>

<ul>
<li>Cassandra appends write operations to a commit log on disk for durability.</li>
<li>This sequential write pattern minimizes disk seeks and maximizes disk throughput, leading to fast write operations.</li>
</ul>


<pre><code class="java">// Insert data into Cassandra using CQL (Cassandra Query Language)
session.execute("INSERT INTO users (user_id, name, email, age) VALUES (uuid(), 'Alice', 'alice@example.com', 30)");
</code></pre>

<a name="In-2d-Memory-Write-Path"></a>
<h3>In-Memory Write Path</h3>

<ul>
<li>Write operations are stored in an in-memory structure called the Memtable.</li>
<li>Memtables are flushed to disk periodically or when they reach a certain size threshold.</li>
<li>Buffering writes in memory before flushing them to disk speeds up write operations.</li>
</ul>


<pre><code class="java">// Insert data into Cassandra using CQL
session.execute("INSERT INTO users (user_id, name, email, age) VALUES (uuid(), 'Alice', 'alice@example.com', 30)");
</code></pre>

<a name="Multi-2d-Threaded-Architecture"></a>
<h3>Multi-Threaded Architecture</h3>

<ul>
<li>Cassandra&rsquo;s architecture allows for parallel processing of writes across multiple threads and cores.</li>
<li>Each node in the Cassandra cluster can handle multiple concurrent writes, maximizing hardware resources utilization.</li>
</ul>


<pre><code class="java">// Insert data into Cassandra using multiple threads
ExecutorService executor = Executors.newFixedThreadPool(10);
for (int i = 0; i &lt; 1000; i++) {
    executor.submit(() -&gt; session.execute("INSERT INTO users (user_id, name, email, age) VALUES (uuid(), 'Alice', 'alice@example.com', 30)"));
}
executor.shutdown();
</code></pre>

<a name="Distributed-Writes"></a>
<h3>Distributed Writes</h3>

<ul>
<li>Cassandra distributes data across multiple nodes using consistent hashing.</li>
<li>Write operations are replicated to multiple nodes based on the configured replication factor.</li>
<li>This distributed nature allows Cassandra to scale horizontally, handling write-heavy workloads with ease.</li>
</ul>


<pre><code class="java">// Insert data into Cassandra with replication factor
session.execute("INSERT INTO users (user_id, name, email, age) VALUES (uuid(), 'Alice', 'alice@example.com', 30)")
        .setConsistencyLevel(DefaultConsistencyLevel.ALL);
</code></pre>

<a name="Tunable-Consistency"></a>
<h3>Tunable Consistency</h3>

<ul>
<li>Cassandra allows for tunable consistency levels for write operations.</li>
<li>Clients can choose the level of consistency required for each write operation, balancing consistency and latency.</li>
</ul>


<pre><code class="java">// Insert data into Cassandra with tunable consistency level
session.execute("INSERT INTO users (user_id, name, email, age) VALUES (uuid(), 'Alice', 'alice@example.com', 30)")
        .setConsistencyLevel(DefaultConsistencyLevel.QUORUM);
</code></pre>

<a name="No-Single-Point-of-Bottleneck"></a>
<h3>No Single Point of Bottleneck</h3>

<ul>
<li>Cassandra&rsquo;s decentralized architecture ensures no single point of bottleneck for writes.</li>
<li>Each node in the cluster can independently process write operations, leading to linear scalability.</li>
</ul>


<pre><code class="java">// Insert data into Cassandra on multiple nodes in the cluster
session.execute("INSERT INTO users (user_id, name, email, age) VALUES (uuid(), 'Alice', 'alice@example.com', 30)")
        .setConsistencyLevel(DefaultConsistencyLevel.LOCAL_QUORUM);
</code></pre>

<a name="Conclusion"></a>
<h3>Conclusion</h3>

<p>Cassandra&rsquo;s fast write performance is achieved through a combination of log-structured storage, in-memory write buffering, multi-threaded architecture, distributed writes, tunable consistency, and decentralized design. By leveraging these design principles and using appropriate configuration options, Cassandra can handle high-throughput write workloads efficiently, making it an ideal choice for applications that require fast ingestion of large volumes of data.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Vector Database: Transforming Data Storage and Retrieval in the AI Era]]></title>
        <link href="https://rishijeet.github.io/blog/vector-database-transforming-data-storage-and-retrieval-in-the-ai-era/"/>
        <updated>2023-11-05T22:09:04+05:30</updated>
        <id>https://rishijeet.github.io/blog/vector-database-transforming-data-storage-and-retrieval-in-the-ai-era</id>
        <content type="html"><![CDATA[<p>The AI revolution has ushered in a new era of innovation, promising breakthroughs across various industries. However, with these advancements come unique challenges, particularly in handling and processing data efficiently. One of the key data types that have gained prominence in AI applications is vector embeddings. Vector databases play a pivotal role in managing and optimizing the retrieval of these embeddings. In this article, we will explore the architecture of vector databases and their crucial role in AI applications.</p>

<a name="What-is-a-Vector-Database-3f-"></a>
<h2>What is a Vector Database?</h2>

<p>A vector database is a specialized database designed to index and store vector embeddings for efficient retrieval and similarity search. These databases offer not only CRUD (Create, Read, Update, Delete) operations but also advanced capabilities like metadata filtering and horizontal scaling. They are essential for AI applications that rely on vector embeddings to understand patterns, relationships, and underlying structures in data.</p>

<p><img src="/images/vector_db2.png" height="300" width="900" alt="Alt text" /><em>Source: Elastic</em></p>

<a name="Vector-Embeddings"></a>
<h3>Vector Embeddings</h3>

<p>Vector embeddings are data representations generated by AI models, such as large language models. They encapsulate semantic information critical for AI to understand and perform complex tasks effectively. These embeddings have multiple attributes or features, making their management a unique challenge.</p>

<p>Traditional scalar-based databases struggle to handle the complexity and scale of vector data, hindering real-time analysis and insights extraction. Vector databases are tailored to address these limitations, providing the performance, scalability, and flexibility needed for extracting valuable insights from vector embeddings.</p>

<!-- more -->


<a name="Vector-Database-Architecture"></a>
<h2>Vector Database Architecture</h2>

<p>Traditional databases store scalar data in rows and columns, whereas vector databases operate on vectors. They differ in the way data is optimized and queried.</p>

<p>In traditional databases, queries typically seek exact matches between query values and database records. In vector databases, similarity metrics are applied to find vectors that are most similar to the query. Vector databases employ a combination of algorithms to enable Approximate Nearest Neighbor (ANN) searches, which optimize retrieval speed.</p>

<a name="Vector-Database-Pipeline"></a>
<h3>Vector Database Pipeline</h3>

<p>A typical vector database pipeline consists of the following stages:</p>

<p><img src="/images/vector_db_pipeline.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<ol>
<li><p><strong>Indexing</strong>: The database indexes vectors using algorithms like PQ (Product Quantization), LSH (Locality-Sensitive Hashing), or HNSW (Hierarchical Navigable Small World). This step maps vectors to data structures for faster searching.</p></li>
<li><p><strong>Querying</strong>: The database compares the indexed query vector to the indexed vectors in the dataset to find the nearest neighbors, applying a similarity metric used by the index.</p></li>
<li><p><strong>Post Processing</strong>: In some cases, the database retrieves the final nearest neighbors from the dataset and may post-process them, such as re-ranking them using a different similarity measure.</p></li>
</ol>


<a name="Algorithms-for-Vector-Indexing"></a>
<h3>Algorithms for Vector Indexing</h3>

<p>Vector databases rely on various algorithms to create efficient indexes for high-dimensional vector embeddings. These algorithms are designed to transform the original vector data into a compressed form, optimizing the query process for faster retrieval.</p>

<a name="Random-Projection"></a>
<h4>Random Projection</h4>

<p>Random projection is a technique that aims to project high-dimensional vectors into a lower-dimensional space using a random projection matrix. Here&rsquo;s how it works:</p>

<ul>
<li><p><strong>Projection Matrix Creation</strong>: A matrix of random numbers is created with the target lower-dimensional value. This matrix is then used to calculate the dot product of input vectors, resulting in a projected matrix that has fewer dimensions but still preserves vector similarity.</p></li>
<li><p><strong>Query Process</strong>: When a query is executed, the same projection matrix is used to project the query vector into the lower-dimensional space. The projected query vector is then compared to the projected vectors in the database to find the nearest neighbors. The reduced dimensionality of the data speeds up the search process.</p></li>
</ul>


<p>It&rsquo;s essential to note that random projection is an approximate method, and the quality of the projection depends on the properties of the projection matrix. Generating a truly random projection matrix can be computationally expensive, especially for large datasets.
<img src="/images/vector_rp.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<a name="Product-Quantization"></a>
<h4>Product Quantization</h4>

<p>Product quantization (PQ) is a lossy compression technique tailored for high-dimensional vectors, such as vector embeddings. The process involves splitting, training, encoding, and querying:</p>

<ul>
<li><p><strong>Splitting</strong>: Vectors are divided into segments.</p></li>
<li><p><strong>Training</strong>: A &ldquo;codebook&rdquo; is created for each segment, representing potential codes for vectors. The codebook is established by performing k-means clustering on each segment, resulting in center points that serve as codes.</p></li>
<li><p><strong>Encoding</strong>: Each vector segment is assigned a specific code from the codebook, typically the nearest value. Multiple PQ codes can represent a segment.</p></li>
<li><p><strong>Query Process</strong>: During querying, vectors are broken down into sub-vectors and quantized using the codebook. The indexed codes are then used to find the nearest vectors to the query vector.</p></li>
</ul>


<p>The number of representative vectors in the codebook involves a trade-off between representation accuracy and computational cost. A larger codebook improves accuracy but increases computational expenses.
<img src="/images/vector_pq.png" height="300" width="900" alt="Alt text" /><em>Source: Towards Data Science</em></p>

<a name="Locality-2d-Sensitive-Hashing--28-LSH-29-"></a>
<h4>Locality-Sensitive Hashing (LSH)</h4>

<p>Locality-sensitive hashing (LSH) is optimized for approximate nearest-neighbor search. LSH maps similar vectors into &ldquo;buckets&rdquo; using a set of hashing functions:</p>

<ul>
<li><p><strong>Indexing</strong>: Similar vectors are grouped into hash tables using the hashing functions.</p></li>
<li><p><strong>Query Process</strong>: To find the nearest neighbors for a query vector, the same hashing functions are used to map the query vector to a specific table. The query vector is then compared with the vectors in that table to find the closest matches. This method accelerates searching by reducing the number of vectors to consider.</p></li>
</ul>


<p>LSH is an approximate method, and the quality of the approximation depends on the properties of the hash functions. Using more hash functions improves approximation quality but can be computationally expensive, especially for large datasets.
<img src="/images/vector_lsh.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<a name="Hierarchical-Navigable-Small-World--28-HNSW-29-"></a>
<h4>Hierarchical Navigable Small World (HNSW)</h4>

<p>HNSW creates a hierarchical, tree-like structure where each node represents a set of vectors, and edges indicate similarity between vectors. The algorithm follows these steps:</p>

<ul>
<li><p><strong>Node Creation</strong>: A set of nodes is established, each containing a small number of vectors. Nodes can be created randomly or by clustering vectors using algorithms like k-means.</p></li>
<li><p><strong>Edge Formation</strong>: The algorithm examines the vectors within each node and establishes edges between the node and those nodes containing the most similar vectors.</p></li>
<li><p><strong>Query Process</strong>: When querying an HNSW index, the algorithm navigates the hierarchical structure, visiting nodes that are likely to contain the closest vectors to the query vector.</p></li>
</ul>


<p><img src="/images/vector_hnsw.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<a name="Similarity-Measures"></a>
<h3>Similarity Measures</h3>

<p>The choice of similarity measure plays a crucial role in vector database performance. Common similarity measures include:</p>

<ul>
<li><p><strong>Cosine Similarity</strong>: Measures the cosine of the angle between two vectors, with a range from -1 to 1. It signifies the degree of similarity between vectors.</p></li>
<li><p><strong>Euclidean Distance</strong>: Measures the straight-line distance between vectors in a vector space, with a range from 0 to infinity.</p></li>
<li><p><strong>Dot Product</strong>: Measures the product of the magnitudes of two vectors and the cosine of the angle between them, with a range from -∞ to ∞.</p></li>
</ul>


<p>The selection of the appropriate similarity measure depends on the specific use case and requirements.</p>

<a name="Filtering"></a>
<h3>Filtering</h3>

<p>Each vector stored in the database includes associated metadata. Vector databases can filter query results based on metadata queries, typically maintaining both vector and metadata indexes. The filtering process can occur before or after the vector search, with trade-offs in terms of efficiency.</p>

<ul>
<li><p><strong>Pre-filtering</strong>: Filters are applied before the vector search. While reducing the search space, it may exclude relevant results not meeting metadata filter criteria and add computational overhead.</p></li>
<li><p><strong>Post-filtering</strong>: Filters are applied after the vector search. This ensures all relevant results are considered but may introduce additional processing overhead.</p></li>
</ul>


<p><img src="/images/vector_filter.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<p>Optimizing the filtering process involves techniques like advanced indexing for metadata and parallel processing to balance performance and accuracy.</p>

<a name="Vector-Database-vs.-Vector-Index"></a>
<h3>Vector Database vs. Vector Index</h3>

<p>While standalone vector indices like FAISS (Facebook AI Similarity Search) can enhance the search and retrieval of vector embeddings, they lack essential database features. Vector databases offer several advantages over standalone vector indices, including:</p>

<ol>
<li><p><strong>Data Management</strong>: Vector databases provide traditional database features for easy data management, such as insertion, deletion, and updating. This simplifies vector data management compared to standalone vector indices like FAISS, which require additional integration with storage solutions.</p></li>
<li><p><strong>Metadata Storage and Filtering</strong>: Vector databases can store metadata associated with each vector entry. Users can query the database using additional metadata filters for more granular queries.</p></li>
<li><p><strong>Scalability</strong>: Vector databases are designed for scalability, supporting the growth of data volumes and user demands. They excel in distributed and parallel processing, while standalone vector indices may require custom solutions for similar scalability.</p></li>
<li><p><strong>Real-time Updates</strong>: Vector databases often support real-time data updates, allowing dynamic changes to the data. Standalone vector indexes may require time-consuming and computationally expensive full re-indexing to incorporate new data.</p></li>
<li><p><strong>Backups and Collections</strong>: Vector databases handle data backup operations, and users can selectively choose specific indexes to back up in the form of &ldquo;collections.&rdquo; This feature ensures data resilience and retrievability.</p></li>
<li><p><strong>Ecosystem Integration</strong>: Vector databases can seamlessly integrate with other components of a data processing ecosystem, streamlining data management workflows. This integration extends to AI-related tools, fostering a comprehensive ecosystem for AI applications.</p></li>
<li><p><strong>Data Security and Access Control</strong>: Vector databases typically include built-in data security features and access control mechanisms to safeguard sensitive information. These security measures may not be available in standalone vector index solutions.</p></li>
</ol>


<p>In summary, vector databases are purpose-built for managing vector embeddings, addressing the limitations of standalone vector indices and offering a more effective and streamlined data management experience.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>In the age of AI, efficient data processing and retrieval are paramount for applications relying on vector embeddings. Vector databases are purpose-built to handle these complex data types, offering advanced capabilities for storage, retrieval, and analysis. Understanding the architecture and capabilities of vector databases empowers organizations to unlock the full potential of their AI applications, gaining a competitive edge in the rapidly evolving AI landscape.</p>
]]></content>
    </entry>
    
</feed>
