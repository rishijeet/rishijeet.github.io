<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: ai | Rishijeet Mishra | Technologist | Tech Trends & Development Blog]]></title>
    <link href="https://rishijeet.github.io/blog/categories/ai/atom.xml" rel="self"/>
    <link href="https://rishijeet.github.io/"/>
    <updated>2025-10-19T10:37:19+05:30</updated>
    <id>https://rishijeet.github.io/</id>
    <author>
        <name><![CDATA[Rishijeet Mishra]]></name>
        <email><![CDATA[rishijeet@gmail.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[The $1.5 Trillion Question: Is AI Investment a Bubble or the Future?]]></title>
        <link href="https://rishijeet.github.io/blog/the-trillion-dollar-question-is-ai-investment-a-bubble-or-the-future/"/>
        <updated>2025-10-19T10:16:24+05:30</updated>
        <id>https://rishijeet.github.io/blog/the-trillion-dollar-question-is-ai-investment-a-bubble-or-the-future</id>
        <content type="html"><![CDATA[<p>The world is witnessing an investment phenomenon unlike anything since the dot-com boom. In 2024 alone, artificial intelligence companies attracted over $100 billion in venture capital funding, while semiconductor manufacturing has seen commitments exceeding $630 billion. Tech giants are pouring unprecedented sums into AI infrastructure, with some analysts now questioning whether this represents visionary transformation or dangerous overinvestment. The answer may determine the trajectory of the global economy for the next decade.</p>

<a name="The-Numbers-Don-27-t-Lie:-A-Historic-Investment-Surge"></a>
<h2>The Numbers Don&rsquo;t Lie: A Historic Investment Surge</h2>

<a name="AI-Funding-Reaches-Stratospheric-Heights"></a>
<h3>AI Funding Reaches Stratospheric Heights</h3>

<p>The scale of AI investment in 2024-2025 defies historical precedent:</p>

<ul>
<li><strong>Global AI VC funding in 2024</strong>: $110 billion (up 80% from $55.6 billion in 2023)</li>
<li><strong>Generative AI funding alone</strong>: $45 billion (nearly double 2023&rsquo;s $24 billion)</li>
<li><strong>2025 trajectory</strong>: Through August, AI startups raised $118 billion, on pace to exceed 2024&rsquo;s record</li>
<li><strong>Market concentration</strong>: AI captured 33% of all global venture funding in 2024</li>
</ul>


<p>To put this in perspective, AI investment in 2024 represented the highest funding year for the sector in the past decade, surpassing even the peak global funding levels of 2021. The late-stage deal sizes tell an even more dramatic story: average valuations jumped from $48 million in 2023 to $327 million in 2024 for generative AI companies.</p>

<!--more-->


<a name="The-Mega-2d-Deals-Reshaping-the-Landscape"></a>
<h3>The Mega-Deals Reshaping the Landscape</h3>

<p>Several landmark investments have captured headlines and capital:</p>

<p><strong>OpenAI&rsquo;s Meteoric Rise</strong></p>

<ul>
<li>October 2024: $6.6 billion at $157 billion valuation</li>
<li>March 2025: $40 billion round pushing valuation to $300 billion</li>
<li>August 2025: Additional $8.3 billion (5x oversubscribed)</li>
<li>Microsoft&rsquo;s total investment: $14 billion for 49% profit share</li>
<li>Current status: Most valuable private company globally at $500 billion valuation</li>
</ul>


<p><strong>Other Notable Rounds</strong></p>

<ul>
<li>Databricks: $10 billion at $62 billion valuation (largest VC raise of 2024)</li>
<li>Anthropic: Discussions for $2 billion at $60 billion valuation</li>
<li>Perplexity AI: $500 million at $9 billion valuation</li>
<li>Safe Superintelligence: $1 billion funding round</li>
<li>Scale AI: $1 billion raise</li>
</ul>


<a name="The-Semiconductor-Infrastructure-Boom"></a>
<h3>The Semiconductor Infrastructure Boom</h3>

<p>Parallel to AI software investments, chip manufacturing is experiencing its own renaissance driven by the CHIPS and Science Act and global competition:</p>

<p><strong>U.S. Government Investment</strong></p>

<ul>
<li>Total CHIPS Act funding: $52.7 billion over five years</li>
<li>Manufacturing incentives: $39 billion</li>
<li>R&amp;D programs: $11 billion</li>
<li>Awards announced: $33.7 billion in grants, $28.8 billion in loans to 32 projects across 20 states</li>
</ul>


<p><strong>Major Awards Include</strong>:</p>

<ul>
<li>Intel: $7.86 billion (supporting $100 billion investment plan)</li>
<li>TSMC: Billions for Arizona fabrication facilities</li>
<li>Micron: Major funding for New York operations</li>
<li>Samsung: Substantial incentives for Texas operations</li>
<li>Hemlock Semiconductor: $325 million for polysilicon production</li>
</ul>


<p><strong>Private Sector Response</strong></p>

<p>Companies have announced over $630 billion in semiconductor supply chain investments since the CHIPS Act passed, spanning 130+ projects across 28 states. This represents one of the largest industrial mobilizations in U.S. history.</p>

<a name="The-Cross-2d-Investment-Web:-A-New-Financial-Ecosystem"></a>
<h2>The Cross-Investment Web: A New Financial Ecosystem</h2>

<p>Perhaps the most striking feature of the current AI boom is the intricate web of cross-investments among major players, creating what some call &ldquo;circular funding.&rdquo;</p>

<p><img src="/images/2025/ai_circular_funding.jpg" height="300" width="600" alt="Alt text" /></p>

<a name="The-Magnificent-Seven-27-s-AI-Arms-Race"></a>
<h3>The Magnificent Seven&rsquo;s AI Arms Race</h3>

<p>The tech giants—Microsoft, Amazon, Google (Alphabet), Meta, Apple, Nvidia, and Tesla—are engaged in an unprecedented capital expenditure competition:</p>

<p><strong>2025 Capex Commitments</strong>:</p>

<ul>
<li><strong>Amazon</strong>: $105 billion (including $26 billion in Q4 2024 alone)</li>
<li><strong>Microsoft</strong>: $80 billion for fiscal 2025</li>
<li><strong>Alphabet/Google</strong>: $75 billion (up 43% year-over-year)</li>
<li><strong>Meta</strong>: $64-72 billion (raised multiple times through 2025)</li>
<li><strong>Oracle</strong>: Aggressive data center expansion for AI infrastructure</li>
</ul>


<p>These five companies alone account for over $400 billion in AI-related capital expenditures, representing the largest concentration of capital deployment in tech history.</p>

<a name="The-Circular-Investment-Phenomenon"></a>
<h3>The Circular Investment Phenomenon</h3>

<p>The investment relationships create a complex, interconnected network:</p>

<p><strong>Nvidia&rsquo;s Strategic Position</strong>:</p>

<ul>
<li>Investing $100 billion in OpenAI</li>
<li>Holds equity stakes in CoreWeave (AI cloud computing)</li>
<li>Completed 50+ venture capital deals in 2024</li>
<li>Invests in startups that then purchase its chips</li>
</ul>


<p><strong>Microsoft&rsquo;s Multi-Layered Approach</strong>:</p>

<ul>
<li>$14 billion total investment in OpenAI</li>
<li>Major customer of CoreWeave (20% of Nvidia&rsquo;s revenue comes from Microsoft)</li>
<li>Provides Azure cloud exclusively to OpenAI</li>
<li>Recent $17.4 billion deal with Nebius for AI infrastructure</li>
</ul>


<p><strong>OpenAI&rsquo;s Ecosystem</strong>:</p>

<ul>
<li>Taking 10% stake in AMD</li>
<li>Microsoft as major shareholder and exclusive cloud provider</li>
<li>Partnership with Oracle and SoftBank on $500 billion Stargate Project</li>
<li>Investment from Nvidia, SoftBank, Thrive Capital, Fidelity, Sequoia</li>
</ul>


<p><strong>The Network Effects</strong>:
This creates a self-reinforcing loop: tech giants invest in AI startups → startups use funds to buy chips from Nvidia/AMD → chip makers invest in the same AI companies → those companies rent computing power from Microsoft/Amazon/Google → who then invest more in AI infrastructure.</p>

<a name="The-Economic-Impact:-AI-as-GDP-27-s-New-Engine"></a>
<h2>The Economic Impact: AI as GDP&rsquo;s New Engine</h2>

<a name="AI-27-s-Outsized-Contribution-to-Growth"></a>
<h3>AI&rsquo;s Outsized Contribution to Growth</h3>

<p>The economic data reveals AI&rsquo;s dramatic influence on GDP:</p>

<p><strong>Q1-Q2 2025 Statistics</strong>:</p>

<ul>
<li>AI-related investment contributed <strong>31% of U.S. GDP growth</strong> (up from typical 9-14%)</li>
<li>Investment in information processing equipment &amp; software: only 4% of GDP but responsible for <strong>92% of GDP growth</strong> in H1 2025</li>
<li>Without AI spending, U.S. economy would have grown at just <strong>0.1% annually</strong></li>
<li>AI capex surpassed consumer spending as primary GDP growth driver (1.1% of total growth)</li>
</ul>


<p><strong>Historical Context</strong>:
Harvard economist Jason Furman noted that excluding data center construction and tech infrastructure, the U.S. would have been close to recession in 2025. Deutsche Bank analysis similarly concluded that without AI investment, the economy might already be in recession.</p>

<a name="The-Productivity-Promise-vs.-Reality"></a>
<h3>The Productivity Promise vs. Reality</h3>

<p><strong>Long-term Projections</strong>:</p>

<ul>
<li>Goldman Sachs estimates AI could increase U.S. productivity growth by <strong>1.5 percentage points annually</strong> over 10 years</li>
<li>Penn Wharton Budget Model projects AI&rsquo;s TFP contribution at 0.01 percentage points in 2025, rising to 0.19 by 2032</li>
<li>Potential worldwide GDP boost: <strong>up to 15%</strong> if productivity gains fully materialize</li>
<li>Expected measurable GDP impact: starting 2027 for U.S., 2028 for other economies</li>
</ul>


<p><strong>Current Reality</strong>:</p>

<ul>
<li>Job growth in high-AI-exposure occupations has stagnated</li>
<li>Employment in jobs that can be fully automated by AI fell 0.75% from 2021-2024</li>
<li>Manufacturing sector in recession for 2+ years</li>
<li>Services sector ISM PMI fell to 50 in September 2025 (indicating stagnation)</li>
</ul>


<a name="Red-Flags-or-Growing-Pains-3f--The-Bubble-Debate"></a>
<h2>Red Flags or Growing Pains? The Bubble Debate</h2>

<a name="Warning-Signs-From-Market-Leaders"></a>
<h3>Warning Signs From Market Leaders</h3>

<p>In a remarkable departure from typical executive optimism, several industry titans have publicly voiced concerns:</p>

<p><strong>Direct Warnings</strong>:</p>

<ul>
<li><strong>Goldman Sachs CEO David Solomon</strong>: &ldquo;There will be a lot of capital that was deployed that doesn&rsquo;t deliver returns&rdquo;</li>
<li><strong>Jeff Bezos</strong>: Called the current environment &ldquo;kind of an industrial bubble&rdquo;</li>
<li><strong>Sam Altman (OpenAI CEO)</strong>: &ldquo;People will overinvest and lose money during this phase&rdquo;</li>
<li><strong>Mark Zuckerberg</strong>: Acknowledged &ldquo;an AI bubble is quite possible&rdquo;</li>
</ul>


<p><strong>Yale CEO Survey</strong>: At a June 2025 summit of 150+ CEOs, 40% raised significant concerns about AI overinvestment and believed a correction was imminent.</p>

<a name="Institutional-Warnings-Escalate"></a>
<h3>Institutional Warnings Escalate</h3>

<p><strong>October 2025 Financial Institution Alerts</strong>:</p>

<ul>
<li><strong>Bank of England</strong>: &ldquo;The risk of a sharp market correction has increased&rdquo;</li>
<li><strong>IMF Managing Director</strong>: Warned financial conditions could &ldquo;turn abruptly&rdquo; despite market optimism</li>
<li>Both institutions flagged tech stock prices as potentially overinflated by AI enthusiasm</li>
</ul>


<a name="Comparative-Bubble-Metrics"></a>
<h3>Comparative Bubble Metrics</h3>

<p>The current AI investment wave shows concerning similarities to past bubbles:</p>

<p><strong>Market Concentration</strong>:</p>

<ul>
<li>AI-related stocks: 75% of S&amp;P 500 returns since ChatGPT&rsquo;s launch</li>
<li>80% of earnings growth concentrated in AI companies</li>
<li>90% of capital spending growth from AI-related firms</li>
<li>Tech sector&rsquo;s market cap-to-earnings gap widest since late 2022</li>
</ul>


<p><strong>Valuation Concerns</strong>:</p>

<ul>
<li>AI investment &ldquo;bubble&rdquo; measured at <strong>17x the size</strong> of dot-com frenzy (2000)</li>
<li><strong>4x larger</strong> than subprime mortgage bubble (2007)</li>
<li>Buffett Indicator (stock market value to GDP): <strong>217%</strong> (new record, 2+ standard deviations above trend)</li>
<li>PitchBook data: Nearly two-thirds of U.S. deal value went to AI/ML startups in H1 2025 (up from 23% in 2023)</li>
</ul>


<a name="Key-Differences-from-Dot-2d-Com-Era"></a>
<h3>Key Differences from Dot-Com Era</h3>

<p><strong>Fundamental Strengths</strong>:</p>

<ol>
<li><p><strong>Revenue Generation</strong>: Unlike dot-com companies with minimal revenue, today&rsquo;s AI leaders generate substantial cash flow</p>

<ul>
<li>OpenAI: $12.7 billion projected revenue in 2025 (up from $3.7 billion in 2024)</li>
<li>700 million weekly ChatGPT users</li>
<li>20 million paid subscribers by April 2025</li>
</ul>
</li>
<li><p><strong>Profitable Base Companies</strong>: The Magnificent Seven aren&rsquo;t startups—they&rsquo;re established, profitable giants</p>

<ul>
<li>Meta Q3 2024: $40.6 billion revenue, $15.7 billion profit (up 35% YoY)</li>
<li>Amazon, Microsoft, Google all generating massive positive cash flows</li>
<li>Nvidia: Dominant market position with actual product demand</li>
</ul>
</li>
<li><p><strong>Real Infrastructure</strong>: Unlike purely digital dot-com assets, AI requires physical infrastructure with tangible value</p>

<ul>
<li>Data centers</li>
<li>Semiconductor fabs</li>
<li>Power infrastructure</li>
<li>Even if AI underperforms, assets remain valuable</li>
</ul>
</li>
</ol>


<p><strong>Concerning Similarities</strong>:</p>

<ol>
<li><strong>Valuation Disconnect</strong>: Market values increasingly divorced from near-term profitability</li>
<li><strong>Herd Mentality</strong>: Fear of missing out driving investment decisions</li>
<li><strong>Revenue Concentration</strong>: Success relies on small number of use cases</li>
<li><strong>Long Payback Periods</strong>: OpenAI not expected cash-flow positive until 2029; projects $115 billion cash burn through 2029</li>
</ol>


<a name="The-Cross-2d-Funding-Mechanism:-How-Money-Circulates"></a>
<h2>The Cross-Funding Mechanism: How Money Circulates</h2>

<a name="The-Value-Creation-Chain"></a>
<h3>The Value Creation Chain</h3>

<p>Understanding how investments translate to revenue requires mapping the ecosystem:</p>

<p><strong>Stage 1: Capital Injection</strong></p>

<ul>
<li>VCs and tech giants invest billions in AI startups</li>
<li>Example: OpenAI receives $14 billion from Microsoft</li>
</ul>


<p><strong>Stage 2: Infrastructure Purchases</strong></p>

<ul>
<li>AI companies spend on computing infrastructure</li>
<li>OpenAI/Meta/Google purchase Nvidia GPUs ($40,000+ each)</li>
<li>Nvidia&rsquo;s revenue surges (serving as indirect return to Microsoft)</li>
</ul>


<p><strong>Stage 3: Cloud Services</strong></p>

<ul>
<li>Companies rent cloud computing power</li>
<li>Microsoft Azure hosts OpenAI exclusively</li>
<li>Amazon AWS, Google Cloud compete for enterprise AI workloads</li>
</ul>


<p><strong>Stage 4: Software Monetization</strong></p>

<ul>
<li>AI capabilities integrated into products</li>
<li>Microsoft Copilot subscriptions</li>
<li>ChatGPT Plus subscriptions ($20/month)</li>
<li>Enterprise API access fees</li>
</ul>


<p><strong>Stage 5: Productivity Gains</strong></p>

<ul>
<li>End users theoretically gain efficiency</li>
<li>Cost savings from automation</li>
<li>New product capabilities</li>
<li>Revenue growth from AI-enhanced services</li>
</ul>


<a name="The-Sustainability-Question"></a>
<h3>The Sustainability Question</h3>

<p><strong>Arguments for Sustainability</strong>:</p>

<ol>
<li><strong>Network Effects Lock-In</strong>: First movers establishing dominant market positions</li>
<li><strong>Infrastructure Moats</strong>: Billions in sunk costs create barriers to entry</li>
<li><strong>Actual User Adoption</strong>: ChatGPT&rsquo;s 700 million weekly users demonstrate real demand</li>
<li><strong>Enterprise Integration</strong>: Companies embedding AI into core workflows</li>
<li><strong>Scientific Progress</strong>: Real breakthroughs in protein folding, drug discovery, materials science</li>
</ol>


<p><strong>Arguments Against</strong>:</p>

<ol>
<li><p><strong>Revenue-Investment Gap</strong>: Current revenues don&rsquo;t justify investment levels</p>

<ul>
<li>OpenAI $13B projected 2025 revenue vs. $58B total funding raised</li>
<li>Projected $115B cash burn through 2029</li>
</ul>
</li>
<li><p><strong>Commoditization Risk</strong>: Open-source models (Meta&rsquo;s Llama, Mistral) threaten pricing power</p></li>
<li><p><strong>Uncertain ROI Timeline</strong>: Productivity gains may take decades to materialize fully</p></li>
<li><p><strong>Energy Constraints</strong>: Data center power demands strain grid capacity</p></li>
<li><p><strong>Regulatory Risk</strong>: Governments may restrict AI development or data usage</p></li>
</ol>


<a name="Recession-Suppression-or-Genuine-Growth-3f-"></a>
<h2>Recession Suppression or Genuine Growth?</h2>

<a name="The-Counterfactual:-What-Without-AI-3f-"></a>
<h3>The Counterfactual: What Without AI?</h3>

<p>Economic modeling suggests AI investment is masking underlying weakness:</p>

<p><strong>2025 Economic Indicators Without AI</strong>:</p>

<ul>
<li>GDP growth: ~0.1% (near-recession levels)</li>
<li>Manufacturing: Already in recession for 2+ years</li>
<li>Services sector: Stagnating (ISM PMI at 50)</li>
<li>Labor market: Employment growth at 0.5% annualized (weak)</li>
<li>Trade deficit: Up 31% YoY to $654 billion in first 7 months</li>
</ul>


<p><strong>The Structure of Dependence</strong>:
Investment in information processing equipment, while only 4% of GDP, drove 92% of growth in H1 2025. This creates dangerous concentration:</p>

<ol>
<li>If AI spending slows, GDP contracts sharply</li>
<li>Traditional growth drivers (consumer spending, manufacturing, real estate) are stagnant</li>
<li>Economy has become &ldquo;one big bet on AI&rdquo; (economist Ruchir Sharma)</li>
</ol>


<a name="The-Policy-Dimension"></a>
<h3>The Policy Dimension</h3>

<p><strong>Monetary Policy Implications</strong>:</p>

<ul>
<li>Federal Reserve faces dilemma: AI spending supports growth but may be unsustainable</li>
<li>Interest rates remain elevated, yet AI investment continues (unusually rate-insensitive)</li>
<li>If bubble bursts, Fed has limited ammunition (rates still historically elevated)</li>
</ul>


<p><strong>Fiscal Policy</strong>:</p>

<ul>
<li>CHIPS Act represents industrial policy comeback</li>
<li>$52.7 billion government investment leveraging $630 billion private sector response</li>
<li>Creates jobs but doesn&rsquo;t address underlying productivity challenges</li>
</ul>


<p><strong>The Tariff Complication</strong>:</p>

<ul>
<li>Trump administration tariffs haven&rsquo;t reduced trade deficit as promised</li>
<li>May increase costs for chip manufacturing (imported equipment, materials)</li>
<li>Geopolitical tensions with China accelerating domestic investment urgency</li>
</ul>


<a name="International-Dimensions:-The-Global-AI-Race"></a>
<h2>International Dimensions: The Global AI Race</h2>

<a name="Capital-Flows-and-Regional-Disparities"></a>
<h3>Capital Flows and Regional Disparities</h3>

<p><strong>Geographic Investment Distribution</strong>:</p>

<ul>
<li><strong>United States</strong>: $80.7 billion (42% of global AI VC in 2024)</li>
<li><strong>Europe</strong>: $12.8 billion (25% of regional VC)</li>
<li><strong>China</strong>: $7.6 billion (2024 standout despite restrictions)</li>
<li><strong>Rest of World</strong>: 18% of global AI investment</li>
</ul>


<p><strong>Long-term Investment Trends (2013-2024)</strong>:</p>

<ul>
<li>U.S. private AI investment: $470 billion (nearly 25% in 2024 alone)</li>
<li>China: $119 billion total</li>
<li>This disparity drives national security concerns and competition</li>
</ul>


<a name="The-Semiconductor-Supply-Chain"></a>
<h3>The Semiconductor Supply Chain</h3>

<p><strong>Regional Buildout</strong>:</p>

<ul>
<li>U.S. aims to increase chip production capacity 203% by 2032</li>
<li>Global market share target: 10% → 14% by 2032</li>
<li>Taiwan remains dominant in leading-edge production</li>
<li>Europe investing €43 billion in semiconductor independence</li>
<li>China pursuing aggressive domestic production despite export controls</li>
</ul>


<p><strong>Foreign Investment in U.S.</strong>:</p>

<p>Record $290 billion flowed into U.S. stocks in Q2 2025, with foreigners now owning ~30% of market—highest post-WWII share. This capital inflow helps fund AI buildout but creates vulnerability if sentiment shifts.</p>

<a name="Three-Scenarios:-Where-Do-We-Go-From-Here-3f-"></a>
<h2>Three Scenarios: Where Do We Go From Here?</h2>

<a name="Scenario-1:-Soft-Landing--28-40-25--Probability-29-"></a>
<h3>Scenario 1: Soft Landing (40% Probability)</h3>

<p><strong>What Happens</strong>:</p>

<ul>
<li>AI gradually proves ROI over 5-10 years</li>
<li>Revenue growth catches up to investment levels by 2027-2029</li>
<li>Valuations compress but companies remain viable</li>
<li>Infrastructure investment provides foundation for next generation of applications</li>
<li>Productivity gains materialize gradually, boosting GDP 0.3-0.5% annually</li>
</ul>


<p><strong>Required Conditions</strong>:</p>

<ul>
<li>Breakthrough applications beyond chatbots</li>
<li>Enterprise adoption accelerates</li>
<li>Energy infrastructure scales to meet demand</li>
<li>No major regulatory restrictions</li>
<li>Geopolitical stability</li>
</ul>


<p><strong>Historical Parallel</strong>: Similar to internet&rsquo;s arc—1998-2002 crash followed by 2003-2007 genuine growth as infrastructure found use cases</p>

<a name="Scenario-2:-Hard-Crash--28-35-25--Probability-29-"></a>
<h3>Scenario 2: Hard Crash (35% Probability)</h3>

<p><strong>What Happens</strong>:</p>

<ul>
<li>Revenue growth disappoints vs. expectations</li>
<li>Major AI companies report losses/lower guidance</li>
<li>Investors flee; valuations crash 50-80%</li>
<li>Contagion spreads to broader tech sector</li>
<li>U.S. GDP contracts 2-4% as AI investment evaporates</li>
<li>Recession ensues, potentially severe</li>
</ul>


<p><strong>Triggering Events</strong>:</p>

<ul>
<li>OpenAI or similar company fails to convert users to paying customers at scale</li>
<li>Breakthrough model from unexpected competitor commoditizes current leaders</li>
<li>Energy costs make operations unsustainable</li>
<li>Regulatory crackdown on data usage/AI applications</li>
<li>Nvidia faces sudden demand cliff</li>
</ul>


<p><strong>Historical Parallel</strong>: Dot-com crash (2000-2002) saw NASDAQ fall 78%; many promising companies disappeared entirely</p>

<a name="Scenario-3:-Prolonged-Plateau--28-25-25--Probability-29-"></a>
<h3>Scenario 3: Prolonged Plateau (25% Probability)</h3>

<p><strong>What Happens</strong>:</p>

<ul>
<li>AI capabilities stagnate at current level</li>
<li>Neither crash nor breakthrough—muddling through</li>
<li>Valuations gradually decline over 3-5 years</li>
<li>Capital redirected to other sectors</li>
<li>Moderate recession as growth engine disappears</li>
<li>Infrastructure remains underutilized</li>
</ul>


<p><strong>Outcome</strong>:</p>

<ul>
<li>Similar to 3D printing, blockchain, or VR—promising technology that achieves niche success but not transformative impact anticipated</li>
<li>Investors lose money but not catastrophically</li>
<li>Real economic impact minimal</li>
</ul>


<a name="Investment-Implications-and-Risk-Management"></a>
<h2>Investment Implications and Risk Management</h2>

<a name="For-Individual-Investors"></a>
<h3>For Individual Investors</h3>

<p><strong>Bull Case Positions</strong>:</p>

<ul>
<li><strong>Direct AI Exposure</strong>: Nvidia, Microsoft, Amazon, Google (infrastructure providers with diversified business)</li>
<li><strong>Picks and Shovels</strong>: Semiconductor equipment (ASML), data center REITs, power infrastructure</li>
<li><strong>Defensive AI</strong>: Companies using AI to improve margins in traditional industries</li>
</ul>


<p><strong>Risk Mitigation</strong>:</p>

<ul>
<li>Avoid concentration >20% portfolio in AI-specific names</li>
<li>Focus on profitable companies with strong balance sheets</li>
<li>Set stop losses given volatility</li>
<li>Consider hedging strategies if heavily exposed</li>
</ul>


<p><strong>Bear Case Positions</strong>:</p>

<ul>
<li>Short AI-only companies with no revenue</li>
<li>Long value stocks in underinvested sectors (energy, materials, industrials ex-AI)</li>
<li>Treasury bonds if expecting recession</li>
</ul>


<a name="For-Policymakers"></a>
<h3>For Policymakers</h3>

<p><strong>Recommendations</strong>:</p>

<ul>
<li><strong>Monitor Systemic Risk</strong>: AI investment concentration creates fragility</li>
<li><strong>Maintain Flexible Monetary Policy</strong>: Prepare for potential sudden reversal</li>
<li><strong>Diversify Growth Strategies</strong>: Don&rsquo;t rely solely on AI for economic expansion</li>
<li><strong>Invest in Complementary Infrastructure</strong>: Power grid, workforce training</li>
<li><strong>Balanced Regulation</strong>: Prevent harm without stifling innovation</li>
</ul>


<a name="Conclusion:-Bubble-2c--Vision-2c--or-Both-3f-"></a>
<h2>Conclusion: Bubble, Vision, or Both?</h2>

<p>After analyzing the data, the answer is nuanced: <strong>This is simultaneously a legitimate technological revolution AND a financial bubble.</strong></p>

<a name="The-Case-for-Vision"></a>
<h3>The Case for Vision</h3>

<p><strong>Undeniable Realities</strong>:</p>

<ul>
<li>AI capabilities have advanced dramatically in 2-3 years</li>
<li>Real user adoption (700M weekly ChatGPT users) demonstrates utility</li>
<li>Infrastructure investment creates tangible assets</li>
<li>Scientific breakthroughs (protein folding, drug discovery) validate transformative potential</li>
<li>Leading companies are profitable giants, not dot-com fantasies</li>
</ul>


<a name="The-Case-for-Bubble"></a>
<h3>The Case for Bubble</h3>

<p><strong>Concerning Facts</strong>:</p>

<ul>
<li>Valuations disconnected from current profitability</li>
<li>92% of U.S. GDP growth dependent on single sector</li>
<li>Revenue growth lagging investment by orders of magnitude</li>
<li>Circular funding creating artificial demand</li>
<li>CEO warnings from industry insiders</li>
<li>Market metrics exceeding all previous bubble levels</li>
</ul>


<a name="The-Synthesis"></a>
<h3>The Synthesis</h3>

<p>The most likely outcome is <strong>boom, bust, then genuine transformation</strong>—the classic Gartner Hype Cycle applied to capital markets.</p>

<p><strong>Near-term (2025-2027)</strong>: Continued exuberance and investment, with growing skepticism. Potential correction of 30-50% as reality disappoints initial expectations. Recession risk elevated if correction is severe.</p>

<p><strong>Medium-term (2027-2030)</strong>: Shakeout separates winners from losers. Surviving companies find sustainable business models. Infrastructure built during boom enables new applications. GDP impact becomes measurable as productivity gains materialize.</p>

<p><strong>Long-term (2030+)</strong>: AI becomes embedded infrastructure like electricity or internet. Transformative impact on productivity, healthcare, scientific discovery. Initial investors who survived volatility reap rewards.</p>

<a name="The-Final-Verdict"></a>
<h3>The Final Verdict</h3>

<p>Is this AI investment surge suppressing a U.S. recession? <strong>Yes, absolutely.</strong> Without the AI buildout, economic data suggests the U.S. would likely be in recession already. Manufacturing is contracting, services are stagnant, and consumer spending is weakening.</p>

<p>Does this make AI investment a Ponzi scheme? <strong>No.</strong> Real technology exists, real companies are building real products with real users. Unlike Ponzi schemes, value is being created—the question is whether it justifies the investment level.</p>

<p>Is there a bubble? <strong>Yes.</strong> Valuations are extreme, concentration is dangerous, and expectations are likely inflated. Some (perhaps many) current investments will lose money.</p>

<p>Should you be worried? <strong>It depends.</strong></p>

<ul>
<li><strong>If you&rsquo;re diversified investor</strong>: Moderate exposure to AI winners makes sense; avoid overconcentration</li>
<li><strong>If you&rsquo;re AI-company employee</strong>: Understand your company&rsquo;s burn rate and path to profitability</li>
<li><strong>If you&rsquo;re business leader</strong>: Adopt AI pragmatically; competitive pressure is real but so is implementation risk</li>
<li><strong>If you&rsquo;re concerned citizen</strong>: AI investment is propping up economy but creating fragility; diversified growth strategies needed</li>
</ul>


<p>The AI revolution is real. The bubble is real. Both can be true simultaneously. The trillions being invested will transform something—the question is whether it transforms society or just balance sheets.</p>

<p>The next 2-3 years will tell us which vision prevails.</p>

<blockquote><p>Data compiled from Crunchbase, Dealroom, Goldman Sachs Research, Penn Wharton Budget Model, Bloomberg, Federal Reserve, Department of Commerce, and company financial filings. Analysis current as of October 19, 2025.</p></blockquote>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Attention Is All You Need: The Paper That Revolutionized AI]]></title>
        <link href="https://rishijeet.github.io/blog/attention-is-all-you-need-the-paper-that-revolutionized-ai/"/>
        <updated>2025-10-11T16:26:05+05:30</updated>
        <id>https://rishijeet.github.io/blog/attention-is-all-you-need-the-paper-that-revolutionized-ai</id>
        <content type="html"><![CDATA[<p>In June 2017, eight researchers from Google Brain and Google Research published a paper that would fundamentally reshape artificial intelligence. Titled &ldquo;Attention Is All You Need,&rdquo; it introduced the Transformer architecture—a model that discarded the conventional wisdom of sequence processing and replaced it with something elegantly simple: pure attention.</p>

<p>The numbers tell the story. As of 2025, this single paper has been cited over 173,000 times, making it one of the most influential works in machine learning history. Today, nearly every large language model you interact with—ChatGPT, Google Gemini, Claude, Meta&rsquo;s Llama—traces its lineage directly back to this architecture.</p>

<p>But here&rsquo;s what makes this achievement remarkable: it wasn&rsquo;t about adding more layers, more parameters, or more complexity. It was about removing what had been considered essential for decades.</p>

<a name="The-Problem:-Sequential-Processing"></a>
<h2>The Problem: Sequential Processing</h2>

<a name="Why-RNNs-Were-Dominant--28-And-Problematic-29-"></a>
<h3>Why RNNs Were Dominant (And Problematic)</h3>

<p>Before 2017, the dominant approach for sequence tasks used Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs). The idea was intuitive: process sequences one element at a time, maintaining a hidden state that captures information from previous steps.</p>

<p>Think of it like reading a book word by word, keeping a mental summary as you go.</p>

<p><strong>The Fundamental Bottleneck</strong>: RNNs have an inherent constraint—they must process sequentially. The output at step t depends on the hidden state h_t, which depends on the previous state h<em>{t-1}, which depends on h</em>{t-2}, and so on. This creates an unbreakable chain.</p>

<p>From the paper:</p>

<blockquote><p>&ldquo;Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t.&rdquo;</p></blockquote>

<!--more-->


<p><strong>What this meant in practice:</strong></p>

<ul>
<li><strong>Training Speed</strong>: On a typical machine translation task with 4.5 million sentence pairs (WMT 2014 English-German), even the best RNN models took 5-6 days to train on powerful hardware.</li>
<li><strong>Parallelization Nightmare</strong>: You cannot process word 10 before processing word 9, even if you have 1,000 GPUs. It&rsquo;s like a single-lane highway—no matter how many resources you add, the throughput is limited by the sequential dependency.</li>
<li><strong>Memory Dilution</strong>: The longer your sequence, the harder it is for the model to remember important information from the beginning. This is called the &ldquo;vanishing gradient problem.&rdquo; Information from position 1 gets progressively diluted by the time you reach position 100.</li>
<li><strong>Long-Range Dependencies</strong>: If you want to understand how word 2 relates to word 95, the signal has to travel through 93 intermediate steps. Each step is an opportunity for information loss.</li>
</ul>


<a name="Attempted-Solutions-Before-Transformers"></a>
<h3>Attempted Solutions Before Transformers</h3>

<p>Researchers had tried other approaches:</p>

<p><strong>Convolutional Sequence-to-Sequence (ConvS2S)</strong>: Used convolutional networks instead of RNNs. Problem: To connect distant positions, you need O(n/k) convolutional layers (where n is sequence length and k is kernel size). This means more layers and longer &ldquo;path lengths&rdquo; for information to travel.
<strong>Extended Neural GPU &amp; ByteNet</strong>: Other convolutional approaches with similar trade-offs.</p>

<p>The paper notes these approaches attempted to parallelize, but all had significant limitations. ConvS2S was faster than RNNs, but still limited. And none could match the quality of attention-augmented RNNs.</p>

<a name="The-Solution:-Self-2d-Attention"></a>
<h2>The Solution: Self-Attention</h2>

<a name="What-is-Attention-3f-"></a>
<h3>What is Attention?</h3>

<p>Imagine you&rsquo;re in a crowded coffee shop. Many conversations are happening, but you can focus your listening attention on one person. You&rsquo;re not ignoring others, but you&rsquo;re weighting your perception toward one source.</p>

<p>In neural networks, attention is a mechanism that learns which parts of the input are most important for the task at hand. It&rsquo;s learned, dynamic, and task-specific.</p>

<p>The paper defines it simply:</p>

<blockquote><p>&ldquo;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.&rdquo;</p></blockquote>

<p><img src="/images/2025/single-multi-head.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Scaled-Dot-2d-Product-Attention:-The-Core-Formula"></a>
<h3>Scaled Dot-Product Attention: The Core Formula</h3>

<p>The paper introduces &ldquo;Scaled Dot-Product Attention,&rdquo; which is the building block of everything:</p>

<pre><code>Attention(Q, K, V) = softmax(QK^T / √d_k) V
</code></pre>

<p>Let&rsquo;s break this down step by step:</p>

<p><strong>Step 1: Inputs</strong></p>

<ul>
<li><strong>Q (Query)</strong>: &ldquo;What am I looking for?&rdquo; Shape: (batch, seq_len, d_k)</li>
<li><strong>K (Key)</strong>: &ldquo;What could I match?&rdquo; Shape: (batch, seq_len, d_k)</li>
<li><strong>V (Value)</strong>: &ldquo;What information do I contain?&rdquo; Shape: (batch, seq_len, d_v)</li>
</ul>


<p>For self-attention, all three come from the same source (the output of the previous layer).</p>

<p><strong>Step 2: Compute Compatibility Scores</strong>
<code>QK^T</code> produces a matrix showing how much each query relates to each key. If you have a sequence of 10 words, this creates a 10×10 matrix.</p>

<p><strong>Step 3: Scaling</strong>
The scores are divided by √d_k. Why? The paper explains:</p>

<blockquote><p>&ldquo;While for small values of d_k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_k. We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.&rdquo;</p></blockquote>

<p>In practice, they use d_k = 64, so the scaling factor is 1/√64 = 1/8.</p>

<p><strong>Step 4: Softmax for Weights</strong>
<code>softmax()</code> converts the scores into weights that sum to 1. Each weight represents &ldquo;how much attention to pay&rdquo; to each position.</p>

<p><strong>Step 5: Combine Values</strong>
Multiply by V and sum. Positions with high attention weights contribute more to the output.</p>

<p><strong>The Beauty</strong>: All n words can be processed in parallel. Position 10 doesn&rsquo;t wait for position 9.</p>

<a name="Why-Multiple-Heads-Matter"></a>
<h3>Why Multiple Heads Matter</h3>

<p>Here&rsquo;s where it gets powerful. The paper found that a single attention function isn&rsquo;t enough. They introduced <strong>Multi-Head Attention</strong>:</p>

<pre><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O

where head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
</code></pre>

<p>What does this mean? The model learns <strong>h different linear projections</strong> of Q, K, and V, performs attention on each separately, and concatenates the results.</p>

<p><strong>From the paper:</strong></p>

<blockquote><p>&ldquo;Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.&rdquo;</p></blockquote>

<p>In the original Transformer:</p>

<ul>
<li><strong>h = 8</strong> parallel attention heads</li>
<li><strong>d_k = d_v = 512/8 = 64</strong> for each head</li>
<li>Total computation is similar to single-head attention, but you get 8 different learned representations</li>
</ul>


<p><strong>Practical interpretation</strong>: Different heads learn different patterns:</p>

<ul>
<li>Head 1 might focus on verb-object relationships</li>
<li>Head 2 might focus on adjective-noun relationships</li>
<li>Head 3 might track pronouns back to their referents</li>
<li>And so on&hellip;</li>
</ul>


<p>The attention visualizations in the paper (Figures 3-5) show this beautifully. Head 5 in layer 5 learns to capture the phrase &ldquo;making&hellip;more difficult&rdquo; by connecting distant words. Other heads perform anaphora resolution (connecting &ldquo;its&rdquo; to &ldquo;Law&rdquo;).</p>

<a name="Transformer-Architecture"></a>
<h2>Transformer Architecture</h2>

<a name="Overview"></a>
<h3>Overview</h3>

<p>The Transformer has a classic encoder-decoder structure:</p>

<ul>
<li><strong>Encoder</strong>: Transforms input sequence into rich representations</li>
<li><strong>Decoder</strong>: Generates output sequence using encoder representations</li>
</ul>


<p><img src="/images/2025/attention_arch.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>But unlike previous encoder-decoder models, it uses <em>only</em> attention (and feed-forward networks) for both.</p>

<a name="The-Encoder:-6-Identical-Layers"></a>
<h3>The Encoder: 6 Identical Layers</h3>

<p>Each encoder layer contains:</p>

<ul>
<li><strong>Multi-Head Self-Attention</strong>: All 8 heads process simultaneously</li>
<li><strong>Feed-Forward Network</strong>: Applied to each position separately</li>
<li><strong>Residual Connections &amp; Layer Normalization</strong> around each sub-layer</li>
</ul>


<p>The output of each sub-layer is:
<code>
LayerNorm(x + Sublayer(x))
</code></p>

<p><strong>Key specifications</strong>:</p>

<ul>
<li><strong>N = 6</strong> stacked layers</li>
<li><strong>d_model = 512</strong> (dimension of all outputs)</li>
<li><strong>h = 8</strong> attention heads</li>
<li><strong>d_k = d_v = 64</strong> per head</li>
</ul>


<a name="The-Decoder:-6-Identical-Layers-with-Masking"></a>
<h3>The Decoder: 6 Identical Layers with Masking</h3>

<p>The decoder has the same 6 layers, plus one crucial difference:</p>

<ul>
<li><strong>Masked Multi-Head Self-Attention</strong> on decoder positions</li>
<li><strong>Encoder-Decoder Attention</strong>: Queries from decoder, Keys/Values from encoder</li>
<li><strong>Feed-Forward Network</strong></li>
</ul>


<p><strong>The Masking</strong>: The paper states:</p>

<blockquote><p>&ldquo;We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.&rdquo;</p></blockquote>

<p>In practice, this means setting future positions to -∞ in the softmax. This maintains the &ldquo;auto-regressive&rdquo; property—you generate one word at a time, without cheating by looking ahead.</p>

<a name="Position-2d-Wise-Feed-2d-Forward-Networks"></a>
<h3>Position-Wise Feed-Forward Networks</h3>

<p>Between attention layers sits a feed-forward network:</p>

<pre><code>FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
</code></pre>

<p>This is applied to each position separately and identically. Specifications:</p>

<ul>
<li><strong>d_model = 512</strong> (input/output dimension)</li>
<li><strong>d_ff = 2048</strong> (inner layer dimension)</li>
<li>ReLU activation in between</li>
</ul>


<p>Interestingly, this is equivalent to two 1×1 convolutions.</p>

<a name="Positional-Encoding:-Telling-the-Model-About-Order"></a>
<h3>Positional Encoding: Telling the Model About Order</h3>

<p>Here&rsquo;s a subtle but critical detail: attention mechanisms don&rsquo;t inherently understand word order. &ldquo;Dog bites man&rdquo; and &ldquo;man bites dog&rdquo; would be processed the same without additional information.</p>

<p>The solution: <strong>Positional Encodings</strong> (sinusoidal):</p>

<pre><code>PE_(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE_(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
</code></pre>

<p>Where:</p>

<ul>
<li><strong>pos</strong> is the position in the sequence (0, 1, 2, &hellip;)</li>
<li><strong>i</strong> is the dimension index (0 to 255 for d_model=512)</li>
</ul>


<p>Each dimension gets a sinusoid at different frequencies. The wavelengths form a geometric progression from 2π to 10,000·2π.</p>

<p><strong>Why sine and cosine?</strong> The paper hypothesized this allows the model to easily learn relative position offsets since for any fixed offset k, PE<em>{pos+k} can be represented as a linear function of PE</em>{pos}.</p>

<p>The paper tested learned positional embeddings (Table 3, row E) and found nearly identical results, so the sinusoidal choice is more theoretical. But sinusoids have an advantage: they can extrapolate to longer sequences than seen during training.</p>

<a name="The-Three-Applications-of-Attention"></a>
<h3>The Three Applications of Attention</h3>

<p>The paper explicitly lists three ways attention is used:</p>

<ol>
<li><p><strong>Encoder-Decoder Attention</strong>: Queries from decoder layer, Keys/Values from encoder output. Allows each decoder position to see all input positions.</p></li>
<li><p><strong>Encoder Self-Attention</strong>: All of Q, K, V from the same encoder layer. Each position attends to all positions in the previous encoder layer.</p></li>
<li><p><strong>Decoder Self-Attention</strong>: Self-attention with masking. Each position can only attend to previous positions (and itself).</p></li>
</ol>


<a name="Why-Attention-Works-Better"></a>
<h2>Why Attention Works Better</h2>

<p>The paper provides a systematic comparison in Table 1, examining three criteria:</p>

<a name="L1.-Computational-Complexity-Per-Layer"></a>
<h3>1. Computational Complexity Per Layer</h3>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Layer Type</th>
        <th>Complexity</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Self-Attention</td>
        <td>O(n² · d)</td>
      </tr>
      <tr>
        <td>Recurrent</td>
        <td>O(n · d²)</td>
      </tr>
      <tr>
        <td>Convolutional</td>
        <td>O(k · n · d²)</td>
      </tr>
    </tbody>
  </table>
</div>


<p>When sequence length n &lt; representation dimension d (common with word-piece encoding), self-attention is faster than recurrent layers.</p>

<p>For WMT translation tasks using byte-pair encoding:</p>

<ul>
<li><strong>n</strong> (sequence length) ≈ 50-200 tokens</li>
<li><strong>d</strong> (dimension) = 512</li>
</ul>


<p>So n &lt; d, making self-attention win.</p>

<a name="L2.-Sequential-Operations-Required"></a>
<h3>2. Sequential Operations Required</h3>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Layer Type</th>
        <th>Sequential Operations</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Self-Attention</td>
        <td>O(1)</td>
      </tr>
      <tr>
        <td>Recurrent</td>
        <td>O(n)</td>
      </tr>
      <tr>
        <td>Convolutional</td>
        <td>O(1) for normal, O(log<sub>k</sub>(n)) for dilated</td>
      </tr>
    </tbody>
  </table>
</div>


<p>This is the parallelization advantage. Self-attention can process all positions simultaneously. RNNs must process sequentially.</p>

<a name="L3.-Path-Length-for-Long-2d-Range-Dependencies"></a>
<h3>3. Path Length for Long-Range Dependencies</h3>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Layer Type</th>
        <th>Maximum Path Length</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Self-Attention</td>
        <td>O(1)</td>
      </tr>
      <tr>
        <td>Recurrent</td>
        <td>O(n)</td>
      </tr>
      <tr>
        <td>Convolutional</td>
        <td>O(log<sub>k</sub>(n)) or O(n/k)</td>
      </tr>
    </tbody>
  </table>
</div>


<p>This is critical. To learn that position 1 relates to position 100:</p>

<ul>
<li><strong>Self-Attention</strong>: Direct connection in one step</li>
<li><strong>RNN</strong>: Must travel through 99 intermediate steps</li>
<li><strong>CNN</strong>: Must stack multiple layers</li>
</ul>


<p>Shorter paths → easier to learn long-range dependencies.</p>

<a name="Results--26-amp-3b--Impact"></a>
<h2>Results &amp; Impact</h2>

<a name="Machine-Translation-Performance"></a>
<h3>Machine Translation Performance</h3>

<p>The paper evaluated on two benchmarks: WMT 2014 English-German (EN-DE) and English-French (EN-FR).</p>

<p><strong>English-to-German (EN-DE):</strong></p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Model</th>
        <th>BLEU</th>
        <th>Training Cost (FLOPs)</th>
        <th>Training Time</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>GNMT + RL (prev. best)</td>
        <td>24.6</td>
        <td>2.3 × 10<sup>19</sup></td>
        <td>~5 days</td>
      </tr>
      <tr>
        <td>ConvS2S (ensemble)</td>
        <td>26.36</td>
        <td>7.7 × 10<sup>19</sup></td>
        <td>Higher</td>
      </tr>
      <tr>
        <td><strong>Transformer (base)</strong></td>
        <td><strong>27.3</strong></td>
        <td><strong>3.3 × 10<sup>18</sup></strong></td>
        <td><strong>12 hours</strong></td>
      </tr>
      <tr>
        <td><strong>Transformer (big)</strong></td>
        <td><strong>28.4</strong></td>
        <td><strong>2.3 × 10<sup>19</sup></strong></td>
        <td><strong>3.5 days</strong></td>
      </tr>
    </tbody>
  </table>
</div>


<p><strong>Improvement</strong>: +2.0 BLEU over previous best (including ensembles), at a fraction of training cost.</p>

<p><strong>English-to-French (EN-FR):</strong></p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Model</th>
        <th>BLEU</th>
        <th>Training Time</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Deep-Att + PosUnk Ensemble</td>
        <td>40.4</td>
        <td>High</td>
      </tr>
      <tr>
        <td>GNMT + RL Ensemble</td>
        <td>41.16</td>
        <td>~6 days</td>
      </tr>
      <tr>
        <td><strong>Transformer (big)</strong></td>
        <td><strong>41.8</strong></td>
        <td><strong>3.5 days</strong></td>
      </tr>
    </tbody>
  </table>
</div>


<p><strong>Improvement</strong>: Beats all previous models with less than &frac14; training cost.</p>

<a name="Key-Numbers"></a>
<h3>Key Numbers</h3>

<ul>
<li><strong>Hardware</strong>: 8 NVIDIA P100 GPUs</li>
<li><strong>Base model training time</strong>: 100,000 steps = 12 hours (0.4 seconds per step)</li>
<li><strong>Big model training time</strong>: 300,000 steps = 3.5 days (1.0 seconds per step)</li>
<li><strong>Dataset (EN-DE)</strong>: 4.5 million sentence pairs</li>
<li><strong>Dataset (EN-FR)</strong>: 36 million sentences</li>
<li><strong>Vocabulary (EN-DE)</strong>: 37,000 tokens (byte-pair encoding)</li>
<li><strong>Vocabulary (EN-FR)</strong>: 32,000 tokens (word-piece)</li>
</ul>


<a name="Generalization-Beyond-Translation"></a>
<h3>Generalization Beyond Translation</h3>

<p>The paper also tested English constituency parsing (Penn Treebank):</p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
    <thead>
      <tr>
        <th>Model</th>
        <th>WSJ Only</th>
        <th>Semi-Supervised</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Previous best (discriminative)</td>
        <td>91.7</td>
        <td>92.1</td>
      </tr>
      <tr>
        <td><strong>Transformer (4 layers)</strong></td>
        <td><strong>91.3</strong></td>
        <td><strong>92.7</strong></td>
      </tr>
    </tbody>
  </table>
</div>


<p>Despite no task-specific tuning, the Transformer achieved state-of-the-art on semi-supervised parsing and competitive results on WSJ-only. This proved the architecture was generalizable.</p>

<a name="Technical-Deep-Dive"></a>
<h2>Technical Deep Dive</h2>

<a name="Model-Variants--26-amp-3b--Ablation-Study"></a>
<h3>Model Variants &amp; Ablation Study</h3>

<p>The paper conducted extensive ablations (Table 3) to understand which components matter:</p>

<p><strong>Multi-Head Variations (Rows A)</strong>:</p>

<ul>
<li>1 head: -0.9 BLEU</li>
<li>4 heads with h=128: No degradation</li>
<li>8 heads with h=64: Best (baseline)</li>
<li>16 heads with h=32: -0.4 BLEU</li>
<li>32 heads with h=16: -0.4 BLEU</li>
</ul>


<p>Finding: Single-head attention hurts, but too many heads also degrades performance. Eight is optimal.</p>

<p><strong>Attention Key Dimension (Rows B)</strong>:</p>

<ul>
<li>d_k = 256/32 = 8: Worse</li>
<li>d_k = 64 (baseline): Best</li>
</ul>


<p>Finding: Smaller keys hurt model quality. The compatibility function needs sufficient dimensionality.</p>

<p><strong>Model Size (Rows C, D)</strong>:</p>

<ul>
<li>d_model = 256: Much worse</li>
<li>d_model = 1024: Better but slower</li>
<li>d_ff = 1024: Better</li>
<li>d_ff = 4096: Even better (but more compute)</li>
</ul>


<p>Finding: Larger models are better, as expected.</p>

<p><strong>Regularization (Rows D)</strong>:</p>

<ul>
<li>P_drop = 0.0: Severe overfitting</li>
<li>P_drop = 0.1: Best for base model</li>
<li>P_drop = 0.3: Better for larger models</li>
</ul>


<p><strong>Positional Encoding (Row E)</strong>:</p>

<ul>
<li>Sinusoidal: 25.8 BLEU</li>
<li>Learned embedding: 25.7 BLEU</li>
</ul>


<p>Finding: Nearly identical, validating sinusoidal choice.</p>

<a name="Training-Details"></a>
<h3>Training Details</h3>

<p><strong>Optimizer</strong>: Adam with β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁹</p>

<p><strong>Learning Rate Schedule</strong>:
<code>
lrate = d_model^(-0.5) · min(step_num^(-0.5), step_num · warmup_steps^(-1.5))
</code></p>

<p>With warmup_steps = 4000. This increases learning rate linearly for first 4000 steps, then decreases proportionally to step<sup>-0.5</sup>.</p>

<p><strong>Regularization Techniques</strong>:</p>

<ul>
<li><strong>Residual Dropout</strong>: Applied to all sub-layer outputs before adding residual connection. P_drop = 0.1 for base model.</li>
<li><strong>Label Smoothing</strong>: ε_ls = 0.1. This prevents model from becoming overconfident in predictions.</li>
</ul>


<p><strong>Inference</strong>: Beam search with beam size = 4, length penalty α = 0.6. Model parameters averaged over last 5-20 checkpoints.</p>

<a name="Understanding-the-Impact"></a>
<h2>Understanding the Impact</h2>

<a name="Why-This-Mattered"></a>
<h3>Why This Mattered</h3>

<p>The Transformer&rsquo;s success opened new possibilities:</p>

<ul>
<li><strong>Massive Scale</strong>: Without sequential constraints, you could train enormous models. GPT-1 (2018) had 117 million
parameters. GPT-3 (2020) had 175 billion. Each could train faster due to Transformer parallelization.</li>
<li><strong>Transfer Learning</strong>: BERT (2018) showed you could pre-train Transformers on massive unlabeled text, then
fine-tune for specific tasks. This revolutionized NLP.</li>
<li><p><strong>Generality</strong>: The same architecture worked for:</p>

<ul>
<li>Machine translation</li>
<li>Text summarization</li>
<li>Question answering</li>
<li>Parsing</li>
<li>(Later) Computer vision (Vision Transformers, 2020)</li>
<li>(Later) Protein folding (AlphaFold, 2020)</li>
<li>(Later) Speech, audio, multimodal tasks</li>
</ul>
</li>
<li><p><strong>Efficiency</strong>: Training became faster and cheaper, democratizing AI research.</p></li>
</ul>


<a name="Modern-Descendants"></a>
<h3>Modern Descendants</h3>

<p>Every major language model today uses Transformer variants:</p>

<ul>
<li><strong>GPT series</strong> (OpenAI): Decoder-only Transformer</li>
<li><strong>BERT</strong> (Google): Encoder-only Transformer</li>
<li><strong>T5</strong> (Google): Full encoder-decoder</li>
<li><strong>GPT-4, Gemini, Claude, Llama</strong>: All Transformer-based</li>
</ul>


<a name="Limitations--26-amp-3b--Future-Directions"></a>
<h2>Limitations &amp; Future Directions</h2>

<p>The paper acknowledges limitations:</p>

<ul>
<li><strong>Quadratic Complexity</strong>: Self-attention is O(n² · d). Long documents become expensive. The paper suggests sparse
attention for very long sequences.</li>
<li><strong>Effective Resolution</strong>: Attention averaging can lose fine-grained information in long sequences.</li>
<li><strong>Generation Speed</strong>: Decoding is still sequential (generates one word at a time).</li>
</ul>


<p>Future work suggested:</p>

<ul>
<li>Restricted self-attention to handle images, audio, video</li>
<li>Local attention mechanisms</li>
<li>Making generation less sequential</li>
</ul>


<p>(Later work addressed these: Sparse Transformers, Longformer, Flash Attention, etc.)</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>&ldquo;Attention Is All You Need&rdquo; presented a deceptively simple idea: replace recurrence and convolution with pure attention. But this simplicity masked profound consequences.</p>

<p>The paper proved that:</p>

<ul>
<li>Sequential processing isn&rsquo;t necessary for sequence understanding</li>
<li>Parallelization matters in practice for training speed</li>
<li>Simpler architectures can outperform complex ones</li>
<li>Elegant solutions often beat engineered complexity</li>
</ul>


<p>The eight authors—Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin—changed AI forever.</p>

<p>Nearly a decade later, we&rsquo;re still discovering applications and improvements based on their core insight. Every conversation with ChatGPT, every Google search result, every code completion in your IDE—all trace back to this paper&rsquo;s ideas.</p>

<p>The lesson transcends AI research: sometimes the breakthrough isn&rsquo;t in complexity. It&rsquo;s in finding the right abstraction that reveals hidden simplicity in what seemed complex before.</p>

<blockquote><p>&ldquo;Attention, after all, might be all we need.&rdquo;</p></blockquote>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Generative AI in 2025: Global Trends, Breakthroughs and Future Horizons]]></title>
        <link href="https://rishijeet.github.io/blog/generative-ai-in-2025-global-trends-breakthroughs-and-future-horizons/"/>
        <updated>2025-09-04T12:02:41+05:30</updated>
        <id>https://rishijeet.github.io/blog/generative-ai-in-2025-global-trends-breakthroughs-and-future-horizons</id>
        <content type="html"><![CDATA[<p>Generative AI (GenAI) has transitioned from an experimental technology to a cornerstone of global innovation by 2025,
reshaping industries, economies, and societal norms. This comprehensive overview draws on recent reports, surveys,
and developments to explore the latest happenings in the GenAI space worldwide, while projecting likely future
trajectories.</p>

<p>From surging investments and enterprise adoption to ethical dilemmas and regulatory frameworks,
GenAI&rsquo;s evolution reflects a blend of unprecedented potential and persistent challenges. We&rsquo;ll examine key trends,
regional variations, technological breakthroughs, and forward-looking predictions, incorporating data from authoritative sources like Stanford&rsquo;s AI Index, McKinsey and Gartner.</p>

<p>In 2025, GenAI has seen explosive growth in enterprise adoption, particularly in functions like marketing, product development, and software engineering. Companies are investing heavily, with traffic surging 890% and budgets growing 60% through 2027. Breakthroughs include multimodal AI, where models process text, images, video, and audio, enabling applications in public sectors for better data search and citizen services. However, issues like AI-generated ransomware and deepfakes are rising, prompting global regulatory responses.</p>

<p><img src="/images/2025/gen_ai_usage.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>

<!--more-->


<a name="Research-Facts"></a>
<h2>Research Facts</h2>

<p>Research suggests that generative AI (GenAI) adoption has surged globally, with 71% of organizations using it in at
least one function as of 2025, up from 65% in early 2024, driven by investments totaling $33.9 billion in 2024.</p>

<p><img src="/images/2025/genai_adoption_trend.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>

<p>It seems likely that AI agents and multimodal models are dominating current developments, with tools like AI-powered agents handling complex tasks autonomously, though challenges like hallucinations and ethical concerns persist.</p>

<p>Evidence leans toward future advancements focusing on reasoning AI, small language models for efficiency, and integration into critical sectors like healthcare and finance, potentially adding trillions to the global economy, while regulations aim to address risks like deepfakes and privacy.</p>

<a name="Surging-Adoption-and-Investment-Momentum"></a>
<h3>Surging Adoption and Investment Momentum</h3>

<p>Global private investment in GenAI reached $33.9 billion in 2024, marking an 18.7% increase from the previous year, according to Stanford&rsquo;s 2025 AI Index Report. This capital influx has fueled widespread adoption: McKinsey&rsquo;s 2025 State of AI survey reveals that 71% of organizations now use GenAI in at least one business function, up from 65% in early 2024 and a dramatic leap from 33% in 2023. Enterprises are prioritizing functions like marketing, sales, product development, service operations, and software engineering, where GenAI drives productivity gains—such as automating reports, emails, and presentations, with 64.78% of users leveraging it for these tasks.</p>

<p><img src="/images/2025/gen_private_invest.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>

<p>In the U.S., Palo Alto Networks reports an 890% surge in GenAI traffic, underscoring its role in boosting creativity and efficiency. Europe sees similar trends, with Denmark introducing legislation to combat deepfakes by protecting individuals&#8217; rights to their body, voice, and facial features against GenAI misuse. In Asia, India&rsquo;s banking sector anticipates a 46% improvement in operations via GenAI, per the Reserve Bank of India. Globally, budgets are expanding: Boston Consulting Group projects a 60% growth in GenAI spending from 2025 to 2027, averaging 7.6% of total IT budgets.</p>

<p>However, ROI remains elusive for many—95% of organizations report zero returns despite $30-40 billion in investments, according to an MIT report. Larger firms (over $500 million in revenue) are adapting faster, investing in AI talent and mitigating risks like hallucinations and bias. On X, discussions highlight market vulnerabilities, with Bloomberg noting that high valuations for companies like Nvidia and Tesla are tied to GenAI hype but show signs of wobbling.</p>

<a name="Key-Technological-Breakthroughs"></a>
<h3>Key Technological Breakthroughs</h3>

<p>GenAI&rsquo;s core advancements in 2025 center on multimodal capabilities, AI agents, and efficiency improvements. Multimodal AI, which integrates text, images, video, and audio, is rising rapidly—Valtech predicts it as a top trend, enabling applications like Google&rsquo;s semantic search for public sector data. Tools like OpenAI&rsquo;s o1 model excel in reasoning, solving complex problems in science, coding, and math with human-like logic.</p>

<p>AI agents represent a paradigm shift: Microsoft&rsquo;s forecast sees them evolving to handle tasks autonomously, from HR queries to report generation. Forbes highlights five transformative trends: AI agents, inference-time compute, very large and small language models, and near-infinite memory. Open-source models like CAMEL-AI&rsquo;s multi-agent workflows and Huawei&rsquo;s Celia Voice Enhancement for hearing-impaired users exemplify this. In robotics, Google DeepMind&rsquo;s Genie 3 creates real-time interactive worlds from prompts, aiding agent training in simulated environments.</p>

<p>Other innovations include optical generative models for energy-efficient AI and synthetic CRISPR systems designed by GenAI, reducing off-target effects by 35%. In creative fields, tools like Runway Aleph enable real-time video generation, while Adobe&rsquo;s AI-powered PDFs abstract away human editing.</p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
  <thead>
    <tr>
      <th>Trend</th>
      <th>Description</th>
      <th>Key Examples</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Multimodal AI</td>
      <td>Processes multiple data types (text, image, video, audio)</td>
      <td>Google Cloud&#8217;s AI for public sector; Huawei Celia</td>
      <td>Enhances accessibility and data analysis; projected to redefine citizen-government interactions</td>
    </tr>
    <tr>
      <td>AI Agents</td>
      <td>Autonomous task performers</td>
      <td>Microsoft 365 Copilot; Salesforce Agentforce</td>
      <td>Boosts productivity by 10x in coding; handles complex workflows</td>
    </tr>
    <tr>
      <td>Small Language Models</td>
      <td>Efficient, specialized models</td>
      <td>Google Gemini Flash; Phi series</td>
      <td>Reduces costs by up to 35%; ideal for edge devices</td>
    </tr>
    <tr>
      <td>Reasoning AI</td>
      <td>Logical problem-solving</td>
      <td>OpenAI o1; Neuro-symbolic hybrids</td>
      <td>Solves math/science problems; enables new theorems by 2026</td>
    </tr>
    <tr>
      <td>World Models</td>
      <td>Simulated environments</td>
      <td>DeepMind Genie 3</td>
      <td>Trains robots/agents in real-time; extends visual memory to 1 minute</td>
    </tr>
  </tbody>
</table>
</div>


<a name="Societal-and-Ethical-Implications"></a>
<h3>Societal and Ethical Implications</h3>

<p>GenAI&rsquo;s proliferation raises concerns: Princeton research shows models becoming &ldquo;indifferent to truth&rdquo; to please users, while WIRED reports AI-fueled ransomware evolution. Job displacement is evident, with CBS News noting entry-level roles replaced by tools like ChatGPT. In media, ABC News highlights GenAI&rsquo;s transformation of the $29.6 billion music industry, sparking debates on creativity. Ethical AI is a priority: 87% of leaders emphasize responsible principles, but implementation lags due to complexity. Lawsuits, like xAI and X vs. Apple/OpenAI, allege monopolies in GenAI markets.</p>

<p>In education, programs like AI4ALL at Princeton bridge digital divides, while clinicians view GenAI peers skeptically, rating them 35% lower in competence. Privacy-preserving tech like federated AI is gaining traction.</p>

<a name="Regional-and-Sector-2d-Specific-Developments"></a>
<h3>Regional and Sector-Specific Developments</h3>

<ul>
<li><strong>North America</strong>: Focus on ROI and agents; Morgan Stanley notes AI reasoning fueling chip demand.</li>
<li><strong>Europe</strong>: Regulatory push; Denmark&rsquo;s deepfake laws and EU variations.</li>
<li><strong>Asia</strong>: Biotech and banking; AI-designed CRISPR in China, 46% banking boost in India.</li>
<li><strong>Public Sector</strong>: Google&rsquo;s trends predict multimodal AI for efficiency.</li>
<li><strong>Healthcare</strong>: Coherent Denoising generates synthetic data for precision medicine, preserving privacy.</li>
</ul>


<a name="Predictions-for-the-Near-Future--28-2025-2d-2026-29-"></a>
<h2>Predictions for the Near Future (2025-2026)</h2>

<p>Looking ahead, Exploding Topics forecasts seven key trends: AI in healthcare/finance/sustainability, with AGI paths emerging. MIT Technology Review highlights agents, small models, and scientific data sets as 2025 hotspots. Agentic AI will mature, with MIT Sloan predicting limited workforce impact in 2025 but growth in internal tasks. Self-optimizing models could boost efficiency by 35%, per recent techniques.</p>

<p>Hybrid workflows, like those in Harvard talks on spatial/visual intelligence, will blur software and content. Physical AI, as Nvidia&rsquo;s Jensen Huang describes, will advance robotics. Gartner&rsquo;s 2025 Hype Cycle emphasizes scaling amid regulations. Overall, GenAI could add $4.4 trillion annually to the economy, but balanced views—celebrating successes while addressing limitations—are essential.</p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
  <thead>
    <tr>
      <th>Future Trend</th>
      <th>Timeline</th>
      <th>Potential Impact</th>
      <th>Challenges</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Agentic AI</td>
      <td>2025-2026</td>
      <td>Automates 60-70% of work activities</td>
      <td>Ethical concerns, human oversight needed</td>
    </tr>
    <tr>
      <td>Reasoning &amp; Causal AI</td>
      <td>Mid-2025</td>
      <td>New theorems, scientific discoveries</td>
      <td>Bias in cause-effect modeling</td>
    </tr>
    <tr>
      <td>Physical/Neuromorphic AI</td>
      <td>2026+</td>
      <td>Advanced robotics, quantum integration</td>
      <td>Experimental stage, high costs</td>
    </tr>
    <tr>
      <td>Privacy-Preserving AI</td>
      <td>Ongoing</td>
      <td>Decentralized learning for healthcare</td>
      <td>Regulatory compliance variations</td>
    </tr>
    <tr>
      <td>AGI Pathways</td>
      <td>Speculative (post-2026)</td>
      <td>Universal transformation</td>
      <td>Conceptual risks like self-awareness</td>
    </tr>
  </tbody>
</table>
</div>


<a name="Emerging-Challenges-and-Opportunities"></a>
<h2>Emerging Challenges and Opportunities</h2>

<p>While 74% of enterprises report ROI from GenAI, many struggle with implementation, including talent shortages and risks like bias. Opportunities abound in areas like healthcare, where AI enhances diagnostics, and finance, improving operations by up to 46% in regions like India. Social impacts include job shifts, with entry-level roles increasingly automated, and concerns over cognitive skill erosion from overreliance on tools like ChatGPT.</p>

<a name="Likely-Future-Trends"></a>
<h3>Likely Future Trends</h3>

<p>It appears probable that by 2026, agentic AI—autonomous systems performing tasks with minimal human input—will become mainstream, alongside advancements in reasoning models like OpenAI&rsquo;s o1, which solve complex problems in fields like science and coding. Expect greater emphasis on ethical AI, privacy-preserving tech, and integration with robotics, potentially transforming industries while navigating stricter regulations. Innovations like real-time interactive world models, such as Google DeepMind&rsquo;s Genie 3, signal a shift toward simulated environments for AI training, accelerating progress in agents and physical AI.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>In summary, 2025 marks GenAI&rsquo;s maturation phase, with global adoption accelerating amid innovation and caution. The path forward promises economic trillions but demands responsible stewardship to mitigate risks and maximize benefits.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Supercharge Reasoning in AI: Hands-On Chain of Thought Builds]]></title>
        <link href="https://rishijeet.github.io/blog/supercharge-reasoning-in-ai-hands-on-chain-of-thought-builds/"/>
        <updated>2025-08-29T13:26:07+05:30</updated>
        <id>https://rishijeet.github.io/blog/supercharge-reasoning-in-ai-hands-on-chain-of-thought-builds</id>
        <content type="html"><![CDATA[<p>Chain of Thought (CoT) is a prompting technique introduced in a 2022 paper by Google researchers (Wei et al., &ldquo;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&rdquo;). The core idea is simple: instead of asking an LLM for a direct answer, you instruct it to <strong>reason step by step</strong>. This elicits better performance on tasks requiring logic, math, commonsense, or multi-step planning.</p>

<p><img src="/images/2025/cot.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>For example:</p>

<ul>
<li><strong>Direct Prompt</strong>: &ldquo;What is 15% of 200?&rdquo;</li>
<li><strong>CoT Prompt</strong>: &ldquo;What is 15% of 200? Let&rsquo;s think step by step.&rdquo;</li>
</ul>


<p>The LLM might respond:</p>

<ul>
<li>&ldquo;Step 1: 15% means 15 per 100, so 15/100 = 0.15.</li>
<li>Step 2: Multiply by 200: 0.15 * 200 = 30. So, the answer is 30.&#8221;</li>
</ul>


<!--more-->


<p>This &ldquo;thinking&rdquo; process isn&rsquo;t magic—it&rsquo;s emergent from the model&rsquo;s training on vast datasets where step-by-step explanations are common. CoT shines in zero-shot (no examples) or few-shot (with examples) scenarios, and variants like <strong>Tree of Thoughts</strong> or <strong>Self-Consistency</strong> build on it for even more robustness.</p>

<a name="Why-Does-CoT-Work-3f-"></a>
<h3>Why Does CoT Work?</h3>

<ul>
<li><strong>Decomposes Complexity</strong>: Breaks problems into manageable sub-steps, reducing error rates.</li>
<li><strong>Transparency</strong>: Users see the &ldquo;thought process,&rdquo; building trust and allowing debugging.</li>
<li><strong>Scalability</strong>: Works with any LLM API; no need for fine-tuning.</li>
<li><strong>Applications</strong>: Math solvers, code debuggers, decision-making tools, chatbots that explain reasoning.</li>
</ul>


<p>Research shows CoT improves accuracy by 10-50% on benchmarks like GSM8K (math) or CommonsenseQA. In interactive apps, it can stream thoughts progressively, giving users a &ldquo;processing&rdquo; indicator.</p>

<a name="Evolution-and-Variants-of-CoT"></a>
<h2>Evolution and Variants of CoT</h2>

<p>CoT has evolved rapidly:</p>

<ul>
<li><strong>Zero-Shot CoT</strong>: Just add &ldquo;Let&rsquo;s think step by step&rdquo; to the prompt.</li>
<li><strong>Few-Shot CoT</strong>: Provide 2-5 examples of step-by-step reasoning before the query.</li>
<li><strong>Automatic CoT</strong>: Use LLMs to generate CoT examples dynamically.</li>
<li><strong>Tree of Thoughts (ToT)</strong>: Explores multiple reasoning paths like a tree search.</li>
<li><strong>Graph of Thoughts</strong>: Models reasoning as a graph for non-linear problems.</li>
</ul>


<p>In 2025, with models like Grok 4, CoT is often combined with tools (e.g., code execution or web search) for agentic systems—AI agents that plan, act, and reflect.</p>

<a name="Building-a-Chain-of-Thought-Application:-Step-2d-by-2d-Step-Guide"></a>
<h2>Building a Chain of Thought Application: Step-by-Step Guide</h2>

<p>To build a CoT application, we&rsquo;ll create a Python-based tool that:</p>

<ol>
<li>Takes user input.</li>
<li>Applies CoT prompting via an LLM API.</li>
<li>Streams the response to show &ldquo;thinking&rdquo; in real-time (using API streaming features).</li>
<li>Parses the final answer for clarity.</li>
</ol>


<p>We&rsquo;ll use OpenAI&rsquo;s API as an example. Assumptions:</p>

<ul>
<li>You have an API key.</li>
<li>Focus on a math/word problem solver, but extensible to any domain.</li>
</ul>


<a name="Step-1:-Set-Up-Your-Environment"></a>
<h3>Step 1: Set Up Your Environment</h3>

<p>Install required libraries:
<code>bash
pip install openai requests
</code></p>

<a name="Step-2:-Basic-CoT-Implementation"></a>
<h3>Step 2: Basic CoT Implementation</h3>

<p>Start with a simple non-streaming version. This script prompts the LLM with CoT and prints the full response.</p>

<pre><code class="python">import openai

# Set your API key
openai.api_key = "your-openai-api-key"  # Replace with actual key

def cot_reasoning(query, model="gpt-4o"):
    """
    Applies Chain of Thought prompting to a query.

    Args:
    - query (str): The user's question.
    - model (str): LLM model to use.

    Returns:
    - str: The reasoned response.
    """
    # CoT Prompt Template (Few-Shot for better results)
    prompt = """
    Solve the following problem step by step.

    Example 1:
    Question: If a car travels 60 miles in 1.5 hours, what is its speed?
    Step 1: Speed is distance divided by time.
    Step 2: Distance = 60 miles, Time = 1.5 hours.
    Step 3: Speed = 60 / 1.5 = 40 mph.
    Answer: 40 mph.

    Example 2:
    Question: What is the next number in the sequence: 2, 4, 8, 16?
    Step 1: Observe the pattern: each number is doubled.
    Step 2: 2 * 2 = 4, 4 * 2 = 8, 8 * 2 = 16.
    Step 3: Next is 16 * 2 = 32.
    Answer: 32.

    Now, your question:
    Question: {query}
    """

    formatted_prompt = prompt.format(query=query)

    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": formatted_prompt}]
    )

    return response.choices[0].message.content

# Usage
query = "What is 25% of 400?"
result = cot_reasoning(query)
print(result)
</code></pre>

<p><strong>Expected Output</strong>:
<code>
Step 1: 25% means 25 per 100, so 0.25.
Step 2: Multiply by 400: 0.25 * 400 = 100.
Answer: 100.
</code></p>

<p>This shows the &ldquo;thinking&rdquo; steps. To make it interactive, add a loop for multiple queries.</p>

<a name="Step-3:-Adding-Real-2d-Time-Processing-Feedback"></a>
<h3>Step 3: Adding Real-Time Processing Feedback</h3>

<p>To &ldquo;let you know that it is processing these steps,&rdquo; use streaming. OpenAI supports response streaming, printing tokens as they arrive—simulating thinking.</p>

<p>Modify the function:</p>

<pre><code class="python">import openai
import sys

openai.api_key = "your-openai-api-key"

def cot_streaming_reasoning(query, model="gpt-4o"):
    """
    Streams Chain of Thought reasoning in real-time.
    """
    prompt = """
    Solve the following problem step by step. Think out loud.

    # Few-shot examples here (same as above)

    Question: {query}
    """
    formatted_prompt = prompt.format(query=query)

    stream = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": formatted_prompt}],
        stream=True  # Enable streaming
    )

    print("Thinking...")
    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.get("content"):
            content = chunk.choices[0].delta.content
            sys.stdout.write(content)  # Print incrementally
            sys.stdout.flush()
            full_response += content
    print("\nDone!")

    return full_response

# Usage
query = "If I have 3 apples and eat 2, how many are left?"
cot_streaming_reasoning(query)
</code></pre>

<p><strong>How It Works</strong>:</p>

<ul>
<li>The <code>stream=True</code> parameter yields partial responses.</li>
<li>We print each chunk, showing steps like &ldquo;Step 1: &hellip;&rdquo; as they generate.</li>
<li>This creates a &ldquo;processing&rdquo; effect—users see thoughts unfolding.</li>
</ul>


<p>For a web app, use Flask or Streamlit to stream via WebSockets.</p>

<a name="Step-4:-Advanced-Features--e2--80--93--Parsing-and-Error-Handling"></a>
<h3>Step 4: Advanced Features – Parsing and Error Handling</h3>

<p>To extract the final answer reliably, parse the response. Add self-consistency by generating multiple CoTs and voting.</p>

<pre><code class="python">def parse_final_answer(response):
    """
    Extracts the final answer from CoT response.
    """
    lines = response.split("\n")
    for line in reversed(lines):
        if line.startswith("Answer:"):
            return line.split("Answer:")[1].strip()
    return "No clear answer found."

# In your main function:
result = cot_streaming_reasoning(query)
final_answer = parse_final_answer(result)
print(f"Final Answer: {final_answer}")
</code></pre>

<p>For robustness (Self-Consistency CoT):</p>

<ul>
<li>Run 3-5 CoT generations.</li>
<li>Use majority vote on answers.</li>
</ul>


<pre><code class="python">def self_consistent_cot(query, num_samples=3):
    answers = []
    for _ in range(num_samples):
        response = cot_reasoning(query)  # Or streaming version
        answer = parse_final_answer(response)
        answers.append(answer)

    # Simple majority vote
    from collections import Counter
    most_common = Counter(answers).most_common(1)
    return most_common[0][0] if most_common else "Inconsistent results"

# Usage
consistent_answer = self_consistent_cot("A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much is the ball?")
print(consistent_answer)  # Should be $0.05
</code></pre>

<a name="Step-5:-Building-a-Full-Application"></a>
<h3>Step 5: Building a Full Application</h3>

<p>For a complete app, use Streamlit for a UI:</p>

<pre><code class="python">import streamlit as st
import openai

openai.api_key = "your-openai-api-key"

st.title("Chain of Thought Reasoner")

query = st.text_input("Enter your question:")

if st.button("Reason"):
    with st.spinner("Thinking step by step..."):
        response = cot_streaming_reasoning(query)  # Use non-streaming for simplicity, or adapt
        st.write(response)
        final = parse_final_answer(response)
        st.success(f"Final Answer: {final}")
</code></pre>

<p>Run with <code>streamlit run app.py</code>. This creates a web interface where users input queries and see the CoT process.</p>

<a name="Challenges-and-Best-Practices"></a>
<h2>Challenges and Best Practices</h2>

<ul>
<li><strong>Token Limits</strong>: CoT increases prompt length; use efficient models.</li>
<li><strong>Bias in Reasoning</strong>: LLMs can hallucinate steps—validate with tools (e.g., code execution for math).</li>
<li><strong>Customization</strong>: For domains like code generation, add &ldquo;Step 1: Understand requirements. Step 2: Plan code structure.&rdquo;</li>
<li><strong>Integration with Agents</strong>: Combine CoT with ReAct (Reason + Act) for tool-using agents.</li>
<li><strong>Ethics</strong>: Ensure transparency; CoT doesn&rsquo;t make AI infallible.</li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>Chain of Thought applications transform LLMs from black boxes into transparent reasoners. By building step-by-step processing into your prompts and code, you create tools that not only solve problems but explain how.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Understanding ReAct in Large Language Models]]></title>
        <link href="https://rishijeet.github.io/blog/understanding-react-in-large-language-models/"/>
        <updated>2025-08-28T08:48:16+05:30</updated>
        <id>https://rishijeet.github.io/blog/understanding-react-in-large-language-models</id>
        <content type="html"><![CDATA[<p>ReAct, short for Reasoning and Acting, is a paradigm for enhancing large language models (LLMs) by integrating verbal reasoning traces with task-specific actions. Introduced in a 2022 paper, it addresses limitations in chain-of-thought (CoT) prompting by allowing models to interact with external environments, such as APIs or databases, to gather real-time data. This makes LLMs more reliable for tasks requiring factual accuracy or multi-step planning.</p>

<p>In the evolving field of artificial intelligence, large language models (LLMs) have transformed how we approach problem-solving, but they often struggle with hallucinations—generating plausible but incorrect information—or handling tasks requiring real-world interaction. Enter ReAct (Reasoning and Acting), a prompting framework that synergizes reasoning traces with actionable steps, enabling LLMs to behave more like intelligent agents. This detailed blog explores ReAct&rsquo;s foundations, mechanics, advantages, and practical implementation, culminating in a sample Python application using LangChain. We&rsquo;ll draw on established research and code examples to provide a comprehensive guide, updated with insights as of 2025.</p>

<a name="How-ReAct-Works"></a>
<h2>How ReAct Works</h2>

<p>In ReAct, the LLM generates a &ldquo;thought&rdquo; to plan, selects an &ldquo;action&rdquo; from available tools, observes the outcome, and iterates. This loop continues until the model outputs a final answer. For example, answering &ldquo;What is Olivia Wilde&rsquo;s boyfriend&rsquo;s age raised to the 0.23 power?&rdquo; might involve searching for the boyfriend, then calculating the power.</p>

<p><img src="/images/2025/reAct.gif" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Key-Points"></a>
<h2>Key Points</h2>

<ul>
<li><strong>ReAct Framework</strong>: It seems likely that ReAct is a prompting technique enabling LLMs to alternate between reasoning (thinking step-by-step) and acting (using tools like searches or calculations), improving accuracy on complex tasks by reducing hallucinations and incorporating external information.</li>
<li><strong>Core Process</strong>: Evidence leans toward a loop of Thought (reasoning), Action (tool invocation), Observation (results), repeating until a final answer, mimicking human problem-solving.</li>
<li><strong>Benefits and Limitations</strong>: Research suggests ReAct enhances interpretability and performance on knowledge-intensive and decision-making tasks, though it may increase computational costs and rely on well-defined tools; it&rsquo;s particularly useful for dynamic environments but less so for simple queries.</li>
</ul>


<!--more-->


<a name="Foundations-of-ReAct"></a>
<h2>Foundations of ReAct</h2>

<p>ReAct was introduced in the 2022 paper &ldquo;ReAct: Synergizing Reasoning and Acting in Language Models&rdquo; by Shunyu Yao et al. It builds on chain-of-thought (CoT) prompting, where LLMs break down problems into intermediate reasoning steps for better performance on tasks like arithmetic or commonsense reasoning. However, CoT relies solely on the model&rsquo;s internal knowledge, leading to issues like factual errors or outdated information.</p>

<p>ReAct addresses this by interleaving reasoning (&ldquo;thoughts&rdquo;) with actions, allowing the model to query external sources (e.g., search engines, calculators, or databases) and incorporate observations back into its reasoning process. This creates a feedback loop inspired by human cognition: think, act, observe, and adjust. As of 2025, ReAct remains a cornerstone for building LLM agents, integrated into frameworks like LangChain and LangGraph, with enhancements for multi-agent systems and reduced latency.</p>

<p>Key components include:</p>

<ul>
<li><strong>Thought</strong>: A verbalized reasoning step where the LLM plans or reflects.</li>
<li><strong>Action</strong>: Invocation of a tool, such as searching Wikipedia or running a calculation.</li>
<li><strong>Observation</strong>: The result from the action, fed back to the LLM.</li>
<li><strong>Final Answer</strong>: Output when the loop concludes, often after several iterations.</li>
</ul>


<p>This structure improves trustworthiness by making the process interpretable—users can trace how the model arrived at an answer.</p>

<a name="How-ReAct-Works:-A-Step-2d-by-2d-Step-Breakdown"></a>
<h2>How ReAct Works: A Step-by-Step Breakdown</h2>

<p>ReAct operates in an iterative loop, typically capped at a maximum number of turns to control costs and latency. Here&rsquo;s the flow:</p>

<ol>
<li><strong>Initialization</strong>: The LLM receives a prompt outlining the ReAct format (e.g., &ldquo;You run in a loop of Thought, Action, PAUSE, Observation&rdquo;).</li>
<li><strong>Thought Generation</strong>: The model reasons about the query, deciding on the next action.</li>
<li><strong>Action Execution</strong>: If an action is needed, the system pauses, executes the tool, and returns an observation.</li>
<li><strong>Observation Integration</strong>: The observation is appended to the prompt, and the loop repeats.</li>
<li><strong>Termination</strong>: The model outputs &ldquo;Answer&rdquo; when confident, or hits the iteration limit.</li>
</ol>


<p>For instance, in a knowledge-intensive task like HotpotQA (multi-hop question answering), ReAct might search for &ldquo;Colorado orogeny,&rdquo; observe the result, reason about the eastern sector, and lookup further details until answering the elevation range.</p>

<p>ReAct excels in domains like:</p>

<ul>
<li><strong>Knowledge Tasks</strong>: Outperforms CoT by accessing external info, reducing hallucinations.</li>
<li><strong>Decision-Making</strong>: Handles interactive environments (e.g., games or web navigation) via tools.</li>
<li><strong>Agentic Workflows</strong>: Integrates with RAG or multi-agent systems for complex automation.</li>
</ul>


<p>However, challenges include dependency on tool quality, potential for infinite loops without safeguards, and higher token usage compared to simpler prompts.</p>

<a name="Comparisons:-ReAct-vs.-Other-LLM-Techniques"></a>
<h2>Comparisons: ReAct vs. Other LLM Techniques</h2>

<p>To contextualize ReAct, consider this comparison table:</p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
  <thead>
    <tr>
      <th>Technique</th>
      <th>Description</th>
      <th>Strengths</th>
      <th>Weaknesses</th>
      <th>Use Cases</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b>Chain-of-Thought (CoT)</b></td>
      <td>Prompts LLMs to reason step-by-step without external actions.</td>
      <td>Simple, low-cost; good for internal logic.</td>
      <td>Prone to hallucinations; no real-world interaction.</td>
      <td>Arithmetic, commonsense QA.</td>
    </tr>
    <tr>
      <td><b>ReAct</b></td>
      <td>Interleaves reasoning with tool-based actions and observations.</td>
      <td>Dynamic, factual; interpretable loop.</td>
      <td>Higher latency; tool-dependent.</td>
      <td>Multi-hop QA, web tasks, agents.</td>
    </tr>
    <tr>
      <td><b>Function Calling</b></td>
      <td>Fine-tuned models output JSON for tool calls, without explicit reasoning.</td>
      <td>Fast, structured; efficient for predictable tasks.</td>
      <td>Less adaptable; opaque reasoning.</td>
      <td>API integrations, simple tools.</td>
    </tr>
    <tr>
      <td><b>ReAct + CoT</b></td>
      <td>Hybrid: Uses ReAct for actions and CoT for pure reasoning switches.</td>
      <td>Optimal performance; flexible.</td>
      <td>Complex implementation.</td>
      <td>Advanced agents, hybrid tasks.</td>
    </tr>
  </tbody>
</table>
</div>


<p>ReAct often outperforms baselines on benchmarks like HotpotQA, Fever, ALFWorld, and WebShop, with gains in accuracy and efficiency when combined with CoT.</p>

<a name="Building-a-Sample-ReAct-Application-in-Python"></a>
<h2>Building a Sample ReAct Application in Python</h2>

<p>A basic ReAct agent can be built using Python libraries like LangChain. The sample below creates an agent that searches the web and performs math, demonstrating the loop in action. You&rsquo;ll need API keys for an LLM (e.g., OpenAI) and tools (e.g., Google Serper for search). For full code and setup, see the detailed survey below. This sample creates an agent that answers questions by searching the web (via Tavily) and performing calculations. It demonstrates the full loop and includes conversational memory for multi-turn interactions.</p>

<a name="Prerequisites"></a>
<h3>Prerequisites</h3>

<ul>
<li>Python 3.10+.</li>
<li>Install dependencies: <code>pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite langchain-openai</code>.</li>
<li>API Keys: Obtain OpenAI (for the LLM) and Tavily (for search) keys. Set them as environment variables:
<code>python
import os
import getpass
os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key: ")
os.environ["TAVILY_API_KEY"] = getpass.getpass("Tavily API Key: ")
</code></li>
<li>For tracing (optional): Set LangSmith keys.</li>
</ul>


<a name="Code-Implementation"></a>
<h3>Code Implementation</h3>

<p>The application uses LangGraph&rsquo;s <code>create_react_agent</code> for the ReAct logic. Here&rsquo;s the complete code:</p>

<pre><code class="python"># Import necessary modules
from langchain_tavily import TavilySearchResults
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpointer import MemorySaver
from langchain_core.messages import HumanMessage

# Define tools
tool = TavilySearchResults(max_results=2)
tools = [tool]

# Initialize the LLM (using OpenAI's GPT-4o-mini for efficiency)
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Bind tools to the model
model_with_tools = model.bind_tools(tools)

# Create the agent with memory (using MemorySaver for conversational state)
memory = MemorySaver()
agent_executor = create_react_agent(model_with_tools, tools, checkpointer=memory)

# Configuration for conversational thread (use a unique ID for each conversation)
config = {"configurable": {"thread_id": "conversation-1"}}

# Function to run the agent
def run_agent(query):
    print(f"User Query: {query}")
    for chunk in agent_executor.stream(
        {"messages": [HumanMessage(content=query)]}, config
    ):
        print(chunk)
        print("----")
    # Extract final response
    response = chunk.get("agent", {}).get("messages", [{}])[0].content
    return response

# Example usage
query = "Who is the current CEO of xAI? What is their age squared?"
response = run_agent(query)
print(f"Final Answer: {response}")
</code></pre>

<a name="Detailed-Explanation"></a>
<h3>Detailed Explanation</h3>

<ol>
<li><strong>Tools Definition</strong>: <code>TavilySearchResults</code> is a web search tool returning up to 2 results. It&rsquo;s added to <code>tools</code> for the agent to use.</li>
<li><strong>LLM Setup</strong>: <code>ChatOpenAI</code> initializes the model (e.g., GPT-4o-mini for cost-effectiveness). <code>bind_tools</code> informs the model about available actions.</li>
<li><strong>Agent Creation</strong>: <code>create_react_agent</code> builds the ReAct loop, with <code>MemorySaver</code> enabling state persistence for follow-ups (e.g., &ldquo;Tell me more about them&rdquo;).</li>
<li><strong>Execution</strong>: The <code>stream</code> method runs the agent, printing intermediate thoughts, actions, and observations. For the example query:

<ul>
<li>Thought: Reason about searching for xAI CEO.</li>
<li>Action: Invoke Tavily search.</li>
<li>Observation: Retrieve CEO (e.g., Elon Musk) and age.</li>
<li>Thought: Calculate age squared.</li>
<li>Final Answer: Output the result (e.g., &ldquo;Elon Musk, age 54 squared is 2916&rdquo;).</li>
</ul>
</li>
<li><strong>Extensions</strong>: Add more tools (e.g., math via <code>LLMMathChain</code>) or integrate with databases for custom applications.</li>
</ol>


<a name="Testing-and-Output"></a>
<h3>Testing and Output</h3>

<p>Running the code might yield:</p>

<ul>
<li>Intermediate: Thought → Action (search) → Observation → Thought (calculate) → Answer.
This ensures factual grounding, e.g., verifying current data as of 2025.</li>
</ul>


<a name="Advanced-Variations-and-Best-Practices"></a>
<h2>Advanced Variations and Best Practices</h2>

<ul>
<li><strong>Simple Non-LangChain Implementation</strong>: For a lightweight version, use a custom loop with OpenAI&rsquo;s API, as in Simon Willison&rsquo;s example. Define actions like <code>wikipedia</code> and parse responses with regex.</li>
<li><strong>With LangGraph</strong>: For production, use LangGraph for visual workflows and error handling.</li>
<li><strong>Best Practices</strong>: Limit iterations (e.g., max_turns=5), use verbose mode for debugging, and combine with CoT for hybrid prompting. Monitor token usage, as ReAct can be resource-intensive.</li>
<li><strong>2025 Updates</strong>: Recent integrations include multimodal support (e.g., image analysis tools) and edge deployment for low-latency agents.</li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>ReAct represents a pivotal shift toward agentic AI, empowering LLMs to not just generate text but actively engage with the world. By implementing the sample above, developers can experiment and scale to real-world applications like automated research or virtual assistants.</p>
]]></content>
    </entry>
    
</feed>
