<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: gpt | Rishijeet Mishra]]></title>
    <link href="https://rishijeet.github.io/blog/categories/gpt/atom.xml" rel="self"/>
    <link href="https://rishijeet.github.io/"/>
    <updated>2024-08-03T23:32:32+05:30</updated>
    <id>https://rishijeet.github.io/</id>
    <author>
        <name><![CDATA[Rishijeet Mishra]]></name>
        <email><![CDATA[rishijeet@gmail.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Enhancing Natural Language Processing with Retrieval-Augmented Generation]]></title>
        <link href="https://rishijeet.github.io/blog/enhancing-natural-language-processing-with-retrieval-augmented-generation/"/>
        <updated>2024-01-13T20:34:07+05:30</updated>
        <id>https://rishijeet.github.io/blog/enhancing-natural-language-processing-with-retrieval-augmented-generation</id>
        <content type="html"><![CDATA[<p>Natural Language Processing (NLP) has witnessed remarkable advancements in recent years, with the advent of sophisticated language models like GPT-3 (Generative Pre-trained Transformer 3). However, one of the challenges that still persists in NLP is the generation of coherent and contextually relevant content. Retrieval-Augmented Generation (RAG) emerges as a powerful solution to address this issue, combining the strengths of both retrieval-based and generation-based approaches.</p>

<a name="Understanding-Retrieval-2d-Augmented-Generation"></a>
<h1>Understanding Retrieval-Augmented Generation</h1>

<p>Retrieval-Augmented Generation is a hybrid approach that integrates the benefits of information retrieval systems with generative models. Let&rsquo;s delve into the mathematical formulations of the key components of RAG.</p>

<p><img src="/images/rag_new.png" height="300" width="900" alt="Alt text" /> Figure: Overview of our approach. We combine a pre-trained retriever <em>(Query Encoder + Document Index)</em> with a pre-trained seq2seq model <em>(Generator)</em> and fine-tune end-to-end. For query \(x\), we use Maximum Inner Product Search (MIPS) to find the top-\(K\) documents \(z_i\). For the final prediction \(y\), we treat \(z\) as a latent variable and marginalize over seq2seq predictions given different documents. <em><a href="https://arxiv.org/pdf/2005.11401.pdf">Source: arxiv.org</a></em></p>

<!-- more -->


<a name="L1.-Generative-Model"></a>
<h2>1. Generative Model</h2>

<p>The generative model in RAG is often based on a pre-trained transformer architecture, such as GPT-3. The core functionality involves generating text given a context. Mathematically, the generative model can be represented as:</p>

<p><div>
<span>
\[ P_{\text{gen}}(Y|X) \]
</span>
where \( Y \) is the generated text and \( X \) is the input context. This probability distribution captures the likelihood of generating \( Y \) given \( X \).
<div></p>

<a name="L2.-Retrieval-Model"></a>
<h2>2. Retrieval Model</h2>

<p>The retrieval model is responsible for fetching relevant information from a knowledge base. This can be achieved through techniques like dense retrieval using embeddings. The retrieval model computes the similarity between the input query and the documents in the knowledge base. Mathematically, this can be expressed as:
<div>
\[ \text{argmax}_{d \in \text{KnowledgeBase}} \text{similarity}(Q, \text{Embed}(d)) \]</p>

<p>where \( Q \) is the query, \( \text{KnowledgeBase} \) is the set of documents, \( d \) represents a document, \( \text{Embed}(\cdot) \) denotes the embedding function, and \( \text{similarity}(\cdot) \) measures the similarity between the query and document embeddings.
</div></p>

<a name="L3.-Indexing-Mechanism"></a>
<h2>3. Indexing Mechanism</h2>

<p>Indexing mechanisms play a crucial role in efficiently retrieving information. Commonly, techniques like approximate nearest neighbors are employed. Mathematically, indexing involves mapping documents to a space such that retrieval operations are expedited. This can be represented as:</p>

<div>
&#92;[ \text{Index}(d) \rightarrow \text{Embed}(d) &#92;]

where &#92;( \text{Index}(\cdot) &#92;) denotes the indexing function mapping documents to their embeddings.
</div>


<a name="L4.-Context-2d-Aware-Integration"></a>
<h2>4. Context-Aware Integration</h2>

<p>To integrate retrieved information into the generative process while maintaining context, the retrieval model output needs to be combined with the generative model&rsquo;s output. A simple formulation for context-aware integration can be:</p>

<div>
<span>
&#92;[ P_{\text{final}}(Y|X, Q) = \alpha \cdot P_{\text{gen}}(Y|X) + (1-\alpha) \cdot P_{\text{ret}}(Y|Q) &#92;]
</span>
where &#92;( P_{\text{final}}(Y|X, Q) &#92;) is the final probability distribution of generating &#92;( Y &#92;) given &#92;( X &#92;) and &#92;( Q &#92;), \( \alpha \) is a hyperparameter controlling the balance between generative and retrieval components, and \( P_{\text{ret}}(Y|Q) \) is the probability distribution of generating \( Y \) given the retrieved information \( Q \).
</div>


<a name="Advantages-of-Retrieval-2d-Augmented-Generation"></a>
<h1>Advantages of Retrieval-Augmented Generation</h1>

<ol>
<li><p><strong>Improved Relevance:</strong>
By integrating information retrieval, RAG ensures that the generated content is contextually relevant and grounded in factual accuracy. This is particularly beneficial in applications where precision and relevance are critical, such as question-answering systems.</p></li>
<li><p><strong>Addressing Data Sparsity:</strong>
In scenarios where training data is limited, RAG can leverage external knowledge bases to compensate for the lack of specific information. This makes the model more robust and capable of handling a broader range of topics.</p></li>
<li><p><strong>Contextual Enrichment:</strong>
The retrieval-augmented approach allows for the enrichment of generated content by pulling in relevant details from a diverse set of sources. This not only enhances the quality of the generated text but also broadens the scope of information covered.</p></li>
<li><p><strong>Reduced Ambiguity:</strong>
Integrating retrieval mechanisms helps in disambiguating the meaning of ambiguous terms or phrases by pulling in contextually appropriate information from the knowledge base.</p></li>
</ol>


<a name="Applications-of-Retrieval-2d-Augmented-Generation"></a>
<h1>Applications of Retrieval-Augmented Generation</h1>

<ol>
<li><p><strong>Question Answering Systems:</strong>
RAG is particularly effective in question-answering systems where precise and contextually relevant answers are essential. The retrieval model can fetch information from a knowledge base to support or augment the generative model&rsquo;s response.</p></li>
<li><p><strong>Content Creation:</strong>
In content creation tasks, such as article writing or summarization, RAG can enhance the coherence and factual accuracy of the generated content by pulling in information from external sources.</p></li>
<li><p><strong>Dialog Systems:</strong>
Conversational agents can benefit from RAG by providing more informative and contextually relevant responses. The retrieval model aids in quickly accessing relevant information to support the generative model&rsquo;s output during a conversation.</p></li>
</ol>


<a name="Challenges-and-Future-Directions"></a>
<h1>Challenges and Future Directions</h1>

<p>While Retrieval-Augmented Generation shows great promise, it is not without its challenges. Fine-tuning the balance between the generative and retrieval components, handling diverse knowledge bases, and addressing potential biases in retrieved information are areas that require further research. Additionally, exploring ways to dynamically update the knowledge base during the generative process could open new possibilities for real-time applications.</p>

<p>In conclusion, Retrieval-Augmented Generation represents a significant step forward in enhancing the capabilities of natural language processing systems. By seamlessly integrating the strengths of generative and retrieval models, RAG holds the potential to revolutionize various NLP applications, making them more accurate, contextually aware, and capable of handling a wide range of tasks. As research in this field continues to progress, we can expect even more sophisticated and versatile language models that leverage the best of both worlds.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[The AI Horizon: Unveiling the Titans - Gemini, Llama2, Olympus, Ajax, and Orca 2]]></title>
        <link href="https://rishijeet.github.io/blog/the-ai-horizon-unveiling-the-titans-gemini/"/>
        <updated>2023-12-23T22:49:43+05:30</updated>
        <id>https://rishijeet.github.io/blog/the-ai-horizon-unveiling-the-titans-gemini</id>
        <content type="html"><![CDATA[<a name="Introduction"></a>
<h2>Introduction</h2>

<p>Artificial Intelligence (AI) has witnessed remarkable advancements in recent years, with various tech giants investing heavily in developing large language models (LLMs) to enhance natural language understanding and generation. This article delves into the technical details of Google&rsquo;s Gemini, Meta&rsquo;s Llama2, Amazon&rsquo;s Olympus, Microsoft&rsquo;s Orca 2, and Apple&rsquo;s Ajax.</p>

<a name="Google-Gemini"></a>
<h2>Google Gemini</h2>

<p>Google&rsquo;s Gemini, introduced by Demis Hassabis, CEO and Co-Founder of Google DeepMind, represents a significant leap in AI capabilities. Gemini is a multimodal AI model designed to seamlessly understand and operate across different types of information, including text, code, audio, image, and video.</p>

<p>Gemini is optimized for three different sizes:</p>

<ul>
<li><strong>Gemini Ultra:</strong> The largest and most capable model for highly complex tasks.</li>
<li><strong>Gemini Pro:</strong> The best model for scaling across a wide range of tasks.</li>
<li><strong>Gemini Nano:</strong> The most efficient model for on-device tasks.</li>
</ul>


<p>Gemini Ultra outperforms state-of-the-art results on various benchmarks, including massive multitask language understanding (MMLU) and multimodal benchmarks. With its native multimodality, Gemini excels in complex reasoning tasks, image understanding, and advanced coding across multiple programming languages.</p>

<p>The model is trained using Google&rsquo;s AI-optimized infrastructure, including Tensor Processing Units (TPUs) v4 and v5e. The announcement also introduces Cloud TPU v5p, the most powerful TPU system to date, designed to accelerate the development of large-scale generative AI models.</p>

<p>Gemini reflects Google&rsquo;s commitment to responsibility and safety, incorporating comprehensive safety evaluations, including bias and toxicity assessments. The model&rsquo;s availability spans various Google products and platforms, with plans for further integration and expansion.</p>

<a name="Meta-Llama2"></a>
<h2>Meta Llama2</h2>

<p>Meta&rsquo;s Llama2 is an open-source large language model (LLM) designed as a response to models like GPT from OpenAI and Google&rsquo;s AI models. Noteworthy for its open availability for research and commercial purposes, Llama2 is poised to make a significant impact in the AI space.</p>

<p>Functioning similarly to other LLMs like GPT-3 and PaLM 2, Llama2 uses a transformer architecture and employs techniques such as pretraining and fine-tuning. It is available in different sizes, with variations like Llama 2 7B Chat, Llama 2 13B Chat, and Llama 2 70B Chat, each optimized for specific use cases.</p>

<!-- more -->


<p>Llama2 was trained on 2 trillion tokens from publicly available sources, including Common Crawl, Wikipedia, and Project Gutenberg. The model undergoes training strategies, including reinforcement learning with human feedback (RLHF), to optimize safety and appropriateness of responses.</p>

<p>What sets Llama2 apart is its open nature, allowing users to access the research paper detailing its creation, download the model, and run it on various platforms. By providing transparency and openness, Meta aims to empower other companies to develop AI applications with more control.</p>

<a name="Amazon-Olympus"></a>
<h2>Amazon Olympus</h2>

<p>Amazon, in its pursuit of AI excellence, is working on an ambitious large language model (LLM) codenamed &ldquo;Olympus.&rdquo; With a staggering 2 trillion parameters, Olympus aims to rival leading models from OpenAI and Alphabet. Led by Rohit Prasad, former head of Alexa, the team behind Olympus brings together expertise from Alexa AI and Amazon&rsquo;s science team.</p>

<p>Amazon&rsquo;s strategy involves training homegrown models to make its offerings more appealing on Amazon Web Services (AWS), catering to enterprise clients seeking top-performing models. While Amazon has trained smaller models like Titan and collaborated with AI startups such as Anthropic and AI21 Labs, there&rsquo;s no specific timeline for the release of Olympus.</p>

<p>Large language models (LLMs) are crucial for AI tools that learn from extensive datasets to generate human-like responses. Despite the increased costs associated with training larger models, Amazon is committed to investing in LLMs and generative AI.</p>

<a name="Apple-Ajax"></a>
<h2>Apple Ajax</h2>

<p>Apple&rsquo;s investment in artificial intelligence is evident through its Foundational Models unit, focusing on conversational AI. Headed by John Giannandrea, Apple&rsquo;s head of AI, this unit is dedicated to improving Siri and developing AI models across multiple teams.</p>

<p>Apple is working on advanced LLMs, including Ajax GPT, trained on over 200 billion parameters, surpassing the capabilities of OpenAI&rsquo;s GPT-3.5. The models have applications ranging from customer interaction in AppleCare to automating multistep tasks with Siri.</p>

<p>In addition to conversational AI, Apple has Visual Intelligence and Multimodal AI units developing image generation models and models capable of recognizing and producing images, video, and text simultaneously.</p>

<p>Apple&rsquo;s commitment to AI innovation is reflected in its pursuit of powerful models and diverse AI applications, ensuring advancements in Siri and other AI-powered features.</p>

<a name="Microsoft-27-s-Orca-2"></a>
<h2>Microsoft&rsquo;s Orca 2</h2>

<p>Microsoft&rsquo;s Orca 2 employs a teacher-student training scheme, where a larger LLM acts as a teacher for a smaller one, aiming to improve the performance of the student model. The training involves teaching the student various reasoning techniques and selecting the most effective strategy for specific tasks.</p>

<p>Orca 2 outperformed baseline models, including Llama 2 and ChatGPT, on reasoning benchmarks. The model&rsquo;s performance is evaluated on tasks such as language understanding, text completion, and summarization. Microsoft&rsquo;s innovative training methodology involves &ldquo;Cautious Reasoning,&rdquo; where prompts eliciting specific problem-solving strategies are used during teacher training, and the prompts are erased during student training.</p>

<p>The comparison with other LLMs, including GPT-4 and Llama 2, demonstrates Orca 2&rsquo;s competitive performance. Microsoft&rsquo;s approach aims to address the challenges of hosting large LLMs and emphasizes the effectiveness of smaller models when fine-tuned.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The landscape of large language models continues to evolve, with major tech players pushing the boundaries of AI capabilities. From Google&rsquo;s Gemini to Meta&rsquo;s Llama2, Amazon&rsquo;s Olympus, Apple&rsquo;s Ajax, and Microsoft&rsquo;s Orca 2, each model brings unique features and applications. The open nature of Llama2 and the innovative training schemes of Orca 2 showcase the diverse approaches in AI research. As these models shape the future of AI applications, transparency, responsibility, and safety remain central to their development and deployment.</p>
]]></content>
    </entry>
    
</feed>
