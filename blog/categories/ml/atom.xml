<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: ml | Rishijeet Mishra]]></title>
    <link href="https://rishijeet.github.io/blog/categories/ml/atom.xml" rel="self"/>
    <link href="https://rishijeet.github.io/"/>
    <updated>2025-09-08T09:18:08+05:30</updated>
    <id>https://rishijeet.github.io/</id>
    <author>
        <name><![CDATA[Rishijeet Mishra]]></name>
        <email><![CDATA[rishijeet@gmail.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Generative AI in 2025: Global Trends, Breakthroughs and Future Horizons]]></title>
        <link href="https://rishijeet.github.io/blog/generative-ai-in-2025-global-trends-breakthroughs-and-future-horizons/"/>
        <updated>2025-09-04T12:02:41+05:30</updated>
        <id>https://rishijeet.github.io/blog/generative-ai-in-2025-global-trends-breakthroughs-and-future-horizons</id>
        <content type="html"><![CDATA[<p>Generative AI (GenAI) has transitioned from an experimental technology to a cornerstone of global innovation by 2025,
reshaping industries, economies, and societal norms. This comprehensive overview draws on recent reports, surveys,
and developments to explore the latest happenings in the GenAI space worldwide, while projecting likely future
trajectories.</p>

<p>From surging investments and enterprise adoption to ethical dilemmas and regulatory frameworks,
GenAI&rsquo;s evolution reflects a blend of unprecedented potential and persistent challenges. We&rsquo;ll examine key trends,
regional variations, technological breakthroughs, and forward-looking predictions, incorporating data from authoritative sources like Stanford&rsquo;s AI Index, McKinsey and Gartner.</p>

<p>In 2025, GenAI has seen explosive growth in enterprise adoption, particularly in functions like marketing, product development, and software engineering. Companies are investing heavily, with traffic surging 890% and budgets growing 60% through 2027. Breakthroughs include multimodal AI, where models process text, images, video, and audio, enabling applications in public sectors for better data search and citizen services. However, issues like AI-generated ransomware and deepfakes are rising, prompting global regulatory responses.</p>

<p><img src="/images/2025/gen_ai_usage.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>

<!--more-->


<a name="Research-Facts"></a>
<h2>Research Facts</h2>

<p>Research suggests that generative AI (GenAI) adoption has surged globally, with 71% of organizations using it in at
least one function as of 2025, up from 65% in early 2024, driven by investments totaling $33.9 billion in 2024.</p>

<p><img src="/images/2025/genai_adoption_trend.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>

<p>It seems likely that AI agents and multimodal models are dominating current developments, with tools like AI-powered agents handling complex tasks autonomously, though challenges like hallucinations and ethical concerns persist.</p>

<p>Evidence leans toward future advancements focusing on reasoning AI, small language models for efficiency, and integration into critical sectors like healthcare and finance, potentially adding trillions to the global economy, while regulations aim to address risks like deepfakes and privacy.</p>

<a name="Surging-Adoption-and-Investment-Momentum"></a>
<h3>Surging Adoption and Investment Momentum</h3>

<p>Global private investment in GenAI reached $33.9 billion in 2024, marking an 18.7% increase from the previous year, according to Stanford&rsquo;s 2025 AI Index Report. This capital influx has fueled widespread adoption: McKinsey&rsquo;s 2025 State of AI survey reveals that 71% of organizations now use GenAI in at least one business function, up from 65% in early 2024 and a dramatic leap from 33% in 2023. Enterprises are prioritizing functions like marketing, sales, product development, service operations, and software engineering, where GenAI drives productivity gains—such as automating reports, emails, and presentations, with 64.78% of users leveraging it for these tasks.</p>

<p><img src="/images/2025/gen_private_invest.png" height="300" width="900" alt="Alt text" /><em>Source: Generated by Matplotlib</em></p>

<p>In the U.S., Palo Alto Networks reports an 890% surge in GenAI traffic, underscoring its role in boosting creativity and efficiency. Europe sees similar trends, with Denmark introducing legislation to combat deepfakes by protecting individuals&#8217; rights to their body, voice, and facial features against GenAI misuse. In Asia, India&rsquo;s banking sector anticipates a 46% improvement in operations via GenAI, per the Reserve Bank of India. Globally, budgets are expanding: Boston Consulting Group projects a 60% growth in GenAI spending from 2025 to 2027, averaging 7.6% of total IT budgets.</p>

<p>However, ROI remains elusive for many—95% of organizations report zero returns despite $30-40 billion in investments, according to an MIT report. Larger firms (over $500 million in revenue) are adapting faster, investing in AI talent and mitigating risks like hallucinations and bias. On X, discussions highlight market vulnerabilities, with Bloomberg noting that high valuations for companies like Nvidia and Tesla are tied to GenAI hype but show signs of wobbling.</p>

<a name="Key-Technological-Breakthroughs"></a>
<h3>Key Technological Breakthroughs</h3>

<p>GenAI&rsquo;s core advancements in 2025 center on multimodal capabilities, AI agents, and efficiency improvements. Multimodal AI, which integrates text, images, video, and audio, is rising rapidly—Valtech predicts it as a top trend, enabling applications like Google&rsquo;s semantic search for public sector data. Tools like OpenAI&rsquo;s o1 model excel in reasoning, solving complex problems in science, coding, and math with human-like logic.</p>

<p>AI agents represent a paradigm shift: Microsoft&rsquo;s forecast sees them evolving to handle tasks autonomously, from HR queries to report generation. Forbes highlights five transformative trends: AI agents, inference-time compute, very large and small language models, and near-infinite memory. Open-source models like CAMEL-AI&rsquo;s multi-agent workflows and Huawei&rsquo;s Celia Voice Enhancement for hearing-impaired users exemplify this. In robotics, Google DeepMind&rsquo;s Genie 3 creates real-time interactive worlds from prompts, aiding agent training in simulated environments.</p>

<p>Other innovations include optical generative models for energy-efficient AI and synthetic CRISPR systems designed by GenAI, reducing off-target effects by 35%. In creative fields, tools like Runway Aleph enable real-time video generation, while Adobe&rsquo;s AI-powered PDFs abstract away human editing.</p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
  <thead>
    <tr>
      <th>Trend</th>
      <th>Description</th>
      <th>Key Examples</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Multimodal AI</td>
      <td>Processes multiple data types (text, image, video, audio)</td>
      <td>Google Cloud&#8217;s AI for public sector; Huawei Celia</td>
      <td>Enhances accessibility and data analysis; projected to redefine citizen-government interactions</td>
    </tr>
    <tr>
      <td>AI Agents</td>
      <td>Autonomous task performers</td>
      <td>Microsoft 365 Copilot; Salesforce Agentforce</td>
      <td>Boosts productivity by 10x in coding; handles complex workflows</td>
    </tr>
    <tr>
      <td>Small Language Models</td>
      <td>Efficient, specialized models</td>
      <td>Google Gemini Flash; Phi series</td>
      <td>Reduces costs by up to 35%; ideal for edge devices</td>
    </tr>
    <tr>
      <td>Reasoning AI</td>
      <td>Logical problem-solving</td>
      <td>OpenAI o1; Neuro-symbolic hybrids</td>
      <td>Solves math/science problems; enables new theorems by 2026</td>
    </tr>
    <tr>
      <td>World Models</td>
      <td>Simulated environments</td>
      <td>DeepMind Genie 3</td>
      <td>Trains robots/agents in real-time; extends visual memory to 1 minute</td>
    </tr>
  </tbody>
</table>
</div>


<a name="Societal-and-Ethical-Implications"></a>
<h3>Societal and Ethical Implications</h3>

<p>GenAI&rsquo;s proliferation raises concerns: Princeton research shows models becoming &ldquo;indifferent to truth&rdquo; to please users, while WIRED reports AI-fueled ransomware evolution. Job displacement is evident, with CBS News noting entry-level roles replaced by tools like ChatGPT. In media, ABC News highlights GenAI&rsquo;s transformation of the $29.6 billion music industry, sparking debates on creativity. Ethical AI is a priority: 87% of leaders emphasize responsible principles, but implementation lags due to complexity. Lawsuits, like xAI and X vs. Apple/OpenAI, allege monopolies in GenAI markets.</p>

<p>In education, programs like AI4ALL at Princeton bridge digital divides, while clinicians view GenAI peers skeptically, rating them 35% lower in competence. Privacy-preserving tech like federated AI is gaining traction.</p>

<a name="Regional-and-Sector-2d-Specific-Developments"></a>
<h3>Regional and Sector-Specific Developments</h3>

<ul>
<li><strong>North America</strong>: Focus on ROI and agents; Morgan Stanley notes AI reasoning fueling chip demand.</li>
<li><strong>Europe</strong>: Regulatory push; Denmark&rsquo;s deepfake laws and EU variations.</li>
<li><strong>Asia</strong>: Biotech and banking; AI-designed CRISPR in China, 46% banking boost in India.</li>
<li><strong>Public Sector</strong>: Google&rsquo;s trends predict multimodal AI for efficiency.</li>
<li><strong>Healthcare</strong>: Coherent Denoising generates synthetic data for precision medicine, preserving privacy.</li>
</ul>


<a name="Predictions-for-the-Near-Future--28-2025-2d-2026-29-"></a>
<h2>Predictions for the Near Future (2025-2026)</h2>

<p>Looking ahead, Exploding Topics forecasts seven key trends: AI in healthcare/finance/sustainability, with AGI paths emerging. MIT Technology Review highlights agents, small models, and scientific data sets as 2025 hotspots. Agentic AI will mature, with MIT Sloan predicting limited workforce impact in 2025 but growth in internal tasks. Self-optimizing models could boost efficiency by 35%, per recent techniques.</p>

<p>Hybrid workflows, like those in Harvard talks on spatial/visual intelligence, will blur software and content. Physical AI, as Nvidia&rsquo;s Jensen Huang describes, will advance robotics. Gartner&rsquo;s 2025 Hype Cycle emphasizes scaling amid regulations. Overall, GenAI could add $4.4 trillion annually to the economy, but balanced views—celebrating successes while addressing limitations—are essential.</p>

<div class="scrollable-table-container">
  <table class="scrollable-table">
  <thead>
    <tr>
      <th>Future Trend</th>
      <th>Timeline</th>
      <th>Potential Impact</th>
      <th>Challenges</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Agentic AI</td>
      <td>2025-2026</td>
      <td>Automates 60-70% of work activities</td>
      <td>Ethical concerns, human oversight needed</td>
    </tr>
    <tr>
      <td>Reasoning &amp; Causal AI</td>
      <td>Mid-2025</td>
      <td>New theorems, scientific discoveries</td>
      <td>Bias in cause-effect modeling</td>
    </tr>
    <tr>
      <td>Physical/Neuromorphic AI</td>
      <td>2026+</td>
      <td>Advanced robotics, quantum integration</td>
      <td>Experimental stage, high costs</td>
    </tr>
    <tr>
      <td>Privacy-Preserving AI</td>
      <td>Ongoing</td>
      <td>Decentralized learning for healthcare</td>
      <td>Regulatory compliance variations</td>
    </tr>
    <tr>
      <td>AGI Pathways</td>
      <td>Speculative (post-2026)</td>
      <td>Universal transformation</td>
      <td>Conceptual risks like self-awareness</td>
    </tr>
  </tbody>
</table>
</div>


<a name="Emerging-Challenges-and-Opportunities"></a>
<h2>Emerging Challenges and Opportunities</h2>

<p>While 74% of enterprises report ROI from GenAI, many struggle with implementation, including talent shortages and risks like bias. Opportunities abound in areas like healthcare, where AI enhances diagnostics, and finance, improving operations by up to 46% in regions like India. Social impacts include job shifts, with entry-level roles increasingly automated, and concerns over cognitive skill erosion from overreliance on tools like ChatGPT.</p>

<a name="Likely-Future-Trends"></a>
<h3>Likely Future Trends</h3>

<p>It appears probable that by 2026, agentic AI—autonomous systems performing tasks with minimal human input—will become mainstream, alongside advancements in reasoning models like OpenAI&rsquo;s o1, which solve complex problems in fields like science and coding. Expect greater emphasis on ethical AI, privacy-preserving tech, and integration with robotics, potentially transforming industries while navigating stricter regulations. Innovations like real-time interactive world models, such as Google DeepMind&rsquo;s Genie 3, signal a shift toward simulated environments for AI training, accelerating progress in agents and physical AI.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>In summary, 2025 marks GenAI&rsquo;s maturation phase, with global adoption accelerating amid innovation and caution. The path forward promises economic trillions but demands responsible stewardship to mitigate risks and maximize benefits.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Supercharge Reasoning in AI: Hands-On Chain of Thought Builds]]></title>
        <link href="https://rishijeet.github.io/blog/supercharge-reasoning-in-ai-hands-on-chain-of-thought-builds/"/>
        <updated>2025-08-29T13:26:07+05:30</updated>
        <id>https://rishijeet.github.io/blog/supercharge-reasoning-in-ai-hands-on-chain-of-thought-builds</id>
        <content type="html"><![CDATA[<p>Chain of Thought (CoT) is a prompting technique introduced in a 2022 paper by Google researchers (Wei et al., &ldquo;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&rdquo;). The core idea is simple: instead of asking an LLM for a direct answer, you instruct it to <strong>reason step by step</strong>. This elicits better performance on tasks requiring logic, math, commonsense, or multi-step planning.</p>

<p><img src="/images/2025/cot.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>For example:</p>

<ul>
<li><strong>Direct Prompt</strong>: &ldquo;What is 15% of 200?&rdquo;</li>
<li><strong>CoT Prompt</strong>: &ldquo;What is 15% of 200? Let&rsquo;s think step by step.&rdquo;</li>
</ul>


<p>The LLM might respond:</p>

<ul>
<li>&ldquo;Step 1: 15% means 15 per 100, so 15/100 = 0.15.</li>
<li>Step 2: Multiply by 200: 0.15 * 200 = 30. So, the answer is 30.&#8221;</li>
</ul>


<!--more-->


<p>This &ldquo;thinking&rdquo; process isn&rsquo;t magic—it&rsquo;s emergent from the model&rsquo;s training on vast datasets where step-by-step explanations are common. CoT shines in zero-shot (no examples) or few-shot (with examples) scenarios, and variants like <strong>Tree of Thoughts</strong> or <strong>Self-Consistency</strong> build on it for even more robustness.</p>

<a name="Why-Does-CoT-Work-3f-"></a>
<h3>Why Does CoT Work?</h3>

<ul>
<li><strong>Decomposes Complexity</strong>: Breaks problems into manageable sub-steps, reducing error rates.</li>
<li><strong>Transparency</strong>: Users see the &ldquo;thought process,&rdquo; building trust and allowing debugging.</li>
<li><strong>Scalability</strong>: Works with any LLM API; no need for fine-tuning.</li>
<li><strong>Applications</strong>: Math solvers, code debuggers, decision-making tools, chatbots that explain reasoning.</li>
</ul>


<p>Research shows CoT improves accuracy by 10-50% on benchmarks like GSM8K (math) or CommonsenseQA. In interactive apps, it can stream thoughts progressively, giving users a &ldquo;processing&rdquo; indicator.</p>

<a name="Evolution-and-Variants-of-CoT"></a>
<h2>Evolution and Variants of CoT</h2>

<p>CoT has evolved rapidly:</p>

<ul>
<li><strong>Zero-Shot CoT</strong>: Just add &ldquo;Let&rsquo;s think step by step&rdquo; to the prompt.</li>
<li><strong>Few-Shot CoT</strong>: Provide 2-5 examples of step-by-step reasoning before the query.</li>
<li><strong>Automatic CoT</strong>: Use LLMs to generate CoT examples dynamically.</li>
<li><strong>Tree of Thoughts (ToT)</strong>: Explores multiple reasoning paths like a tree search.</li>
<li><strong>Graph of Thoughts</strong>: Models reasoning as a graph for non-linear problems.</li>
</ul>


<p>In 2025, with models like Grok 4, CoT is often combined with tools (e.g., code execution or web search) for agentic systems—AI agents that plan, act, and reflect.</p>

<a name="Building-a-Chain-of-Thought-Application:-Step-2d-by-2d-Step-Guide"></a>
<h2>Building a Chain of Thought Application: Step-by-Step Guide</h2>

<p>To build a CoT application, we&rsquo;ll create a Python-based tool that:</p>

<ol>
<li>Takes user input.</li>
<li>Applies CoT prompting via an LLM API.</li>
<li>Streams the response to show &ldquo;thinking&rdquo; in real-time (using API streaming features).</li>
<li>Parses the final answer for clarity.</li>
</ol>


<p>We&rsquo;ll use OpenAI&rsquo;s API as an example. Assumptions:</p>

<ul>
<li>You have an API key.</li>
<li>Focus on a math/word problem solver, but extensible to any domain.</li>
</ul>


<a name="Step-1:-Set-Up-Your-Environment"></a>
<h3>Step 1: Set Up Your Environment</h3>

<p>Install required libraries:
<code>bash
pip install openai requests
</code></p>

<a name="Step-2:-Basic-CoT-Implementation"></a>
<h3>Step 2: Basic CoT Implementation</h3>

<p>Start with a simple non-streaming version. This script prompts the LLM with CoT and prints the full response.</p>

<pre><code class="python">import openai

# Set your API key
openai.api_key = "your-openai-api-key"  # Replace with actual key

def cot_reasoning(query, model="gpt-4o"):
    """
    Applies Chain of Thought prompting to a query.

    Args:
    - query (str): The user's question.
    - model (str): LLM model to use.

    Returns:
    - str: The reasoned response.
    """
    # CoT Prompt Template (Few-Shot for better results)
    prompt = """
    Solve the following problem step by step.

    Example 1:
    Question: If a car travels 60 miles in 1.5 hours, what is its speed?
    Step 1: Speed is distance divided by time.
    Step 2: Distance = 60 miles, Time = 1.5 hours.
    Step 3: Speed = 60 / 1.5 = 40 mph.
    Answer: 40 mph.

    Example 2:
    Question: What is the next number in the sequence: 2, 4, 8, 16?
    Step 1: Observe the pattern: each number is doubled.
    Step 2: 2 * 2 = 4, 4 * 2 = 8, 8 * 2 = 16.
    Step 3: Next is 16 * 2 = 32.
    Answer: 32.

    Now, your question:
    Question: {query}
    """

    formatted_prompt = prompt.format(query=query)

    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": formatted_prompt}]
    )

    return response.choices[0].message.content

# Usage
query = "What is 25% of 400?"
result = cot_reasoning(query)
print(result)
</code></pre>

<p><strong>Expected Output</strong>:
<code>
Step 1: 25% means 25 per 100, so 0.25.
Step 2: Multiply by 400: 0.25 * 400 = 100.
Answer: 100.
</code></p>

<p>This shows the &ldquo;thinking&rdquo; steps. To make it interactive, add a loop for multiple queries.</p>

<a name="Step-3:-Adding-Real-2d-Time-Processing-Feedback"></a>
<h3>Step 3: Adding Real-Time Processing Feedback</h3>

<p>To &ldquo;let you know that it is processing these steps,&rdquo; use streaming. OpenAI supports response streaming, printing tokens as they arrive—simulating thinking.</p>

<p>Modify the function:</p>

<pre><code class="python">import openai
import sys

openai.api_key = "your-openai-api-key"

def cot_streaming_reasoning(query, model="gpt-4o"):
    """
    Streams Chain of Thought reasoning in real-time.
    """
    prompt = """
    Solve the following problem step by step. Think out loud.

    # Few-shot examples here (same as above)

    Question: {query}
    """
    formatted_prompt = prompt.format(query=query)

    stream = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": formatted_prompt}],
        stream=True  # Enable streaming
    )

    print("Thinking...")
    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.get("content"):
            content = chunk.choices[0].delta.content
            sys.stdout.write(content)  # Print incrementally
            sys.stdout.flush()
            full_response += content
    print("\nDone!")

    return full_response

# Usage
query = "If I have 3 apples and eat 2, how many are left?"
cot_streaming_reasoning(query)
</code></pre>

<p><strong>How It Works</strong>:</p>

<ul>
<li>The <code>stream=True</code> parameter yields partial responses.</li>
<li>We print each chunk, showing steps like &ldquo;Step 1: &hellip;&rdquo; as they generate.</li>
<li>This creates a &ldquo;processing&rdquo; effect—users see thoughts unfolding.</li>
</ul>


<p>For a web app, use Flask or Streamlit to stream via WebSockets.</p>

<a name="Step-4:-Advanced-Features--e2--80--93--Parsing-and-Error-Handling"></a>
<h3>Step 4: Advanced Features – Parsing and Error Handling</h3>

<p>To extract the final answer reliably, parse the response. Add self-consistency by generating multiple CoTs and voting.</p>

<pre><code class="python">def parse_final_answer(response):
    """
    Extracts the final answer from CoT response.
    """
    lines = response.split("\n")
    for line in reversed(lines):
        if line.startswith("Answer:"):
            return line.split("Answer:")[1].strip()
    return "No clear answer found."

# In your main function:
result = cot_streaming_reasoning(query)
final_answer = parse_final_answer(result)
print(f"Final Answer: {final_answer}")
</code></pre>

<p>For robustness (Self-Consistency CoT):</p>

<ul>
<li>Run 3-5 CoT generations.</li>
<li>Use majority vote on answers.</li>
</ul>


<pre><code class="python">def self_consistent_cot(query, num_samples=3):
    answers = []
    for _ in range(num_samples):
        response = cot_reasoning(query)  # Or streaming version
        answer = parse_final_answer(response)
        answers.append(answer)

    # Simple majority vote
    from collections import Counter
    most_common = Counter(answers).most_common(1)
    return most_common[0][0] if most_common else "Inconsistent results"

# Usage
consistent_answer = self_consistent_cot("A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much is the ball?")
print(consistent_answer)  # Should be $0.05
</code></pre>

<a name="Step-5:-Building-a-Full-Application"></a>
<h3>Step 5: Building a Full Application</h3>

<p>For a complete app, use Streamlit for a UI:</p>

<pre><code class="python">import streamlit as st
import openai

openai.api_key = "your-openai-api-key"

st.title("Chain of Thought Reasoner")

query = st.text_input("Enter your question:")

if st.button("Reason"):
    with st.spinner("Thinking step by step..."):
        response = cot_streaming_reasoning(query)  # Use non-streaming for simplicity, or adapt
        st.write(response)
        final = parse_final_answer(response)
        st.success(f"Final Answer: {final}")
</code></pre>

<p>Run with <code>streamlit run app.py</code>. This creates a web interface where users input queries and see the CoT process.</p>

<a name="Challenges-and-Best-Practices"></a>
<h2>Challenges and Best Practices</h2>

<ul>
<li><strong>Token Limits</strong>: CoT increases prompt length; use efficient models.</li>
<li><strong>Bias in Reasoning</strong>: LLMs can hallucinate steps—validate with tools (e.g., code execution for math).</li>
<li><strong>Customization</strong>: For domains like code generation, add &ldquo;Step 1: Understand requirements. Step 2: Plan code structure.&rdquo;</li>
<li><strong>Integration with Agents</strong>: Combine CoT with ReAct (Reason + Act) for tool-using agents.</li>
<li><strong>Ethics</strong>: Ensure transparency; CoT doesn&rsquo;t make AI infallible.</li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>Chain of Thought applications transform LLMs from black boxes into transparent reasoners. By building step-by-step processing into your prompts and code, you create tools that not only solve problems but explain how.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Efficient Fine-Tuning of Large Language Models: A Deep Dive into LoRA and QLoRA]]></title>
        <link href="https://rishijeet.github.io/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora/"/>
        <updated>2025-08-17T18:27:01+05:30</updated>
        <id>https://rishijeet.github.io/blog/efficient-fine-tuning-of-large-language-models-a-deep-dive-into-lora-and-qlora</id>
        <content type="html"><![CDATA[<p>In the era of large language models (LLMs) like GPT-3 and Llama, fine-tuning these behemoths for specific tasks has become a cornerstone of AI development. However, traditional full fine-tuning demands enormous computational resources, often requiring hundreds of GBs of GPU memory and extensive training time. This is where parameter-efficient fine-tuning (PEFT) techniques shine, allowing us to adapt massive models with minimal overhead. Among these, Low-Rank Adaptation (LoRA) and its quantized variant, Quantized LoRA (QLoRA), stand out for their efficiency and effectiveness. In this technical blog, we&rsquo;ll explore the mechanics, mathematics, advantages, and practical implementations of LoRA and QLoRA, drawing from foundational research and real-world applications.</p>

<a name="Understanding-Fine-2d-Tuning-Challenges"></a>
<h2>Understanding Fine-Tuning Challenges</h2>

<p>Full fine-tuning involves updating all parameters of a pre-trained model on a downstream dataset, which maximizes performance but at a steep cost. For instance, fine-tuning a 175B-parameter model like GPT-3 requires retraining every weight, leading to high memory usage and deployment challenges. PEFT methods mitigate this by updating only a subset of parameters or adding lightweight adapters, reducing trainable parameters by orders of magnitude while preserving model quality.</p>

<p><img src="/images/2025/lora_qlora.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<!--more-->


<a name="What-is-LoRA-3f-"></a>
<h2>What is LoRA?</h2>

<p>LoRA, or Low-Rank Adaptation, was introduced by Microsoft researchers in 2021 as a method to fine-tune LLMs by injecting low-rank trainable matrices into the model&rsquo;s layers without altering the original weights. The core idea stems from the observation that weight updates during fine-tuning often lie in a low-dimensional subspace—meaning the changes to the model&rsquo;s weights have a low &ldquo;intrinsic rank.&rdquo; Instead of updating the full weight matrix, LoRA decomposes these updates into smaller, low-rank factors.</p>

<a name="How-LoRA-Works"></a>
<h3>How LoRA Works</h3>

<p>Consider a pre-trained weight matrix \( W_0 \in \mathbb{R}^{d \times k} \) in a Transformer layer (e.g., attention weights like \( W_q, W_k, W_v, W_o \)). During fine-tuning, the update is constrained as \( \Delta W = BA \), where \( B \in \mathbb{R}^{d \times r} \), \( A \in \mathbb{R}^{r \times k} \), and \( r \ll \min(d, k) \) is the rank (typically 1-64). The original weights \( W_0 \) remain frozen, and only \( A \) and \( B \) are trained.</p>

<p>The forward pass becomes:
\[
h = W_0 x + \frac{\alpha}{r} BA x
\]
where \( \alpha \) is a scaling factor (often set to twice the rank for stability), and the division by \( r \) normalizes the update magnitude. Initialization is key: \( A \) starts with random Gaussian values, while \( B \) is zeroed to ensure no initial change.</p>

<p>LoRA is typically applied to attention layers in Transformers, as empirical studies show this yields the best results with fewer parameters. For a model like GPT-3 175B, this reduces trainable parameters from billions to just thousands (e.g., 0.3M for r=8), slashing GPU memory needs by 3x and trainable parameters by 10,000x compared to full fine-tuning.</p>

<a name="Advantages-of-LoRA"></a>
<h3>Advantages of LoRA</h3>

<ul>
<li><strong>Efficiency</strong>: Training throughput increases due to fewer gradients and optimizer states. For example, on GPT-3, LoRA matches or exceeds full fine-tuning quality on benchmarks like RoBERTa and DeBERTa while using far less memory.</li>
<li><strong>Deployment Flexibility</strong>: Post-training, \( BA \) can be merged into \( W_0 \), incurring no inference latency. Multiple LoRA adapters can share the base model, enabling quick task-switching.</li>
<li><strong>Hyperparameter Tips</strong>: Common ranks are 4-32; alpha is often 2x rank. Libraries like Hugging Face&rsquo;s PEFT make integration seamless.</li>
</ul>


<a name="Introducing-QLoRA:-Quantization-Meets-LoRA"></a>
<h2>Introducing QLoRA: Quantization Meets LoRA</h2>

<p>While LoRA is efficient, fine-tuning still requires loading the full model in high-precision formats (e.g., FP16), which can exceed single-GPU limits for models over 30B parameters. QLoRA, proposed in 2023, extends LoRA by quantizing the base model to 4 bits, enabling fine-tuning of 65B+ models on a single 48GB GPU without performance loss.</p>

<a name="How-QLoRA-Builds-on-LoRA"></a>
<h3>How QLoRA Builds on LoRA</h3>

<p>QLoRA freezes a 4-bit quantized version of the pre-trained model and backpropagates gradients through it into LoRA adapters. Key innovations include:</p>

<ul>
<li><strong>4-bit NormalFloat (NF4) Quantization</strong>: An information-theoretically optimal data type for normally distributed weights. Weights are normalized to [-1, 1] and quantized into bins with equal expected values from a N(0,1) distribution, avoiding outliers.</li>
<li><strong>Double Quantization</strong>: Quantizes the quantization constants themselves (e.g., to 8-bit), saving ~0.37 bits per parameter by reducing constants&#8217; memory footprint.</li>
<li><strong>Paged Optimizers</strong>: Uses NVIDIA unified memory to page optimizer states to CPU RAM during spikes, preventing OOM errors.</li>
</ul>


<p>Mathematically, for a linear layer:</p>

<div class="math-wrap">
&#92;[
Y_{&#92;text{BF16}} = X_{&#92;text{BF16}} &#92;cdot &#92;text{doubleDequant}(c_{&#92;text{FP32}_1}, c_{k&#92;text{-bit}_2}, W_{&#92;text{NF4}})
+ X_{&#92;text{BF16}} &#92;cdot L_{&#92;text{BF16}_1} &#92;cdot L_{&#92;text{BF16}_2}
&#92;]
</div>


<p>Gradients are computed in BF16 for LoRA params (\( L_1, L_2 \)) only, while the quantized base remains frozen.</p>

<p>QLoRA matches 16-bit full fine-tuning on benchmarks like Vicuna, with models like Guanaco achieving 99.3% of ChatGPT&rsquo;s performance after 24 hours on one GPU.</p>

<a name="Advantages-of-QLoRA-Over-LoRA"></a>
<h3>Advantages of QLoRA Over LoRA</h3>

<ul>
<li><strong>Memory Savings</strong>: Reduces footprint from >780GB to &lt;48GB for 65B models, democratizing access.</li>
<li><strong>No Performance Trade-off</strong>: Unlike naive quantization, QLoRA preserves accuracy by fine-tuning adapters on high-quality data.</li>
<li><strong>Scalability</strong>: Enables fine-tuning on consumer hardware, with extensions like 8-bit integration in Hugging Face for even broader use.</li>
</ul>


<a name="Practical-Implementation-with-Hugging-Face"></a>
<h2>Practical Implementation with Hugging Face</h2>

<p>Hugging Face&rsquo;s libraries (Transformers, PEFT, Diffusers) simplify LoRA/QLoRA usage. For LoRA fine-tuning of Stable Diffusion:</p>

<pre><code class="python">from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
pipe.unet.enable_lora(rank=4)  # Enable LoRA with rank 4
# Train on dataset, then merge and infer
</code></pre>

<p>For QLoRA with Gemma:
&#8220;`python
from keras_nlp.models import GemmaLM
gemma_lm = GemmaLM.from_preset(&ldquo;gemma_2b_en&rdquo;)
gemma_lm.quantize(&ldquo;int8&rdquo;)  # Quantize to 8-bit (extendable to 4-bit)
gemma_lm.backbone.enable_lora(rank=4)</p>

<a name="Fine-2d-tune-and-evaluate"></a>
<h1>Fine-tune and evaluate</h1>

<p>&#8220;`</p>

<p>Recent tutorials emphasize dataset preparation and evaluation for vision-language models like QWEN2.5VL.</p>

<a name="Applications-and-Case-Studies"></a>
<h2>Applications and Case Studies</h2>

<p>LoRA/QLoRA power domain-specific adaptations, from chatbots (e.g., Guanaco) to image generation (e.g., Pokémon fine-tuning). In production, they&rsquo;ve enabled zero-shot learning via hypernetworks and optimized LLMs for edge devices. Studies from Lightning AI show QLoRA excelling in memory-constrained environments across hundreds of experiments.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>LoRA and QLoRA revolutionize LLM fine-tuning by making it accessible, efficient, and scalable. LoRA&rsquo;s low-rank decomposition minimizes parameters, while QLoRA&rsquo;s quantization pushes boundaries further, enabling massive models on modest hardware. As AI evolves, these techniques will be pivotal for customizing foundation models. Experiment with Hugging Face tools to harness their power in your projects.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[MLX vs CUDA: A Detailed Technical Comparison]]></title>
        <link href="https://rishijeet.github.io/blog/mlx-vs-cuda-a-detailed-technical-comparison/"/>
        <updated>2025-01-21T07:45:30+05:30</updated>
        <id>https://rishijeet.github.io/blog/mlx-vs-cuda-a-detailed-technical-comparison</id>
        <content type="html"><![CDATA[<p>Machine learning frameworks and technologies continue to evolve, leading to the rise of competing platforms designed to maximize performance, flexibility, and ease of use for modern AI workloads. Two prominent frameworks, MLX (Machine Learning Exchange) and CUDA (Compute Unified Device Architecture), are often compared in terms of performance and functionality. This article provides a detailed exploration of the differences between MLX and CUDA, focusing on their architecture, usability, and benchmarking scores.</p>

<a name="L-3c-strong-3e-What-is-CUDA-3f--3c--2f-strong-3e-"></a>
<h3><strong>What is CUDA?</strong></h3>

<p>CUDA is a parallel computing platform and programming model developed by NVIDIA, specifically designed for NVIDIA GPUs. It allows developers to use C, C++, Fortran, and Python to write applications that can leverage GPU acceleration. CUDA provides low-level access to the GPU hardware, enabling high performance for applications like deep learning, scientific computing, and high-performance simulations.</p>

<p><img src="/images/2025/cuda.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>Key features of CUDA:</p>

<ul>
<li><strong>Low-level optimization</strong>: Offers direct control over GPU memory and thread management.</li>
<li><strong>Rich ecosystem</strong>: Integrated with libraries like cuDNN, NCCL, and TensorRT.</li>
<li><strong>Highly mature</strong>: Over a decade of optimizations and wide industry adoption.</li>
</ul>


<!--more-->


<a name="L-3c-strong-3e-What-is-MLX-3f--3c--2f-strong-3e-"></a>
<h3><strong>What is MLX?</strong></h3>

<p>MLX (Machine Learning Exchange) is an emerging platform that abstracts machine learning and deep learning workflows. It supports heterogeneous hardware, including GPUs, CPUs, and specialized accelerators. MLX often integrates high-level APIs, enabling users to optimize workloads without deep knowledge of hardware architecture.</p>

<p><img src="/images/2025/mlx.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>Key features of MLX:</p>

<ul>
<li><strong>Cross-platform support</strong>: Runs on multiple hardware types.</li>
<li><strong>High-level abstraction</strong>: Simplifies model training and deployment.</li>
<li><strong>Integration-friendly</strong>: Works well with TensorFlow, PyTorch, and ONNX.</li>
</ul>


<a name="L-3c-strong-3e-Architecture-3c--2f-strong-3e-"></a>
<h2><strong>Architecture</strong></h2>

<ul>
<li><p><strong>CUDA</strong>:</p>

<ul>
<li>CUDA provides fine-grained control over GPU execution.</li>
<li>Thread and memory management are handled explicitly, giving developers the ability to maximize performance through detailed tuning.</li>
<li>Works exclusively on NVIDIA GPUs, leveraging specialized hardware like Tensor Cores.</li>
</ul>
</li>
<li><p><strong>MLX</strong>:</p>

<ul>
<li>MLX abstracts the underlying hardware, making it easier for developers to build models without focusing on device-specific optimizations.</li>
<li>Supports a variety of hardware, including GPUs (NVIDIA, AMD), CPUs, and emerging accelerators like TPUs.</li>
<li>Focuses on portability over deep hardware-specific optimizations.</li>
</ul>
</li>
</ul>


<a name="L-3c-strong-3e-Performance-Benchmarks-3c--2f-strong-3e-"></a>
<h2><strong>Performance Benchmarks</strong></h2>

<p>Benchmarking was conducted to evaluate the performance of MLX and CUDA on several machine learning workloads. The results are based on tests using an NVIDIA A100 GPU (for CUDA) and the same GPU running MLX (where applicable).</p>

<p><img src="/images/2025/cuda_mlx_benchmark.png" height="300" width="900" alt="Alt text" /></p>

<p>The performance benchmark chart above highlights the comparison between CUDA and MLX for training throughput and inference latency across three tasks: Image Classification, Object Detection, and Transformer Models.</p>

<ul>
<li>Training Throughput: CUDA consistently achieves higher throughput (bars on the left for each task), demonstrating
its fine-grained optimization for NVIDIA GPUs.</li>
<li>Inference Latency: CUDA also exhibits lower latency (lines with green markers) compared to MLX (lines with red
markers), showcasing its efficiency in real-time workloads.
This visualization emphasizes CUDA&rsquo;s advantage in both raw performance and latency, particularly on NVIDIA GPUs, while MLX offers competitive results with a broader hardware compatibility.

<a name="L-3c-strong-3e-Key-Observations-3c--2f-strong-3e-"></a>
<h3><strong>Key Observations</strong></h3></li>
<li>CUDA consistently outperformed MLX in raw throughput and latency due to its hardware-specific optimizations and direct access to NVIDIA GPU architecture.</li>
<li>MLX’s performance was competitive, particularly for workflows prioritizing hardware-agnostic support.</li>
<li>The performance gap was more pronounced in tasks involving fine-grained GPU operations, such as training BERT or running YOLOv5.</li>
</ul>


<a name="L-3c-strong-3e-Energy-Efficiency-3c--2f-strong-3e-"></a>
<h3><strong>Energy Efficiency</strong></h3>

<p>Energy consumption was measured for both frameworks during the benchmarks.
<img src="/images/2025/cuda_mlx_efficiency.png" height="300" width="900" alt="Alt text" /></p>

<p>Here is the graphical representation of the energy efficiency comparison between CUDA and MLX. It highlights:</p>

<ul>
<li>The average power consumption (W) for each framework (shown as bars).</li>
<li>The energy efficiency (images/sec/W) (shown as a line plot).</li>
</ul>


<p>CUDA demonstrated better energy efficiency due to optimized GPU utilization and reduced overhead.</p>

<a name="L-3c-strong-3e-Use-Cases-3c--2f-strong-3e-"></a>
<h2><strong>Use Cases</strong></h2>

<ul>
<li><p><strong>CUDA</strong>:</p>

<ul>
<li>Ideal for applications requiring peak performance, such as autonomous vehicles, financial modeling, and real-time simulations.</li>
<li>Suitable for research and production environments where NVIDIA GPUs are the standard.</li>
</ul>
</li>
<li><p><strong>MLX</strong>:</p>

<ul>
<li>Best suited for teams working across heterogeneous hardware environments or those prioritizing ease of use.</li>
<li>Effective for organizations building portable machine learning solutions for diverse infrastructure.</li>
</ul>
</li>
</ul>


<a name="L-3c-strong-3e-Conclusion-3c--2f-strong-3e-"></a>
<h2><strong>Conclusion</strong></h2>

<p>CUDA remains the gold standard for GPU-accelerated machine learning, offering unparalleled performance and efficiency. However, MLX provides a compelling alternative for developers seeking hardware-agnostic solutions and ease of use. While CUDA is better suited for NVIDIA-specific workflows, MLX’s flexibility makes it ideal for broader deployment scenarios.</p>

<p>Ultimately, the choice between MLX and CUDA depends on your specific requirements: if peak performance on NVIDIA GPUs is critical, CUDA is the clear choice. For portability and simplicity, MLX offers significant advantages.</p>
]]></content>
    </entry>
    
</feed>
