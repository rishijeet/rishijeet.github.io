<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: llm | Rishijeet Mishra]]></title>
    <link href="https://rishijeet.github.io/blog/categories/llm/atom.xml" rel="self"/>
    <link href="https://rishijeet.github.io/"/>
    <updated>2025-05-27T23:33:43+05:30</updated>
    <id>https://rishijeet.github.io/</id>
    <author>
        <name><![CDATA[Rishijeet Mishra]]></name>
        <email><![CDATA[rishijeet@gmail.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Smartcase Engine: A Modern Framework for Intelligent Case Management]]></title>
        <link href="https://rishijeet.github.io/blog/smartcase-engine-a-modern-framework-for-intelligent-case-management/"/>
        <updated>2025-05-27T22:54:42+05:30</updated>
        <id>https://rishijeet.github.io/blog/smartcase-engine-a-modern-framework-for-intelligent-case-management</id>
        <content type="html"><![CDATA[<p>In today&rsquo;s dynamic business environment, efficient case management is paramount. Enter <a href="https://github.com/rishijeet/smartcase-engine">Smartcase Engine</a>, an advanced case management framework designed to streamline complex case handling through real-time tracking, efficient workflows, and automated decision-making processes.</p>

<a name="What-is-Smartcase-Engine-3f-"></a>
<h2>What is Smartcase Engine?</h2>

<p>Smartcase Engine is a modular, microservices-based platform tailored for managing intricate case workflows. It offers:</p>

<ul>
<li><strong>Real-Time Case Tracking</strong>: Monitor cases as they progress through various stages.</li>
<li><strong>Efficient Workflows</strong>: Automate and optimize the sequence of tasks involved in case resolution.</li>
<li><strong>Automated Decision-Making</strong>: Leverage predefined rules and AI to make informed decisions without manual intervention.</li>
</ul>


<p><img src="/images/2025/smartcase_engine.png" height="300" width="900" alt="Alt text" /><em>Source: <a href="https://github.com/rishijeet/smartcase-engine">Rishijeet Mishra&rsquo;s Blog</a></em></p>

<!--more-->


<a name="Architectural-Overview"></a>
<h2>Architectural Overview</h2>

<p>Great! Let&rsquo;s dive deeper into each component of the <strong>Architectural Overview</strong> section of the <code>smartcase-engine</code> repo and explain their roles, interactions, and technical underpinnings in a more detailed, blog-friendly format.</p>

<hr />

<a name="Deep-Dive:-Smartcase-Engine-Architecture"></a>
<h2>Deep Dive: Smartcase Engine Architecture</h2>

<p>At the heart of Smartcase Engine is a clean, extensible <strong>microservices-based architecture</strong> designed to support complex workflows in case/dispute management scenarios. The system is broken into discrete services that communicate via REST APIs and Kafka for event-driven interactions. This modular approach allows teams to scale and evolve components independently.</p>

<p>Let’s explore the core services that power this engine:</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Intake-Service-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Intake Service</strong></h3>

<p><strong>Purpose</strong>: This is the gateway to the system — the service responsible for accepting new cases or disputes.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Receive new dispute cases via API or event (Kafka).</li>
<li>Validate and enrich the incoming payload.</li>
<li>Generate a unique dispute ID.</li>
<li>Store initial metadata and emit an event to kick off downstream workflow.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Built using Java + Quarkus for lightweight runtime.</li>
<li>Connects to Kafka for emitting intake-completed events.</li>
<li>Persists initial data in a database (e.g., PostgreSQL or any pluggable DB).</li>
<li>Implements REST endpoints for manual intake testing or system integration.</li>
</ul>


<p><strong>Why it matters</strong>:
This service ensures that all disputes entering the system are properly structured and immediately traceable — forming the root of all subsequent orchestration.</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Workflow-Service-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Workflow Service</strong></h3>

<p><strong>Purpose</strong>: This is the <em>brain</em> of the engine, orchestrating the lifecycle of a dispute across multiple business stages.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Define and manage the state machine (or BPMN-style flow) for a dispute.</li>
<li>Trigger actions based on status updates (e.g., escalate, resolve, pause).</li>
<li>Call external services when required (e.g., fetch additional metadata, update agent queue).</li>
<li>Track state transitions and support retries/failures.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Uses a stateless orchestration model.</li>
<li>Integration with Kafka allows event-driven step progression.</li>
<li>Could be integrated with BPMN engines (like Camunda or Flowable) for visual modeling.</li>
</ul>


<p><strong>Why it matters</strong>:
This is the system&rsquo;s core engine. It allows Smartcase to define complex, non-linear workflows without hardcoding logic into the intake or agent UI layers.</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Classification-Service-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Classification Service</strong></h3>

<p><strong>Purpose</strong>: Adds intelligence to the process by classifying disputes into appropriate categories.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Use business rules or ML models to assign dispute types (e.g., &ldquo;billing error&rdquo;, &ldquo;fraud&rdquo;, &ldquo;product defect&rdquo;).</li>
<li>Optionally flag high-risk or high-priority disputes.</li>
<li>Feed classification results back into the workflow service to route the case accordingly.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Stateless classification service.</li>
<li>Integrates with a basic rule engine or external ML service (could be backed by Python/ONNX, or a local inference server).</li>
<li>Input: structured dispute metadata. Output: classification code or tag.</li>
</ul>


<p><strong>Why it matters</strong>:
Classification drives automation. By programmatically tagging and triaging disputes, Smartcase avoids human bottlenecks and supports intelligent queue assignment.</p>

<hr />

<a name="L-3c-strong-3e-Agent-UI-Service-3c--2f-strong-3e-"></a>
<h3><strong>Agent UI Service</strong></h3>

<p><strong>Purpose</strong>: The interface between human agents and the system.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Display dispute data and current status.</li>
<li>Allow agents to take actions (e.g., approve, reject, escalate).</li>
<li>Show workflow progression.</li>
<li>Track comments, attachments, and communication logs.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Frontend (typically in React or Angular).</li>
<li>Backend proxy or BFF layer in Quarkus serving data via REST.</li>
<li>Authenticated access with role-based views (agent, supervisor, auditor).</li>
<li>Pagination, search, filters, and sort capabilities to handle large volumes.</li>
</ul>


<p><strong>Why it matters</strong>:
No matter how automated the backend is, disputes often need human judgment. This UI is purpose-built for efficiency and transparency in resolution workflows.</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Common-Module-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Common Module</strong></h3>

<p><strong>Purpose</strong>: A shared library of core utilities and contracts.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Define POJOs (Plain Old Java Objects) and DTOs (Data Transfer Objects).</li>
<li>Common validation logic.</li>
<li>Central configuration definitions.</li>
<li>Shared Kafka event schema.</li>
<li>Error handling standards and API response models.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Packaged as a reusable JAR.</li>
<li>Imported by all services as a dependency.</li>
<li>Promotes DRY principles and API consistency.</li>
</ul>


<p><strong>Why it matters</strong>:
Microservices need to stay loosely coupled — but shared types and utilities must remain consistent. This module enforces a standard language across the ecosystem.</p>

<hr />

<a name="Optional-Add-2d-ons--28-Future-Ready-29-"></a>
<h3>Optional Add-ons (Future Ready)</h3>

<p>Depending on your scale and use case, Smartcase Engine can be extended with:</p>

<ul>
<li><strong>Notification Service</strong>: For sending SMS/email alerts to users or agents.</li>
<li><strong>Audit Logging</strong>: For compliance with financial or legal audits.</li>
<li><strong>Retry/Dead-letter Queue Mechanism</strong>: To gracefully handle transient failures.</li>
<li><strong>Observability Tools</strong>: Integrated with Prometheus, Grafana, and OpenTelemetry for distributed tracing.</li>
</ul>


<hr />

<a name="Deployment-and-Setup"></a>
<h2>Deployment and Setup</h2>

<p>Smartcase Engine is containerized using Docker, facilitating seamless deployment. Key scripts and configurations include:</p>

<ul>
<li><strong>Dockerfile</strong>: Defines the environment for each microservice.</li>
<li><strong>docker-compose.yml</strong>: Orchestrates multi-container deployments for local development and testing.</li>
<li><strong>build-and-deploy.sh</strong>: Automates the build and deployment process.</li>
<li><strong>start-services.sh</strong>: Initiates all services concurrently.</li>
</ul>


<p>To get started:</p>

<pre><code class="bash">git clone https://github.com/rishijeet/smartcase-engine.git
cd smartcase-engine
chmod +x build-and-deploy.sh start-services.sh
./build-and-deploy.sh
./start-services.sh
</code></pre>

<a name="Testing-and-Validation"></a>
<h2>Testing and Validation</h2>

<p>Each microservice includes unit and integration tests to ensure reliability. The modular design allows for isolated testing, simplifying debugging and maintenance.</p>

<a name="Potential-Use-Cases"></a>
<h2>Potential Use Cases</h2>

<p>Smartcase Engine&rsquo;s versatility makes it suitable for various domains:</p>

<ul>
<li><strong>Financial Services</strong>: Automating dispute resolutions in banking and insurance.</li>
<li><strong>Customer Support</strong>: Managing customer complaints and service requests.</li>
<li><strong>Legal Case Management</strong>: Tracking legal cases, evidence, and proceedings.</li>
<li><strong>Healthcare</strong>: Handling patient grievances and administrative cases.([GitHub][1])</li>
</ul>


<a name="Contributing-to-Smartcase-Engine"></a>
<h2>Contributing to Smartcase Engine</h2>

<p>The project welcomes contributions from the developer community. To contribute:</p>

<ol>
<li>Fork the repository.</li>
<li>Create a new branch for your feature or bugfix.</li>
<li>Ensure code quality by running existing tests and adding new ones if necessary.</li>
<li>Submit a pull request with a clear description of your changes.</li>
</ol>


<a name="Further-Reading"></a>
<h2>Further Reading</h2>

<p>For more in-depth information:</p>

<ul>
<li><a href="https://github.com/rishijeet/smartcase-engine">Smartcase Engine GitHub Repository</a></li>
<li><a href="https://martinfowler.com/articles/microservices.html">Microservices Architecture</a></li>
<li><a href="https://docs.docker.com/">Docker Documentation</a></li>
<li><a href="https://www.omg.org/bpmn/">BPMN Standards</a></li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>Smartcase Engine exemplifies how modern architectural principles can be harnessed to build robust, scalable, and efficient case management systems. Whether you&rsquo;re looking to streamline dispute resolutions or manage complex workflows, Smartcase Engine offers a solid foundation to build upon.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Model Context Protocol (MCP): The Backbone of Dynamic AI Workflows]]></title>
        <link href="https://rishijeet.github.io/blog/model-context-protocol-mcp-the-backbone-of-dynamic-ai-workflows/"/>
        <updated>2025-04-08T23:14:14+05:30</updated>
        <id>https://rishijeet.github.io/blog/model-context-protocol-mcp-the-backbone-of-dynamic-ai-workflows</id>
        <content type="html"><![CDATA[<p>As the AI landscape rapidly evolves, the demand for systems that support <strong>modular</strong>, <strong>context-aware</strong>, and <strong>efficient orchestration</strong> of models has grown. Enter the <strong>Model Context Protocol (MCP)</strong> — a rising standard that enables dynamic, multi-agent AI systems to exchange context, manage state, and chain model invocations intelligently.</p>

<p>In this article, we’ll explore what MCP is, why it matters, and how it’s becoming a key component in the infrastructure stack for advanced AI applications. We’ll also walk through a conceptual example of building an MCP-compatible server.</p>

<a name="What-is-the-Model-Context-Protocol--28-MCP-29--3f-"></a>
<h2>What is the Model Context Protocol (MCP)?</h2>

<p><strong>MCP</strong> is a protocol designed to manage the <strong>contextual state</strong> of AI models across requests in multi-agent, multi-model environments. It’s part of a broader effort to make LLMs (Large Language Models) more <strong>stateful</strong>, <strong>collaborative</strong>, and <strong>task-aware</strong>.</p>

<p>At its core, MCP provides:</p>

<ul>
<li>A way to <strong>pass and maintain context</strong> (like conversation history, task progress, or shared knowledge) across AI agents or model calls.</li>
<li>A standardized protocol to support <strong>chained inference</strong>, where multiple models collaborate on subtasks.</li>
<li>Support for <strong>stateful computation</strong>, which is critical in complex reasoning or long-running workflows.</li>
</ul>


<p><img src="/images/2025/mcp_server" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<!--more-->


<a name="Why-is-MCP-Relevant-Now-3f-"></a>
<h2>Why is MCP Relevant Now?</h2>

<p>The growing interest in <strong>AI agents</strong>, <strong>function-calling APIs</strong>, and <strong>model interoperability</strong> has created a pressing need for something like MCP. Some trends driving MCP adoption include:</p>

<style>
  .trend-impact-table {
    width: 100%;
    border-collapse: collapse;
    font-family: Arial, sans-serif;
  }
  .trend-impact-table th,
  .trend-impact-table td {
    text-align: left;
    padding: 12px;
    border-bottom: 1px solid #ccc;
    vertical-align: top;
  }
  .trend-impact-table th {
    background-color: #f2f2f2;
    width: 25%;
  }
</style>




<table class="trend-impact-table">
  <thead>
    <tr>
      <th>Trend</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Agentic Workflows</td>
      <td>Models need shared context to collaborate efficiently (e.g., ReAct, AutoGPT, BabyAGI).</td>
    </tr>
    <tr>
      <td>LLM Orchestration Frameworks</td>
      <td>Tools like LangChain, Semantic Kernel, and OpenDevin push for context-aware memory and model chaining.</td>
    </tr>
    <tr>
      <td>Open Model Ecosystems</td>
      <td>Efforts like Hugging Face&#8217;s Inference Endpoints, vLLM, and Modal want to standardize inference behavior.</td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation (RAG)</td>
      <td>Persistent context and metadata handling are vital for grounded reasoning.</td>
    </tr>
  </tbody>
</table>


<p>Leading companies like <strong>OpenAI (via ChatGPT APIs)</strong>, <strong>Anthropic (via Claude’s memory)</strong>, and <strong>Mistral</strong> are integrating ideas from MCP implicitly, if not through standardized APIs.</p>

<a name="Core-Concepts-of-MCP"></a>
<h2>Core Concepts of MCP</h2>

<p>An MCP server typically supports the following concepts:</p>

<a name="L-3c-strong-3e-Model-Context-3c--2f-strong-3e-"></a>
<h3><strong>Model Context</strong></h3>

<pre><code class="json">{
  "session_id": "abc-123",
  "user_id": "user-456",
  "context": {
    "history": [
      { "role": "user", "content": "Generate a project plan." },
      { "role": "assistant", "content": "Sure, here's a draft..." }
    ],
    "task": "project_planning",
    "dependencies": ["retrieval_plugin", "summarizer_model"]
  }
}
</code></pre>

<a name="L-3c-strong-3e-Model-Invocation-with-Context-3c--2f-strong-3e-"></a>
<h3><strong>Model Invocation with Context</strong></h3>

<pre><code class="json">{
  "model": "gpt-4",
  "input": "What are the next steps?",
  "context_ref": "abc-123",
  "metadata": {
    "requested_capability": "planning.summarize"
  }
}
</code></pre>

<a name="L-3c-strong-3e-Chained-Outputs-and-Shared-State-3c--2f-strong-3e-"></a>
<h3><strong>Chained Outputs and Shared State</strong></h3>

<p>Each model contributes to a shared state, stored either in an in-memory store (like Redis) or a structured store (like Postgres + pgvector for embeddings).</p>

<a name="Building-a-Basic-MCP-Server"></a>
<h2>Building a Basic MCP Server</h2>

<p>Let’s outline what a minimal MCP-compatible server might look like using <strong>FastAPI</strong> and <strong>Redis</strong>.</p>

<a name="Basic-Server-with-Context-Store"></a>
<h3>Basic Server with Context Store</h3>

<pre><code class="python">from fastapi import FastAPI, Request
import redis
import uuid
import json

app = FastAPI()
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

@app.post("/invoke")
async def invoke_model(request: Request):
    payload = await request.json()
    context_ref = payload.get("context_ref")
    input_text = payload["input"]
    model = payload["model"]

    # Load context
    context = json.loads(r.get(context_ref)) if context_ref else {}
    history = context.get("history", [])

    # Simulate model response
    history.append({"role": "user", "content": input_text})
    response = f"Simulated response to: {input_text}"
    history.append({"role": "assistant", "content": response})

    # Save updated context
    new_context_ref = context_ref or str(uuid.uuid4())
    r.set(new_context_ref, json.dumps({"history": history}))

    return {"output": response, "context_ref": new_context_ref}
</code></pre>

<a name="Add-Capability-Metadata"></a>
<h3>Add Capability Metadata</h3>

<p>Enhance the server to log requested capabilities and dependency resolution (e.g., invoking tools or submodels).</p>

<pre><code class="python">capability = payload.get("metadata", {}).get("requested_capability")
log_event(user_id, session_id, model, capability)
</code></pre>

<hr />

<a name="MCP-vs-Alternatives"></a>
<h2>MCP vs Alternatives</h2>

<p>MCP aims to serve as the <strong>underlying protocol</strong>, while frameworks like LangChain act as <strong>developer tooling on top</strong>.</p>

<style>
  .comparison-table {
    width: 100%;
    border-collapse: collapse;
    font-family: Arial, sans-serif;
  }
  .comparison-table th,
  .comparison-table td {
    text-align: center;
    padding: 12px;
    border-bottom: 1px solid #ccc;
    width: 20%;
  }
  .comparison-table th {
    background-color: #f2f2f2;
  }
</style>




<table class="comparison-table">
  <thead>
    <tr>
      <th>Feature</th>
      <th>MCP</th>
      <th>LangChain</th>
      <th>Semantic Kernel</th>
      <th>ChatML (OpenAI)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Context Persistence</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
      <td>Partial</td>
    </tr>
    <tr>
      <td>Model-Agnostic</td>
      <td>✅</td>
      <td>❌<br><small>(Python-specific)</small></td>
      <td>✅</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>Stateful Memory</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
      <td>Partial</td>
    </tr>
    <tr>
      <td>Chaining Support</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>Explicit Protocol</td>
      <td>✅</td>
      <td>❌</td>
      <td>❌</td>
      <td>✅<br><small>(format only)</small></td>
    </tr>
  </tbody>
</table>


<a name="Adoption-and-Ecosystem-Signals"></a>
<h2>Adoption and Ecosystem Signals</h2>

<ul>
<li><strong>LangChain and LlamaIndex</strong>: Moving towards standardizing memory interfaces with composable context.</li>
<li><strong>OpenAI’s Assistant API</strong>: Explicitly supports persistent threads, similar to MCP session_id and shared memory.</li>
<li><strong>Anthropic&rsquo;s Memory Plans</strong>: Incorporates long-term memory slots, resembling MCP’s context model.</li>
<li><strong>Meta’s Multi-Agent Research (2024)</strong>: Proposes architectures that are context-routing centric — aligning with MCP’s goals.</li>
</ul>


<a name="Challenges-and-Future-Directions"></a>
<h2>Challenges and Future Directions</h2>

<a name="Technical-Challenges"></a>
<h3>Technical Challenges</h3>

<ul>
<li>Efficient context storage and retrieval at scale.</li>
<li>Dynamic resolution of capabilities and tool invocation.</li>
<li>Real-time chaining with latency constraints.</li>
</ul>


<a name="What-e2--80--99-s-Next-3f-"></a>
<h3>What’s Next?</h3>

<ul>
<li><strong>Open spec for MCP</strong>: Standardization akin to OpenAPI or GraphQL.</li>
<li><strong>Plugin Interop</strong>: Tool APIs that conform to context-aware interfaces.</li>
<li><strong>LLMOps Integration</strong>: Tracking usage, debugging flows, and observability in agentic systems.</li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The <strong>Model Context Protocol</strong> is a foundational building block for the next wave of <strong>AI-native applications</strong>. It abstracts and manages the complexity of context, model chaining, and agent collaboration — enabling AI systems that behave less like stateless endpoints and more like intelligent software agents.</p>

<p>As the AI ecosystem matures, MCP (whether explicitly named or not) will become central to orchestrating rich, multi-turn, multi-model AI systems.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[High-Flyer: Pioneering AI in Finance]]></title>
        <link href="https://rishijeet.github.io/blog/high-flyer-pioneering-ai-in-finance/"/>
        <updated>2025-03-30T20:13:12+05:30</updated>
        <id>https://rishijeet.github.io/blog/high-flyer-pioneering-ai-in-finance</id>
        <content type="html"><![CDATA[<p>In the rapidly evolving landscape of artificial intelligence (AI), China&rsquo;s DeepSeek has emerged as a formidable contender, challenging established players and redefining industry standards. This ascent is deeply intertwined with High-Flyer, an AI-driven quantitative hedge fund whose strategic investments and visionary leadership have propelled DeepSeek to the forefront of AI innovation.</p>

<p><img src="/images/2025/deepseek.jpg" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>Founded in February 2016 by Liang Wenfeng, High-Flyer—officially known as Hangzhou Huanfang Technology Ltd Co.—quickly distinguished itself in the financial sector by leveraging AI models for investment decisions. By late 2017, AI systems managed the majority of High-Flyer&rsquo;s trading activities, solidifying its reputation as a leader in AI-driven stock trading. The firm&rsquo;s portfolio burgeoned to an impressive 100 billion yuan (approximately $13.79 billion), underscoring the efficacy of its AI-centric strategies.</p>

<!--more-->


<p><img src="/images/2025/high_flyer.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Strategic-Investments-in-Computing-Power"></a>
<h2>Strategic Investments in Computing Power</h2>

<p>Anticipating the critical role of computational resources in AI advancement, Liang Wenfeng initiated substantial investments in high-performance hardware. Between 2020 and 2021, High-Flyer constructed two AI supercomputing clusters comprising Nvidia&rsquo;s A100 graphics processing units (GPUs). The first cluster, operational in 2020, integrated 1,100 A100 chips at a cost of 200 million yuan.
The subsequent year saw the completion of a second, more expansive cluster with approximately 10,000 A100 chips, representing a 1 billion yuan investment. These strategic acquisitions occurred prior to the U.S. government&rsquo;s 2022 restrictions on exporting advanced chips to China, positioning High-Flyer advantageously amid tightening technological embargoes.</p>

<p><img src="/images/2025/quant_china.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Transition-to-Artificial-General-Intelligence--28-AGI-29-"></a>
<h2>Transition to Artificial General Intelligence (AGI)</h2>

<p>In 2023, High-Flyer announced a pivotal shift towards pursuing artificial general intelligence (AGI), aiming to develop autonomous systems capable of outperforming humans in most economically valuable tasks. This initiative led to the establishment of DeepSeek as an independent research entity dedicated to exploring the essence of AGI. Liang Wenfeng assumed a leadership role in DeepSeek, guiding its strategic direction and research endeavors.</p>

<p><img src="/images/2025/hedge_fund_mania.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p><img src="/images/2025/high_flyer_return.avif" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="DeepSeek-27-s-Breakthroughs-and-Industry-Impact"></a>
<h2>DeepSeek&rsquo;s Breakthroughs and Industry Impact</h2>

<p>DeepSeek&rsquo;s trajectory has been marked by a series of groundbreaking AI models:</p>

<ul>
<li><p><strong>DeepSeek-V2 (May 2024):</strong> This chatbot model gained widespread acclaim in China for its cost-efficiency and superior performance, outperforming offerings from major tech companies such as ByteDance, Tencent, Baidu, and Alibaba. Its release triggered a price war, compelling competitors to significantly reduce prices on their AI models.</p></li>
<li><p><strong>DeepSeek-V3 (December 2024):</strong> Building upon its predecessor, DeepSeek-V3 further enhanced capabilities and solidified DeepSeek&rsquo;s dominance in the AI sector.</p></li>
<li><p><strong>DeepSeek-R1 (January 2025):</strong> This reasoning model and its associated chatbot application marked DeepSeek&rsquo;s entry into the international market, challenging the prevailing assumption of U.S. dominance in AI.</p></li>
</ul>


<p>The sophistication of DeepSeek&rsquo;s models has garnered praise from Silicon Valley competitors, a first for a Chinese AI model. Notably, DeepSeek claims to have achieved these advancements using a fraction of the computing power deployed by leading U.S. firms, a revelation that contributed to a global selloff of tech shares.</p>

<a name="Navigating-Challenges-and-Future-Outlook"></a>
<h2>Navigating Challenges and Future Outlook</h2>

<p>Despite its rapid ascent, DeepSeek faces challenges, particularly concerning access to advanced computing hardware due to export restrictions. Liang Wenfeng has expressed concerns about the embargo on high-end chips, acknowledging it as a significant hurdle. Nevertheless, DeepSeek continues to innovate, relying on existing resources, efficiency improvements, and exploring domestic alternatives.</p>

<p>DeepSeek&rsquo;s success has also influenced China&rsquo;s financial markets. The Chinese stock market experienced a resurgence in investor interest, with equity issuance doubling in the first quarter of 2025 compared to the previous year, reaching $16.8 billion. This optimism is partly driven by the emergence of DeepSeek, which offers AI products at lower costs, encouraging global investors to reconsider China&rsquo;s potential despite ongoing trade tensions.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>DeepSeek&rsquo;s meteoric rise, underpinned by High-Flyer&rsquo;s strategic foresight and investment in AI, exemplifies China&rsquo;s growing prowess in the global AI landscape. By prioritizing research over immediate commercial profit and fostering a meritocratic environment, DeepSeek has not only achieved remarkable technological advancements but also challenged existing paradigms, signaling a shift towards a more diversified and competitive global AI ecosystem.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Buddhi: Pushing the Boundaries of Long-Context Open-Source AI]]></title>
        <link href="https://rishijeet.github.io/blog/buddhi-pushing-the-boundaries-of-long-context-open-source-ai/"/>
        <updated>2025-03-25T08:24:38+05:30</updated>
        <id>https://rishijeet.github.io/blog/buddhi-pushing-the-boundaries-of-long-context-open-source-ai</id>
        <content type="html"><![CDATA[<p>AI Planet has introduced Buddhi-128K-Chat-7B, an open-source chat model distinguished by its expansive 128,000-token context window. This advancement enables the model to process and retain extensive contextual information, enhancing its performance in tasks requiring deep context understanding.</p>

<p><img src="/images/2025/buddhi.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Model-Architecture"></a>
<h2>Model Architecture</h2>

<p>Buddhi-128K-Chat-7B is fine-tuned from the Mistral-7B Instruct v0.2 base model, selected for its superior reasoning capabilities. The Mistral-7B architecture incorporates features such as Grouped-Query Attention and a Byte-fallback BPE tokenizer, originally supporting a maximum of 32,768 position embeddings. To extend this to 128K, the Yet another Rope Extension (YaRN) technique was employed, modifying positional embeddings to accommodate the increased context length.</p>

<!--more-->


<a name="Dataset-Composition"></a>
<h2>Dataset Composition</h2>

<p>The training dataset comprises three sections tailored for chat model development:</p>

<ul>
<li><strong>Stack Exchange Data</strong>: Consists of question-and-answer pairs, refined using the Mistral model to enhance
formatting for chat applications.</li>
<li><strong>PG19-Based Data with Alpaca Formatting</strong>: Utilizes the PG19 dataset as context, with question-answer pairs
generated by GPT-3.</li>
<li><strong>PG19-Based Data with GPT-4</strong>: Similar to the previous section but with question-answer pairs generated by GPT-4,
ensuring a diverse conversational scope.</li>
</ul>


<p>This strategic composition ensures comprehensive coverage of dialogue scenarios, optimizing the model&rsquo;s performance across various contexts.</p>

<a name="Benchmark-Performance"></a>
<h2>Benchmark Performance</h2>

<p>Buddhi-128K-Chat-7B has been evaluated using both short and long context benchmarks:</p>

<p><strong>Short Context Benchmarks</strong>: The model&rsquo;s performance on tasks like HellaSwag, ARC Challenge, MMLU, TruthfulQA, and Winogrande has been assessed, with metrics available on the Open LLM Leaderboard.</p>

<p><img src="/images/2025/buddhi_bench.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p><strong>Long Context Benchmarks</strong>: Evaluations on datasets such as Banking77 have demonstrated the model&rsquo;s capability to handle extensive context effectively.</p>

<p><img src="/images/2025/buddhi_bench_lc.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>These benchmarks indicate that Buddhi-128K-Chat-7B matches or surpasses other models in its size class, particularly in handling long-context scenarios.</p>

<a name="Inference-and-Hardware-Requirements"></a>
<h2>Inference and Hardware Requirements</h2>

<p>To utilize the full 128K context length, the following hardware specifications are recommended:</p>

<ul>
<li><strong>128K Context Length</strong>: Requires 80GB VRAM, with A100 GPUs preferred.</li>
<li><strong>32K Context Length</strong>: Requires 40GB VRAM, with A100 GPUs preferred.</li>
</ul>


<p><img src="/images/2025/a100.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>For optimized inference, integrating vLLM, which employs Paged Attention to reduce memory footprint, is advisable. Additionally, bitsandbytes quantization can enable the model to run on GPUs with lower VRAM, such as T4 GPUs.</p>

<a name="Use-Cases-and-Applications"></a>
<h2>Use Cases and Applications</h2>

<p>The extended context window of Buddhi-128K-Chat-7B unlocks several practical applications:</p>

<ul>
<li><strong>Enhanced Memory and Recall</strong>: The model can reference earlier parts of a conversation, leading to more coherent and natural dialogues.</li>
<li><strong>Accurate Instruction Following</strong>: Capable of retaining and executing multi-step instructions without missing details.</li>
<li><strong>Efficient Workflow Automation</strong>: Suitable for processing large datasets in fields like legal document review and medical records analysis.</li>
<li><strong>Improved Coherence in Text Generation</strong>: Able to generate long-form content without losing context, ensuring consistency throughout the text.</li>
<li><strong>Deep Analysis and Insight Generation</strong>: Facilitates comprehensive analysis in research-intensive fields by understanding extensive documents in a single pass.</li>
</ul>


<p>While Buddhi-128K-Chat-7B offers significant advancements, it also presents challenges such as increased computational requirements and potential latency issues. Ongoing research and development are expected to address these limitations, further enhancing the model&rsquo;s efficiency and accessibility.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>In conclusion, Buddhi-128K-Chat-7B represents a notable step forward in open-source chat models, offering an extended context window that enhances its applicability across various domains requiring deep contextual understanding.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[The Role of GPUs in Large Language Models (LLMs): Types, Requirements & Costs]]></title>
        <link href="https://rishijeet.github.io/blog/the-role-of-gpus-in-large-language-models-llms/"/>
        <updated>2024-07-03T10:32:17+05:30</updated>
        <id>https://rishijeet.github.io/blog/the-role-of-gpus-in-large-language-models-llms</id>
        <content type="html"><![CDATA[<p>Large Language Models (LLMs) like GPT-3, BERT, and T5 have revolutionized natural language processing (NLP). However, training and fine-tuning these models require substantial computational resources. Graphics Processing Units (GPUs) are critical in this context, providing the necessary power to handle the vast amounts of data and complex calculations involved. In this blog, we will explore why GPUs are essential for LLMs, the types of GPUs required, and the associated costs.</p>

<p><img src="/images/2024/nvidia_a100.jpg" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Why-GPUs-are-Essential-for-LLMs"></a>
<h3>Why GPUs are Essential for LLMs</h3>

<ul>
<li> <strong>Parallel Processing</strong>

<ul>
<li>GPUs excel at parallel processing, allowing them to handle multiple computations simultaneously. This capability is
crucial for training LLMs, which involve large-scale matrix multiplications and operations on high-dimensional tensors.</li>
</ul>
</li>
<li> <strong>High Throughput</strong>

<ul>
<li>GPUs offer high computational throughput, significantly speeding up the training process. This is vital for LLMs,
which require processing vast datasets and performing numerous iterations to achieve optimal performance.</li>
</ul>
</li>
<li> <strong>Memory Bandwidth</strong>

<ul>
<li>Training LLMs involves frequent data transfer between the processor and memory. GPUs provide high memory bandwidth,
facilitating the rapid movement of large amounts of data, which is essential for efficient training.</li>
</ul>
</li>
<li> <strong>Optimized Libraries</strong>

<ul>
<li>Many deep learning frameworks (e.g., TensorFlow, PyTorch) offer GPU-optimized libraries, enabling efficient
implementation of complex neural network operations and reducing training time.</li>
</ul>
</li>
</ul>


<!--more-->


<p><img src="/images/2024/nvidia_time_sol.svg" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Types-of-GPUs-Required-for-LLMs"></a>
<h3>Types of GPUs Required for LLMs</h3>

<p>Different LLM tasks have varying computational requirements, and the choice of GPU depends on the model size, dataset size, and specific application. Here are some common GPU types used for LLMs:</p>

<p><strong>NVIDIA A100:</strong></p>

<ul>
<li><strong>Overview:</strong> The NVIDIA A100 is designed for high-performance computing and AI workloads. It is based on the Ampere architecture and offers exceptional performance for training and inference of LLMs.</li>
<li><strong>Key Features:</strong>

<ul>
<li>6912 CUDA cores</li>
<li>40 GB or 80 GB HBM2 memory</li>
<li>Up to 1.6 TB/s memory bandwidth</li>
<li>Multi-instance GPU (MIG) technology for partitioning into smaller, independent GPUs</li>
</ul>
</li>
<li><strong>Cost:</strong> Approximately $10,000 - $15,000 per GPU</li>
</ul>


<p><strong>NVIDIA V100:</strong></p>

<ul>
<li><strong>Overview:</strong> The NVIDIA V100, based on the Volta architecture, is a widely used GPU for deep learning and AI. It
provides excellent performance for training large-scale models.</li>
<li><strong>Key Features:</strong>

<ul>
<li>5120 CUDA cores</li>
<li>16 GB or 32 GB HBM2 memory</li>
<li>Up to 900 GB/s memory bandwidth</li>
<li>Tensor Cores for accelerating matrix operations</li>
</ul>
</li>
<li><strong>Cost:</strong> Approximately $8,000 - $12,000 per GPU</li>
</ul>


<p><strong>NVIDIA T4:</strong></p>

<ul>
<li><strong>Overview:</strong> The NVIDIA T4 is optimized for inference and low-power applications. It offers a good balance of
performance and cost, making it suitable for deploying LLMs.</li>
<li><strong>Key Features:</strong>

<ul>
<li>2560 CUDA cores</li>
<li>16 GB GDDR6 memory</li>
<li>Up to 320 GB/s memory bandwidth</li>
<li>Low power consumption (70W)</li>
</ul>
</li>
<li><strong>Cost:</strong> Approximately $2,000 - $3,000 per GPU</li>
</ul>


<p><strong>NVIDIA RTX 3090:</strong></p>

<ul>
<li><strong>Overview:</strong> The NVIDIA RTX 3090 is a consumer-grade GPU that provides high performance for deep learning tasks.
It is based on the Ampere architecture and is popular among researchers and enthusiasts.</li>
<li><strong>Key Features:</strong>

<ul>
<li>10496 CUDA cores</li>
<li>24 GB GDDR6X memory</li>
<li>Up to 936 GB/s memory bandwidth</li>
</ul>
</li>
<li><strong>Cost:</strong> Approximately $1,500 - $2,500 per GPU</li>
</ul>


<p><img src="/images/2024/nvidia_perf.svg" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Cost-Considerations"></a>
<h3>Cost Considerations</h3>

<p>The cost of GPUs varies based on their performance, memory capacity, and features. Here are some factors to consider when budgeting for GPUs in LLM projects:</p>

<p><strong>Performance Needs:</strong></p>

<ul>
<li>Higher-end GPUs like the NVIDIA A100 and V100 are suitable for large-scale training but come at a higher cost.
For smaller tasks or inference, more affordable options like the T4 or RTX 3090 might suffice.</li>
</ul>


<p><strong>Scalability:</strong></p>

<ul>
<li>Consider the scalability of your setup. If you plan to scale up your operations, investing in higher-end GPUs
might provide better long-term value due to their superior performance and efficiency.</li>
</ul>


<p><strong>Cloud vs. On-Premise:</strong></p>

<ul>
<li>Cloud providers (e.g., AWS, Google Cloud, Azure) offer GPU instances, allowing you to pay for usage rather than
upfront costs. This can be cost-effective for short-term projects or when starting.</li>
</ul>


<p><strong>Total Cost of Ownership:</strong></p>

<ul>
<li>Factor in additional costs such as electricity, cooling, and maintenance when running GPUs on-premise. These
operational costs can add up, especially for high-power GPUs.</li>
</ul>


<p>While NVIDIA is the dominant player in the GPU market, there are indeed other companies that produce GPUs. However, NVIDIA&rsquo;s significant presence in the deep learning and AI sectors often overshadows these competitors. Let&rsquo;s explore some of these companies, their offerings, and why they are less frequently discussed in the context of LLMs.</p>

<a name="Other-GPU-Manufacturers"></a>
<h3>Other GPU Manufacturers</h3>

<p><strong>AMD (Advanced Micro Devices):</strong></p>

<ul>
<li><strong>Overview:</strong> AMD is a well-known player in the GPU market, offering both consumer and professional-grade GPUs
under the Radeon and Radeon Pro brands.</li>
<li><strong>Key Products:</strong>

<ul>
<li><strong>Radeon RX Series:</strong> Consumer GPUs aimed at gaming but also used for deep learning tasks.</li>
<li><strong>Radeon Pro Series:</strong> Professional GPUs designed for content creation, CAD, and scientific computing.</li>
</ul>
</li>
<li><strong>Why Less Prominent for LLMs:</strong> AMD GPUs are generally not as optimized for deep learning frameworks as NVIDIA&rsquo;s.
CUDA, NVIDIA&rsquo;s parallel computing platform, is widely supported and has become the industry standard, giving NVIDIA an edge in the AI space.</li>
</ul>


<p><strong>Intel:</strong></p>

<ul>
<li><strong>Overview:</strong> Intel, primarily known for its CPUs, has also ventured into the GPU market with its Xe graphics
architecture.</li>
<li><strong>Key Products:</strong>

<ul>
<li><strong>Intel Iris Xe:</strong> Integrated and discrete GPUs aimed at mainstream computing tasks.</li>
<li><strong>Intel Xeon Phi:</strong> Co-processors designed for high-performance computing tasks, including AI and machine learning.</li>
</ul>
</li>
<li><strong>Why Less Prominent for LLMs:</strong> Intel&rsquo;s GPUs are relatively new entrants to the market and lack the extensive ecosystem and software support that NVIDIA GPUs enjoy. Additionally, Intel&rsquo;s focus has traditionally been on CPUs, making their GPUs less prominent in the AI and deep learning communities.</li>
</ul>


<p><strong>Google (TPUs - Tensor Processing Units):</strong></p>

<ul>
<li><strong>Overview:</strong> Google developed TPUs specifically for accelerating machine learning workloads. These are not
traditional GPUs but are worth mentioning due to their specialized role in AI.</li>
<li><strong>Key Products:</strong>

<ul>
<li><strong>TPU v4:</strong> The latest generation of TPUs, designed for both training and inference of large models.</li>
</ul>
</li>
<li><strong>Why Less Prominent for General Use:</strong> TPUs are primarily available through Google Cloud and are tailored for Google&rsquo;s ecosystem. They are not as widely accessible as NVIDIA GPUs for general-purpose deep learning tasks.</li>
</ul>


<p><strong>Huawei (Ascend):</strong></p>

<ul>
<li><strong>Overview:</strong> Huawei produces AI processors under the Ascend brand, designed for deep learning and AI workloads.</li>
<li><strong>Key Products:</strong>

<ul>
<li><strong>Ascend 910:</strong> A high-performance AI processor aimed at training large models.</li>
</ul>
</li>
<li><strong>Why Less Prominent:</strong> Huawei&rsquo;s market presence is more regional, and their products are not as widely adopted globally compared to NVIDIA&rsquo;s offerings.</li>
</ul>


<a name="Why-NVIDIA-Dominates-the-LLM-Space"></a>
<h3>Why NVIDIA Dominates the LLM Space</h3>

<p><strong>CUDA Ecosystem:</strong></p>

<ul>
<li><strong>Software Support:</strong> CUDA has become the de facto standard for parallel computing in deep learning. Most deep
learning frameworks, such as TensorFlow and PyTorch, are highly optimized for CUDA.</li>
<li><strong>Libraries and Tools:</strong> NVIDIA provides a rich set of libraries (cuDNN, NCCL, TensorRT) and tools that simplify the development and deployment of deep learning models.</li>
</ul>


<p><strong>Performance:</strong></p>

<ul>
<li><strong>Specialized Hardware:</strong> NVIDIA&rsquo;s GPUs are equipped with Tensor Cores specifically designed for accelerating
deep learning tasks, providing superior performance for training large models.</li>
<li><strong>Scalability:</strong> NVIDIA&rsquo;s NVLink and multi-GPU setups enable efficient scaling of deep learning workloads, essential for training LLMs.</li>
</ul>


<p><strong>Industry Adoption:</strong></p>

<ul>
<li><strong>Research and Development:</strong> Many leading research institutions and tech companies use NVIDIA GPUs, resulting in
a wealth of community knowledge, tutorials, and research papers centered around NVIDIA hardware.</li>
<li><strong>Cloud Integration:</strong> Major cloud providers (AWS, Google Cloud, Azure) offer extensive support for NVIDIA GPUs, making them accessible for scalable deep learning applications.</li>
</ul>


<a name="Conclusion"></a>
<h3>Conclusion</h3>

<p>GPUs are indispensable for training and fine-tuning Large Language Models due to their parallel processing capabilities, high throughput, and optimized performance for deep learning tasks. Selecting the right GPU involves balancing performance needs, budget constraints, and scalability requirements. High-end GPUs like the NVIDIA A100 and V100 are ideal for large-scale training, while more affordable options like the T4 and RTX 3090 are suitable for smaller tasks and inference.</p>

<p>By understanding the different types of GPUs and their costs, you can make informed decisions that align with your LLM project goals, ensuring efficient and cost-effective model development and deployment.</p>
]]></content>
    </entry>
    
</feed>
