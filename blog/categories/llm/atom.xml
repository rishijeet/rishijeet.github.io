<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: llm | Rishijeet Mishra]]></title>
    <link href="https://rishijeet.github.io/blog/categories/llm/atom.xml" rel="self"/>
    <link href="https://rishijeet.github.io/"/>
    <updated>2025-07-27T21:00:52+05:30</updated>
    <id>https://rishijeet.github.io/</id>
    <author>
        <name><![CDATA[Rishijeet Mishra]]></name>
        <email><![CDATA[rishijeet@gmail.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Energy Requirements for AI Infrastructure: Current and Future Impacts]]></title>
        <link href="https://rishijeet.github.io/blog/energy-requirements-for-ai-infrastructure-current-and-future-impacts/"/>
        <updated>2025-07-26T21:16:34+05:30</updated>
        <id>https://rishijeet.github.io/blog/energy-requirements-for-ai-infrastructure-current-and-future-impacts</id>
        <content type="html"><![CDATA[<p>The rapid expansion of artificial intelligence (AI), particularly large language models (LLMs) and generative AI, has driven an unprecedented surge in energy demand due to the computational intensity of training and operating these systems. Eric Schmidt, former Google CEO, has highlighted electricity as the primary limiter of AI growth, estimating that the U.S. will require an additional 92 gigawatts (GW) of power‚Äîequivalent to the output of 92 nuclear power plants‚Äîto sustain the AI revolution. This analysis explores the current energy consumption of major companies‚Äô AI infrastructure, projects future energy needs through 2035, and examines how these demands will reshape the energy sector, drawing on available data from web sources and posts on X.</p>

<a name="Current-Energy-Consumption-by-Major-Companies"></a>
<h2>Current Energy Consumption by Major Companies</h2>

<a name="Overview"></a>
<h3>Overview</h3>

<p>Major tech companies, or ‚Äúhyperscalers‚Äù (e.g., Microsoft, Google, Meta, Amazon, OpenAI), are the primary drivers of AI infrastructure energy demand, operating massive data centers for training and inference of AI models. Training a single state-of-the-art AI model, such as OpenAI‚Äôs GPT-4, can consume 50 gigawatt-hours (GWh) of electricity, equivalent to the annual energy use of 4,800 U.S. households. Inference (running AI models for user queries) is also energy-intensive, with a single ChatGPT query requiring approximately 2.9 watt-hours, nearly 10 times that of a Google search (0.3 watt-hours). Below is an overview of key players‚Äô energy footprints based on available data:</p>

<p><img src="/images/2025/Screenshot%202025-07-26%20at%208.43.24%E2%80%AFPM.png" height="300" width="900" alt="Alt text" /></p>

<!--more-->


<ul>
<li><p><strong>Microsoft</strong>:</p>

<ul>
<li><strong>Current Usage</strong>: Microsoft‚Äôs data centers, which support Azure and AI initiatives like Copilot, consumed an estimated 17 GW of power globally in 2023, with AI workloads accounting for a growing share. Training a single large model can require 50-100 GWh, and inference for millions of daily queries adds significantly to this demand.</li>
<li><strong>Initiatives</strong>: Microsoft has committed to purchasing 10.5 GW of renewable energy from Brookfield Asset Management between 2026 and 2030 to power its AI data centers, marking one of the largest corporate renewable energy deals to date.<a href="https://www.energypolicy.columbia.edu/projecting-the-electricity-demand-growth-of-generative-ai-large-language-models-in-the-us/"></a></li>
<li><strong>Specific Projects</strong>: Microsoft‚Äôs deal to buy power from the reopened Three Mile Island nuclear reactor (Constellation Energy) will provide 835 MW of carbon-free power for 20 years, targeting AI data center needs.<a href="https://energy.mit.edu/news/the-multi-faceted-challenge-of-powering-ai/"></a></li>
</ul>
</li>
<li><p><strong>Google</strong>:</p>

<ul>
<li><strong>Current Usage</strong>: Google‚Äôs global data center power consumption is estimated at 15-20 GW, with AI workloads (e.g., Gemini models) contributing 14% of this demand in 2023. Google expects to spend $75 billion on AI infrastructure in 2025, much of which will go toward energy-intensive data centers.<a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/"></a><a href="https://www.goldmansachs.com/insights/articles/ai-to-drive-165-increase-in-data-center-power-demand-by-2030"></a></li>
<li><strong>Initiatives</strong>: Google is investing in renewable energy and has issued a roadmap emphasizing AI‚Äôs potential to drive a ‚Äúnew era of American innovation‚Äù through productivity gains, but acknowledges that energy demands will outpace current infrastructure.<a href="https://www.city-journal.org/article/artificial-intelligence-energy-electricity-demand"></a></li>
</ul>
</li>
<li><p><strong>Meta</strong>:</p>

<ul>
<li><strong>Current Usage</strong>: Meta‚Äôs planned $10 billion AI data center in Louisiana, set to begin operations in 2028, will require 2 GW for computation alone, with additional power for cooling. This single facility‚Äôs energy demand is equivalent to that of 200,000 households.<a href="https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/"></a></li>
<li><strong>Initiatives</strong>: Meta is investing over $200 million in infrastructure (roads, water systems) to support its data centers and claims to cover the full cost of utility grid upgrades to avoid passing costs to consumers.<a href="https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/"></a></li>
</ul>
</li>
<li><p><strong>OpenAI</strong>:</p>

<ul>
<li><strong>Current Usage</strong>: Training GPT-4 consumed an estimated 50 GWh, enough to power San Francisco for three days. With millions of daily queries, OpenAI‚Äôs inference energy demand is significant, though exact figures are undisclosed due to the company‚Äôs lack of transparency.<a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/"></a><a href="https://www.forbes.com/sites/arielcohen/2024/05/23/ai-is-pushing-the-world-towards-an-energy-crisis/"></a></li>
<li><strong>Initiatives</strong>: OpenAI‚Äôs Stargate initiative, announced with President Donald Trump, aims to build data centers consuming up to 5 GW each, with a total investment of $500 billion. This project could require 15-25 GW across multiple facilities, equivalent to the power needs of a small state.<a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/"></a><a href="https://www.cfr.org/blog/america-may-not-need-massive-energy-build-out-power-ai-revolution"></a></li>
</ul>
</li>
<li><p><strong>Amazon</strong>:</p>

<ul>
<li><strong>Current Usage</strong>: Amazon Web Services (AWS) operates over 100 data centers globally, with an estimated power consumption of 20 GW in 2023. AI workloads, including Amazon Bedrock and custom AI chips, are driving increased energy use, though specific AI-related figures are not publicly detailed.</li>
<li><strong>Initiatives</strong>: Amazon is investing in renewable energy and energy-efficient data center designs but faces challenges in scaling power supply to meet AI-driven demand.<a href="https://www.nature.com/articles/d41586-025-00616-z"></a></li>
</ul>
</li>
</ul>


<a name="Aggregate-Industry-Impact"></a>
<h3>Aggregate Industry Impact</h3>

<ul>
<li><strong>Global Data Center Demand</strong>: In 2023, global data centers consumed 240-340 terawatt-hours (TWh), or 1-1.3% of world electricity demand, with AI-specific data centers accounting for approximately 14 GW of additional capacity.<a href="https://www.energypolicy.columbia.edu/projecting-the-electricity-demand-growth-of-generative-ai-large-language-models-in-the-us/"></a><a href="https://www.nature.com/articles/d41586-025-00616-z"></a></li>
<li><strong>U.S. Data Center Demand</strong>: U.S. data centers used 176 TWh (4.4% of national electricity) in 2023, with AI workloads driving a significant portion. By 2027, AI data centers alone could require 68 GW globally, nearly matching California‚Äôs 2022 total power capacity of 86 GW.<a href="https://www.rand.org/pubs/research_reports/RRA3572-1.html"></a><a href="https://energy.mit.edu/news/the-multi-faceted-challenge-of-powering-ai/"></a></li>
</ul>


<a name="Future-Energy-Needs--28-2025-2d-2035-29-"></a>
<h2>Future Energy Needs (2025-2035)</h2>

<a name="Projections"></a>
<h3>Projections</h3>

<p>The energy demands of AI are expected to grow exponentially due to increasing model complexity, widespread adoption, and the shift toward AI ‚Äúagents‚Äù that perform tasks autonomously. Key projections include:</p>

<p><img src="/images/2025/Screenshot%202025-07-26%20at%208.43.39%E2%80%AFPM.png" height="300" width="900" alt="Alt text" /></p>

<ul>
<li><p><strong>By 2027</strong>:</p>

<ul>
<li><strong>Global Demand</strong>: AI data centers could require 68 GW of power, doubling global data center power requirements from 2022. Training runs for advanced models may demand up to 1 GW per location, with inference needs growing as AI agents handle billions of daily queries.<a href="https://www.rand.org/pubs/research_reports/RRA3572-1.html"></a></li>
<li><strong>U.S. Demand</strong>: Goldman Sachs Research forecasts a 50% increase in data center power demand by 2027, with AI contributing 27% of the total (up from 14% in 2023). This translates to 84 GW globally, with the U.S. accounting for a significant share.<a href="https://www.goldmansachs.com/insights/articles/ai-to-drive-165-increase-in-data-center-power-demand-by-2030"></a></li>
<li><strong>Eric Schmidt‚Äôs Estimate</strong>: Schmidt‚Äôs claim of an additional 92 GW needed in the U.S. aligns with projections for AI-driven demand, equivalent to powering four to five projects on the scale of OpenAI‚Äôs Stargate (15-25 GW).<a href="https://www.cfr.org/blog/america-may-not-need-massive-energy-build-out-power-ai-revolution"></a></li>
</ul>
</li>
<li><p><strong>By 2030</strong>:</p>

<ul>
<li><strong>Global Demand</strong>: The International Energy Agency (IEA) projects global data center electricity demand to reach 945 TWh (slightly more than Japan‚Äôs current consumption), with AI-optimized data centers quadrupling in power needs.<a href="https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to-transform-how-the-energy-sector-works"></a></li>
<li><strong>U.S. Demand</strong>: U.S. data centers are expected to account for nearly half of the growth in national electricity demand, rising from 3-4% of total power in 2023 to 11-12% by 2030. This could require an additional 50 GW of capacity, with investments exceeding $500 billion in data center infrastructure alone (excluding grid upgrades).<a href="https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power"></a></li>
<li><strong>Training Needs</strong>: By 2030, training a single large AI model could demand up to 8 GW, equivalent to eight nuclear reactors, if current scaling trends continue.<a href="https://www.rand.org/pubs/research_reports/RRA3572-1.html"></a></li>
</ul>
</li>
<li><p><strong>By 2035</strong>:</p>

<ul>
<li><strong>Global Demand</strong>: If AI adoption and computational growth continue at current rates (doubling every 100 days), data center power demand could grow 160-165% from 2023 levels, reaching 3-4% of global electricity.<a href="https://www.goldmansachs.com/insights/articles/ai-to-drive-165-increase-in-data-center-power-demand-by-2030"></a><a href="https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demand"></a></li>
<li><strong>U.S. Demand</strong>: The U.S. could see data center power consumption triple, potentially requiring 100 GW of additional capacity, as suggested by Schmidt‚Äôs estimate and supported by Duke University‚Äôs analysis of existing grid ‚Äúheadroom.‚Äù<a href="https://www.cfr.org/blog/america-may-not-need-massive-energy-build-out-power-ai-revolution"></a></li>
</ul>
</li>
</ul>


<a name="Energy-Bottlenecks"></a>
<h3>Energy Bottlenecks</h3>

<p><img src="/images/2025/Screenshot%202025-07-26%20at%208.46.11%E2%80%AFPM.png" height="300" width="900" alt="Alt text" /></p>

<ul>
<li><strong>Infrastructure Constraints</strong>: The U.S. power grid faces challenges in congestion, reliability, and transmission capacity. Current grid upgrades are insufficient to meet AI-driven demand, with new transmission line construction dropping from 4,000 miles in 2013 to 1,000 miles annually today.<a href="https://www.energypolicy.columbia.edu/projecting-the-electricity-demand-growth-of-generative-ai-large-language-models-in-the-us/"></a><a href="https://www.forbes.com/sites/arielcohen/2024/05/23/ai-is-pushing-the-world-towards-an-energy-crisis/"></a></li>
<li><strong>Energy Source Limitations</strong>: Renewable energy (solar, wind) is intermittent, requiring expensive storage solutions or backup gas/diesel generators, which conflict with zero-carbon goals. Nuclear power, while reliable, takes nearly a decade to scale, making it a medium-term solution.<a href="https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power"></a><a href="https://energy.mit.edu/news/the-multi-faceted-challenge-of-powering-ai/"></a></li>
<li><strong>Geographic Challenges</strong>: Data centers are concentrated in regions like Northern Virginia (consuming electricity equivalent to 800,000 homes), straining local grids and causing price hikes for residents.<a href="https://www.forbes.com/sites/arielcohen/2024/05/23/ai-is-pushing-the-world-towards-an-energy-crisis/"></a></li>
</ul>


<a name="Shaping-the-Future-Energy-Landscape"></a>
<h2>Shaping the Future Energy Landscape</h2>

<a name="Strategies-to-Meet-Demand"></a>
<h3>Strategies to Meet Demand</h3>

<p><img src="/images/2025/Screenshot%202025-07-26%20at%208.46.26%E2%80%AFPM.png" height="300" width="900" alt="Alt text" /></p>

<ul>
<li><strong>Renewable Energy Investments</strong>: Companies like Microsoft, Google, and Amazon are investing heavily in renewables. Microsoft‚Äôs 10.5 GW deal and Google‚Äôs $75 billion AI infrastructure budget signal a shift toward carbon-free energy, though intermittency remains a challenge.<a href="https://www.energypolicy.columbia.edu/projecting-the-electricity-demand-growth-of-generative-ai-large-language-models-in-the-us/"></a><a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/"></a></li>
<li><strong>Nuclear Power Revival</strong>: Hyperscalers are turning to nuclear energy for reliable, high-capacity power. Microsoft‚Äôs Three Mile Island deal and Meta‚Äôs exploration of nuclear options in Louisiana highlight this trend. Small modular reactors and geothermal energy are also being evaluated.<a href="https://energy.mit.edu/news/the-multi-faceted-challenge-of-powering-ai/"></a><a href="https://www.rand.org/pubs/research_reports/RRA3572-1.html"></a></li>
<li><strong>Energy Efficiency</strong>: Innovations like Google‚Äôs 40% reduction in data center energy use through AI-driven cooling and more efficient chips (e.g., DeepSeek‚Äôs potential efficiency gains) could mitigate demand growth. Shared data centers and cloud computing can further reduce energy waste.<a href="https://www.weforum.org/stories/2024/04/how-to-manage-ais-energy-demand-today-tomorrow-and-in-the-future/"></a></li>
<li><strong>Grid Flexibility</strong>: Duke University‚Äôs report suggests that existing U.S. grid ‚Äúheadroom‚Äù could support 100 GW of new data center capacity if consumption is managed during peak hours. This requires data centers to adopt flexible load scheduling.<a href="https://www.cfr.org/blog/america-may-not-need-massive-energy-build-out-power-ai-revolution"></a></li>
</ul>


<a name="Economic-and-Social-Impacts"></a>
<h3>Economic and Social Impacts</h3>

<p><img src="/images/2025/Screenshot%202025-07-26%20at%208.46.38%E2%80%AFPM.png" height="300" width="900" alt="Alt text" /></p>

<ul>
<li><strong>Cost Increases</strong>: The $500 billion+ investment in data center infrastructure could lead to higher electricity bills for consumers, as utilities pass on grid upgrade costs. Meta‚Äôs Louisiana project, for instance, may raise rates after 15 years if gas turbines remain in use.<a href="https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/"></a></li>
<li><strong>Geopolitical Implications</strong>: If U.S. energy constraints persist, companies may build data centers abroad (e.g., Malaysia), risking national security and AI leadership. Schmidt‚Äôs emphasis on energy as a bottleneck underscores the need for domestic investment to maintain competitiveness.<a href="https://www.rand.org/pubs/research_reports/RRA3572-1.html"></a></li>
<li><strong>Environmental Concerns</strong>: AI‚Äôs energy demands could double data center carbon emissions by 2030, with a ‚Äúsocial cost‚Äù of $125-140 billion unless mitigated by renewables or nuclear power. Natural gas plants, planned to meet immediate needs, could lock in emissions for decades.<a href="https://www.goldmansachs.com/insights/articles/ai-to-drive-165-increase-in-data-center-power-demand-by-2030"></a><a href="https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/"></a></li>
</ul>


<a name="Policy-and-Innovation-Needs"></a>
<h3>Policy and Innovation Needs</h3>

<ul>
<li><strong>Government Action</strong>: President Biden‚Äôs Executive Order (January 2025) aims to accelerate AI infrastructure development by leasing federal lands for gigawatt-scale data centers powered by clean energy. The Defense Production Act could address energy shortfalls.<a href="https://rpower1.com/articles/the-power-of-ai-building-the-energy-infrastructure-for-americas-ai-revolution/"></a></li>
<li><strong>Infrastructure Upgrades</strong>: Over $800 billion in transmission and distribution investments are needed in Europe alone, with similar needs in the U.S. to support AI growth.<a href="https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demand"></a></li>
<li><strong>Transparency</strong>: Researchers and the IEA call for greater transparency from AI companies on energy consumption to improve planning and mitigate environmental impacts.<a href="https://www.nature.com/articles/d41586-025-00616-z"></a><a href="https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to-transform-how-the-energy-sector-works"></a></li>
</ul>


<a name="Critical-Analysis"></a>
<h2>Critical Analysis</h2>

<ul>
<li><strong>Schmidt‚Äôs 92 GW Estimate</strong>: While ambitious, Schmidt‚Äôs projection aligns with estimates like RAND‚Äôs 68 GW by 2027 and Goldman Sachs‚Äô 100 GW by 2030, though it assumes aggressive AI adoption. Duke University‚Äôs claim of existing grid capacity suggests this demand could be met without massive new infrastructure, but only with significant flexibility in data center operations.<a href="https://www.cfr.org/blog/america-may-not-need-massive-energy-build-out-power-ai-revolution"></a><a href="https://www.rand.org/pubs/research_reports/RRA3572-1.html"></a><a href="https://www.goldmansachs.com/insights/articles/ai-to-drive-165-increase-in-data-center-power-demand-by-2030"></a></li>
<li><strong>Uncertainties</strong>: Estimates vary widely due to unpredictable AI efficiency gains (e.g., DeepSeek‚Äôs potential), adoption rates, and infrastructure development timelines. Overbuilding natural gas plants risks stranded assets if AI demand slows.<a href="https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/"></a></li>
<li><strong>Environmental Trade-offs</strong>: The reliance on natural gas to meet immediate needs conflicts with zero-carbon goals, potentially locking in emissions for decades. Nuclear and renewable solutions are promising but face scalability and cost challenges.<a href="https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/"></a><a href="https://energy.mit.edu/news/the-multi-faceted-challenge-of-powering-ai/"></a></li>
<li><strong>Data Gaps</strong>: Lack of transparency from companies like OpenAI hinders accurate forecasting. More granular data on training and inference energy use is needed to refine projections.<a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/"></a></li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The AI revolution, driven by hyperscalers like Microsoft, Google, Meta, Amazon, and OpenAI, is pushing energy demands to unprecedented levels, with current U.S. data center consumption at 4.4% of national electricity and projected to reach 11-12% by 2030. Schmidt‚Äôs estimate of an additional 92 GW by 2035 underscores the scale of the challenge, requiring investments in renewables, nuclear power, and grid upgrades. While innovations in efficiency and flexible grid management offer hope, the risk of cost increases, environmental impacts, and geopolitical shifts looms large. To maintain AI leadership and sustainability, the U.S. must prioritize energy infrastructure development, transparency, and strategic policy interventions to balance growth with environmental and economic stability.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[From Text to Tokens: The Complete Guide to Tokenization in LLMs]]></title>
        <link href="https://rishijeet.github.io/blog/from-text-to-tokens-the-complete-guide-to-tokenization-in-llms/"/>
        <updated>2025-06-28T08:55:51+05:30</updated>
        <id>https://rishijeet.github.io/blog/from-text-to-tokens-the-complete-guide-to-tokenization-in-llms</id>
        <content type="html"><![CDATA[<p>In the ever-evolving field of artificial intelligence, large language models (LLMs) like GPT-4, Claude, Gemini, and LLaMA have reshaped how machines understand and generate human language. Behind the impressive capabilities of these models lies a deceptively simple but foundational step: <strong>tokenization</strong>.</p>

<p>In this blog, we will dive deep into the concept of tokenization, understand its types, why it&rsquo;s needed, the challenges it solves, how it works under the hood, and where it‚Äôs headed in the future. This is a one-stop technical deep-dive for anyone looking to fully grasp the backbone of language understanding in LLMs.</p>

<hr />

<a name="L-3c-strong-3e-What-is-Tokenization-3f--3c--2f-strong-3e-"></a>
<h2><strong>What is Tokenization?</strong></h2>

<p>At its core, tokenization is the process of converting raw text into smaller units called <strong>tokens</strong> that a language model can understand and process. These tokens can be:</p>

<ul>
<li>Characters</li>
<li>Words</li>
<li>Subwords</li>
<li>Byte-pair sequences</li>
<li>WordPieces</li>
<li>SentencePieces</li>
<li>Byte-level representations</li>
</ul>


<p>Each model has its own strategy, depending on design goals like efficiency, vocabulary size, multilingual handling, and memory constraints.</p>

<!--more-->


<p>For example, the sentence:</p>

<pre><code>"Tokenization is crucial for LLMs."
</code></pre>

<p>May be tokenized as:</p>

<ul>
<li>Word-level: <code>["Tokenization", "is", "crucial", "for", "LLMs", "."]</code></li>
<li>Character-level: <code>["T", "o", "k", ..., "L", "L", "M", "s", "."]</code></li>
<li>Subword (BPE): <code>["Token", "ization", "is", "cru", "cial", "for", "LL", "Ms", "."]</code></li>
</ul>


<a name="Tokens"></a>
<h4>Tokens</h4>

<p><img src="/images/2025/token.png" height="300" width="900" alt="Alt text" /></p>

<a name="Token-IDs"></a>
<h4>Token IDs</h4>

<p><img src="/images/2025/tokenid.png" height="300" width="900" alt="Alt text" /></p>

<hr />

<a name="L-3c-strong-3e-Why-Tokenization-is-Needed-in-LLMs-3c--2f-strong-3e-"></a>
<h2><strong>Why Tokenization is Needed in LLMs</strong></h2>

<p>Language models operate over numbers (tensors), not raw strings. Before any neural network processes your prompt, the words must be:</p>

<ol>
<li><strong>Split into atomic units (tokens)</strong></li>
<li><strong>Mapped to numerical IDs (vocabulary embedding)</strong></li>
<li><strong>Fed into the model as vectors</strong></li>
</ol>


<p>Without tokenization:</p>

<ul>
<li>Models would struggle with infinite vocabulary.</li>
<li>Multilingual text and compound words would explode the vocabulary.</li>
<li>There would be no efficient way to control sequence length or positional encoding.</li>
</ul>


<hr />

<a name="L-3c-strong-3e-Types-of-Tokenization-Strategies-3c--2f-strong-3e-"></a>
<h2><strong>Types of Tokenization Strategies</strong></h2>

<a name="L-3c-strong-3e-Word-2d-Level-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Word-Level Tokenization</strong></h3>

<p>Each word is a token. Simple but inefficient for:</p>

<ul>
<li>Unknown words (out-of-vocabulary issues)</li>
<li>Morphologically rich languages</li>
<li>Compound words</li>
</ul>


<p><strong>Example:</strong>
&ldquo;unhappiness&rdquo; ‚Üí 1 token ‚Üí [‚Äúunhappiness‚Äù]
If unseen during training, this is a problem.</p>

<hr />

<a name="L-3c-strong-3e-Character-2d-Level-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Character-Level Tokenization</strong></h3>

<p>Each character is a token. Solves OOV issues but leads to longer sequences and loss of semantic granularity.</p>

<p><strong>Example:</strong>
&ldquo;unhappiness&rdquo; ‚Üí [‚Äúu‚Äù, ‚Äún‚Äù, ‚Äúh‚Äù, ‚Äúa‚Äù, ‚Äúp‚Äù, ‚Ä¶]</p>

<hr />

<a name="L-3c-strong-3e-Subword-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Subword Tokenization</strong></h3>

<p>Breaks words into frequent subword units using statistical techniques like:</p>

<ul>
<li><strong>Byte Pair Encoding (BPE)</strong> ‚Äì used by GPT-2, GPT-3</li>
<li><strong>WordPiece</strong> ‚Äì used by BERT</li>
<li><strong>Unigram Language Model</strong> ‚Äì used by SentencePiece (T5, LLaMA)</li>
</ul>


<p><strong>Example (BPE):</strong>
&ldquo;unhappiness&rdquo; ‚Üí [‚Äúun‚Äù, ‚Äúhappi‚Äù, ‚Äúness‚Äù]</p>

<p><strong>Benefits:</strong></p>

<ul>
<li>Handles unknown words gracefully</li>
<li>Reduces vocabulary size</li>
<li>Efficient for multilingual models</li>
</ul>


<hr />

<a name="L-3c-strong-3e-Byte-2d-Level-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Byte-Level Tokenization</strong></h3>

<p>Tokenizes text at the byte level, including UTF-8 encodings.</p>

<p>Used by models like GPT-3.5/4 to handle raw binary inputs and emojis robustly.</p>

<p><strong>Example:</strong>
‚Äúüî•‚Äù ‚Üí byte sequence ‚Üí [240, 159, 148, 165]</p>

<hr />

<a name="L-3c-strong-3e-SentencePiece-3c--2f-strong-3e-"></a>
<h3><strong>SentencePiece</strong></h3>

<p>A library that trains subword models using BPE or Unigram LM on raw text. Used in multilingual LLMs like T5, mT5.</p>

<p>It allows training on raw text without pre-tokenization (no need for whitespace-based splitting).</p>

<hr />

<a name="L-3c-strong-3e-How-Tokenization-Works:-Under-the-Hood-3c--2f-strong-3e-"></a>
<h2><strong>How Tokenization Works: Under the Hood</strong></h2>

<a name="L-3c-strong-3e-Training-a-Tokenizer-3c--2f-strong-3e-"></a>
<h3><strong>Training a Tokenizer</strong></h3>

<p>During tokenizer training, the process involves:</p>

<ul>
<li>Reading a large corpus</li>
<li>Building frequency tables of substrings</li>
<li>Iteratively merging the most frequent substrings</li>
<li>Forming a vocabulary of tokens</li>
<li>Saving a tokenizer model (vocab + merge rules)</li>
</ul>


<a name="L-3c-strong-3e-Encoding-3c--2f-strong-3e-"></a>
<h3><strong>Encoding</strong></h3>

<p>At inference or training:</p>

<ul>
<li>Input string ‚Üí split into substrings based on learned merges</li>
<li>Tokens ‚Üí mapped to numerical IDs via the vocabulary</li>
</ul>


<pre><code class="python">from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokens = tokenizer.tokenize("Tokenization is powerful.")
print(tokens)
# Output: ['Token', 'ization', 'ƒ†is', 'ƒ†powerful', '.']
</code></pre>

<p><code>ƒ†</code> indicates a space in GPT-2 tokenizers.</p>

<hr />

<a name="L-3c-strong-3e-Tokenization-and-Model-Limits-3c--2f-strong-3e-"></a>
<h2><strong>Tokenization and Model Limits</strong></h2>

<p>Most LLMs have a context window defined in <strong>tokens</strong>, not characters. For instance:</p>

<ul>
<li>GPT-3.5: 4,096 tokens</li>
<li>GPT-4 (o4): 128,000 tokens</li>
<li>Claude 3 Opus: \~200,000 tokens</li>
</ul>


<p>So, 1000 words of English ‚âà 750 tokens.</p>

<p>This is crucial for prompt design, summarization, RAG (Retrieval Augmented Generation), and efficient inference.</p>

<a name="L-3c-strong-3e-Challenges-and-Trade-2d-offs-3c--2f-strong-3e-"></a>
<h2><strong>Challenges and Trade-offs</strong></h2>

<table style="width:100%; border-collapse: collapse; text-align: left; font-family: sans-serif;">
  <thead style="background-color: #f2f2f2;">
    <tr>
      <th style="border: 1px solid #ddd; padding: 12px;">Challenge</th>
      <th style="border: 1px solid #ddd; padding: 12px;">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">OOV (Out-of-Vocabulary)</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Especially for word-level tokenization</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">Token inflation</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Some languages (e.g., Chinese, Japanese) produce more tokens</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">Inconsistency</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Subword boundaries may not align with morphemes</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">Efficiency vs Accuracy</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Smaller tokens = longer sequences = more compute</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">Encoding Bias</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Tokenizers trained on certain scripts or corpora may underperform on others</td>
    </tr>
  </tbody>
</table>


<a name="L-3c-strong-3e-Tokenization-in-Multilingual-and-Code-Models-3c--2f-strong-3e-"></a>
<h2><strong>Tokenization in Multilingual and Code Models</strong></h2>

<a name="L-3c-strong-3e-Multilingual-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Multilingual Tokenization</strong></h3>

<ul>
<li>Unicode-aware models must handle multiple scripts (Latin, Devanagari, Arabic, etc.)</li>
<li>Token inflation can disadvantage languages like Hindi and Tamil</li>
<li>SentencePiece helps standardize across languages</li>
</ul>


<a name="L-3c-strong-3e-Code-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Code Tokenization</strong></h3>

<ul>
<li>Code models (e.g., Codex, CodeBERT) often use language-specific tokenizers</li>
<li>Must preserve syntax, spacing, indentation, and even comments</li>
</ul>


<p>Example:</p>

<pre><code class="python">def say_hello():
    print("Hello")
</code></pre>

<p>‚Üí <code>[‚Äòdef‚Äô, ‚Äòƒ†say‚Äô, ‚Äò_‚Äô, ‚Äòhello‚Äô, ‚Äò()‚Äô, ‚Äò:‚Äô, ‚Äòƒ†‚Äô, ‚Äòprint‚Äô, ‚Äò(‚Äù, ‚ÄòHello‚Äô, ‚Äò‚Äù)‚Äô]</code></p>

<hr />

<a name="L-3c-strong-3e-Compression-2c--Prompt-Engineering-2c--and-Token-Optimization-3c--2f-strong-3e-"></a>
<h2><strong>Compression, Prompt Engineering, and Token Optimization</strong></h2>

<p>Tokenization also directly affects:</p>

<ul>
<li><strong>Prompt length limits</strong> (compressed prompts ‚Üí more room for data)</li>
<li><strong>Token cost in inference/billing</strong></li>
<li><strong>RAG performance</strong> (chunking based on tokens)</li>
<li><strong>Training data deduplication</strong> (token-based hashing)</li>
</ul>


<p>Optimizing prompts for fewer tokens can reduce cost and latency.</p>

<hr />

<a name="L-3c-strong-3e-Tokenization-vs.-Embeddings-3c--2f-strong-3e-"></a>
<h2><strong>Tokenization vs. Embeddings</strong></h2>

<p>It‚Äôs important to note:</p>

<ul>
<li><strong>Tokenization</strong> comes before embedding.</li>
<li>Token ‚Üí token ID ‚Üí embedding vector</li>
</ul>


<p>A poor tokenization scheme = noisy embeddings = reduced model performance.</p>

<hr />

<a name="L-3c-strong-3e-The-Future-of-Tokenization-in-LLMs-3c--2f-strong-3e-"></a>
<h2><strong>The Future of Tokenization in LLMs</strong></h2>

<a name="L-3c-strong-3e-Token-2d-Free-Models-3c--2f-strong-3e-"></a>
<h3><strong>Token-Free Models</strong></h3>

<p>Efforts like <strong>Charformer</strong> and <strong>Byte-level transformers</strong> aim to bypass static tokenization and learn from raw bytes or characters.</p>

<a name="L-3c-strong-3e-Neural-Tokenization-3c--2f-strong-3e-"></a>
<h3><strong>Neural Tokenization</strong></h3>

<p>Trainable tokenizers using neural nets to learn optimal segmentation dynamically.</p>

<a name="L-3c-strong-3e-Universal-Tokenizers-3c--2f-strong-3e-"></a>
<h3><strong>Universal Tokenizers</strong></h3>

<p>Tokenizers trained across modalities (text, image, code) using a common vocabulary to unify multimodal models.</p>

<a name="L-3c-strong-3e-Efficient-Context-Windows-3c--2f-strong-3e-"></a>
<h3><strong>Efficient Context Windows</strong></h3>

<p>With sliding-window and compression-based methods (e.g., Mamba, Hyena), token overhead may reduce for long contexts.</p>

<hr />

<a name="L-3c-strong-3e-Major-LLMs-and-Their-Tokenization-3c--2f-strong-3e-"></a>
<h2><strong>Major LLMs and Their Tokenization</strong></h2>

<table style="width:100%; border-collapse: collapse; text-align: left; font-family: sans-serif;">
  <thead style="background-color: #f2f2f2;">
    <tr>
      <th style="border: 1px solid #ddd; padding: 12px;">Model</th>
      <th style="border: 1px solid #ddd; padding: 12px;">Tokenizer</th>
      <th style="border: 1px solid #ddd; padding: 12px;">Type</th>
      <th style="border: 1px solid #ddd; padding: 12px;">Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">GPT-3/4</td>
      <td style="border: 1px solid #ddd; padding: 12px;">GPT2Tokenizer</td>
      <td style="border: 1px solid #ddd; padding: 12px;">BPE + byte</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Handles Unicode well</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">BERT</td>
      <td style="border: 1px solid #ddd; padding: 12px;">WordPiece</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Subword</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Requires pre-tokenization</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">RoBERTa</td>
      <td style="border: 1px solid #ddd; padding: 12px;">BPE (FairSeq)</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Subword</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Custom vocabulary</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">T5</td>
      <td style="border: 1px solid #ddd; padding: 12px;">SentencePiece</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Unigram LM</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Whitespace-free tokenization</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">LLaMA 2/3/4</td>
      <td style="border: 1px solid #ddd; padding: 12px;">SentencePiece</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Unigram LM</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Supports multiple languages</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;">Claude</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Byte-level BPE</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Proprietary</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Handles emojis and long context</td>
    </tr>
  </tbody>
</table>


<hr />

<a name="L-3c-strong-3e-Conclusion-3c--2f-strong-3e-"></a>
<h2><strong>Conclusion</strong></h2>

<p>Tokenization may appear trivial at first glance, but it&rsquo;s the hidden workhorse powering the language capabilities of every modern LLM. From enabling multilingual understanding to compressing long documents into tight prompts, tokenization determines what the model sees, learns, and generates.</p>

<p>As we step into an era of 1M+ token context windows, modality fusion, and instruction-following agents, tokenization will either evolve or be replaced by more fluid, learned representations.</p>

<p>But for now‚Äîand the foreseeable future‚Äîit remains a vital piece of the LLM puzzle.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Smartcase Engine: A Modern Framework for Intelligent Case Management]]></title>
        <link href="https://rishijeet.github.io/blog/smartcase-engine-a-modern-framework-for-intelligent-case-management/"/>
        <updated>2025-05-27T22:54:42+05:30</updated>
        <id>https://rishijeet.github.io/blog/smartcase-engine-a-modern-framework-for-intelligent-case-management</id>
        <content type="html"><![CDATA[<p>In today&rsquo;s dynamic business environment, efficient case management is paramount. Enter <a href="https://github.com/rishijeet/smartcase-engine">Smartcase Engine</a>, an advanced case management framework designed to streamline complex case handling through real-time tracking, efficient workflows, and automated decision-making processes.</p>

<a name="What-is-Smartcase-Engine-3f-"></a>
<h2>What is Smartcase Engine?</h2>

<p>Smartcase Engine is a modular, microservices-based platform tailored for managing intricate case workflows. It offers:</p>

<ul>
<li><strong>Real-Time Case Tracking</strong>: Monitor cases as they progress through various stages.</li>
<li><strong>Efficient Workflows</strong>: Automate and optimize the sequence of tasks involved in case resolution.</li>
<li><strong>Automated Decision-Making</strong>: Leverage predefined rules and AI to make informed decisions without manual intervention.</li>
</ul>


<p><img src="/images/2025/smartcase_engine.png" height="300" width="900" alt="Alt text" /><em>Source: <a href="https://github.com/rishijeet/smartcase-engine">Rishijeet Mishra&rsquo;s Blog</a></em></p>

<!--more-->


<a name="Deep-Dive:-Smartcase-Engine-Architecture"></a>
<h2>Deep Dive: Smartcase Engine Architecture</h2>

<p>At the heart of Smartcase Engine is a clean, extensible <strong>microservices-based architecture</strong> designed to support complex workflows in case/dispute management scenarios. The system is broken into discrete services that communicate via REST APIs and Kafka for event-driven interactions. This modular approach allows teams to scale and evolve components independently.</p>

<p>Let‚Äôs explore the core services that power this engine:</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Intake-Service-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Intake Service</strong></h3>

<p><strong>Purpose</strong>: This is the gateway to the system ‚Äî the service responsible for accepting new cases or disputes.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Receive new dispute cases via API or event (Kafka).</li>
<li>Validate and enrich the incoming payload.</li>
<li>Generate a unique dispute ID.</li>
<li>Store initial metadata and emit an event to kick off downstream workflow.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Built using Java + Quarkus for lightweight runtime.</li>
<li>Connects to Kafka for emitting intake-completed events.</li>
<li>Persists initial data in a database (e.g., PostgreSQL or any pluggable DB).</li>
<li>Implements REST endpoints for manual intake testing or system integration.</li>
</ul>


<p><strong>Why it matters</strong>:
This service ensures that all disputes entering the system are properly structured and immediately traceable ‚Äî forming the root of all subsequent orchestration.</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Workflow-Service-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Workflow Service</strong></h3>

<p><strong>Purpose</strong>: This is the <em>brain</em> of the engine, orchestrating the lifecycle of a dispute across multiple business stages.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Define and manage the state machine (or BPMN-style flow) for a dispute.</li>
<li>Trigger actions based on status updates (e.g., escalate, resolve, pause).</li>
<li>Call external services when required (e.g., fetch additional metadata, update agent queue).</li>
<li>Track state transitions and support retries/failures.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Uses a stateless orchestration model.</li>
<li>Integration with Kafka allows event-driven step progression.</li>
<li>Could be integrated with BPMN engines (like Camunda or Flowable) for visual modeling.</li>
</ul>


<p><strong>Why it matters</strong>:
This is the system&rsquo;s core engine. It allows Smartcase to define complex, non-linear workflows without hardcoding logic into the intake or agent UI layers.</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Classification-Service-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Classification Service</strong></h3>

<p><strong>Purpose</strong>: Adds intelligence to the process by classifying disputes into appropriate categories.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Use business rules or ML models to assign dispute types (e.g., &ldquo;billing error&rdquo;, &ldquo;fraud&rdquo;, &ldquo;product defect&rdquo;).</li>
<li>Optionally flag high-risk or high-priority disputes.</li>
<li>Feed classification results back into the workflow service to route the case accordingly.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Stateless classification service.</li>
<li>Integrates with a basic rule engine or external ML service (could be backed by Python/ONNX, or a local inference server).</li>
<li>Input: structured dispute metadata. Output: classification code or tag.</li>
</ul>


<p><strong>Why it matters</strong>:
Classification drives automation. By programmatically tagging and triaging disputes, Smartcase avoids human bottlenecks and supports intelligent queue assignment.</p>

<hr />

<a name="L-3c-strong-3e-Agent-UI-Service-3c--2f-strong-3e-"></a>
<h3><strong>Agent UI Service</strong></h3>

<p><strong>Purpose</strong>: The interface between human agents and the system.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Display dispute data and current status.</li>
<li>Allow agents to take actions (e.g., approve, reject, escalate).</li>
<li>Show workflow progression.</li>
<li>Track comments, attachments, and communication logs.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Frontend (typically in React or Angular).</li>
<li>Backend proxy or BFF layer in Quarkus serving data via REST.</li>
<li>Authenticated access with role-based views (agent, supervisor, auditor).</li>
<li>Pagination, search, filters, and sort capabilities to handle large volumes.</li>
</ul>


<p><strong>Why it matters</strong>:
No matter how automated the backend is, disputes often need human judgment. This UI is purpose-built for efficiency and transparency in resolution workflows.</p>

<hr />

<a name="L-3c-strong-3e-Dispute-Common-Module-3c--2f-strong-3e-"></a>
<h3><strong>Dispute Common Module</strong></h3>

<p><strong>Purpose</strong>: A shared library of core utilities and contracts.</p>

<p><strong>Responsibilities</strong>:</p>

<ul>
<li>Define POJOs (Plain Old Java Objects) and DTOs (Data Transfer Objects).</li>
<li>Common validation logic.</li>
<li>Central configuration definitions.</li>
<li>Shared Kafka event schema.</li>
<li>Error handling standards and API response models.</li>
</ul>


<p><strong>Technical Highlights</strong>:</p>

<ul>
<li>Packaged as a reusable JAR.</li>
<li>Imported by all services as a dependency.</li>
<li>Promotes DRY principles and API consistency.</li>
</ul>


<p><strong>Why it matters</strong>:
Microservices need to stay loosely coupled ‚Äî but shared types and utilities must remain consistent. This module enforces a standard language across the ecosystem.</p>

<hr />

<a name="Optional-Add-2d-ons--28-Future-Ready-29-"></a>
<h3>Optional Add-ons (Future Ready)</h3>

<p>Depending on your scale and use case, Smartcase Engine can be extended with:</p>

<ul>
<li><strong>Notification Service</strong>: For sending SMS/email alerts to users or agents.</li>
<li><strong>Audit Logging</strong>: For compliance with financial or legal audits.</li>
<li><strong>Retry/Dead-letter Queue Mechanism</strong>: To gracefully handle transient failures.</li>
<li><strong>Observability Tools</strong>: Integrated with Prometheus, Grafana, and OpenTelemetry for distributed tracing.</li>
</ul>


<hr />

<a name="Deployment-and-Setup"></a>
<h2>Deployment and Setup</h2>

<p>Smartcase Engine is containerized using Docker, facilitating seamless deployment. Key scripts and configurations include:</p>

<ul>
<li><strong>Dockerfile</strong>: Defines the environment for each microservice.</li>
<li><strong>docker-compose.yml</strong>: Orchestrates multi-container deployments for local development and testing.</li>
<li><strong>build-and-deploy.sh</strong>: Automates the build and deployment process.</li>
<li><strong>start-services.sh</strong>: Initiates all services concurrently.</li>
</ul>


<p>To get started:</p>

<pre><code class="bash">git clone https://github.com/rishijeet/smartcase-engine.git
cd smartcase-engine
chmod +x build-and-deploy.sh start-services.sh
./build-and-deploy.sh
./start-services.sh
</code></pre>

<a name="Testing-and-Validation"></a>
<h2>Testing and Validation</h2>

<p>Each microservice includes unit and integration tests to ensure reliability. The modular design allows for isolated testing, simplifying debugging and maintenance.</p>

<a name="Potential-Use-Cases"></a>
<h2>Potential Use Cases</h2>

<p>Smartcase Engine&rsquo;s versatility makes it suitable for various domains:</p>

<ul>
<li><strong>Financial Services</strong>: Automating dispute resolutions in banking and insurance.</li>
<li><strong>Customer Support</strong>: Managing customer complaints and service requests.</li>
<li><strong>Legal Case Management</strong>: Tracking legal cases, evidence, and proceedings.</li>
<li><strong>Healthcare</strong>: Handling patient grievances and administrative cases.</li>
</ul>


<a name="Contributing-to-Smartcase-Engine"></a>
<h2>Contributing to Smartcase Engine</h2>

<p>The project welcomes contributions from the developer community. To contribute:</p>

<ol>
<li>Fork the repository.</li>
<li>Create a new branch for your feature or bugfix.</li>
<li>Ensure code quality by running existing tests and adding new ones if necessary.</li>
<li>Submit a pull request with a clear description of your changes.</li>
</ol>


<a name="Further-Reading"></a>
<h2>Further Reading</h2>

<p>For more in-depth information:</p>

<ul>
<li><a href="https://github.com/rishijeet/smartcase-engine">Smartcase Engine GitHub Repository</a></li>
<li><a href="https://martinfowler.com/articles/microservices.html">Microservices Architecture</a></li>
<li><a href="https://docs.docker.com/">Docker Documentation</a></li>
<li><a href="https://www.omg.org/bpmn/">BPMN Standards</a></li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>Smartcase Engine exemplifies how modern architectural principles can be harnessed to build robust, scalable, and efficient case management systems. Whether you&rsquo;re looking to streamline dispute resolutions or manage complex workflows, Smartcase Engine offers a solid foundation to build upon.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Model Context Protocol (MCP): The Backbone of Dynamic AI Workflows]]></title>
        <link href="https://rishijeet.github.io/blog/model-context-protocol-mcp-the-backbone-of-dynamic-ai-workflows/"/>
        <updated>2025-04-08T23:14:14+05:30</updated>
        <id>https://rishijeet.github.io/blog/model-context-protocol-mcp-the-backbone-of-dynamic-ai-workflows</id>
        <content type="html"><![CDATA[<p>As the AI landscape rapidly evolves, the demand for systems that support <strong>modular</strong>, <strong>context-aware</strong>, and <strong>efficient orchestration</strong> of models has grown. Enter the <strong>Model Context Protocol (MCP)</strong> ‚Äî a rising standard that enables dynamic, multi-agent AI systems to exchange context, manage state, and chain model invocations intelligently.</p>

<p>In this article, we‚Äôll explore what MCP is, why it matters, and how it‚Äôs becoming a key component in the infrastructure stack for advanced AI applications. We‚Äôll also walk through a conceptual example of building an MCP-compatible server.</p>

<a name="What-is-the-Model-Context-Protocol--28-MCP-29--3f-"></a>
<h2>What is the Model Context Protocol (MCP)?</h2>

<p><strong>MCP</strong> is a protocol designed to manage the <strong>contextual state</strong> of AI models across requests in multi-agent, multi-model environments. It‚Äôs part of a broader effort to make LLMs (Large Language Models) more <strong>stateful</strong>, <strong>collaborative</strong>, and <strong>task-aware</strong>.</p>

<p>At its core, MCP provides:</p>

<ul>
<li>A way to <strong>pass and maintain context</strong> (like conversation history, task progress, or shared knowledge) across AI agents or model calls.</li>
<li>A standardized protocol to support <strong>chained inference</strong>, where multiple models collaborate on subtasks.</li>
<li>Support for <strong>stateful computation</strong>, which is critical in complex reasoning or long-running workflows.</li>
</ul>


<p><img src="/images/2025/mcp_server" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<!--more-->


<a name="Why-is-MCP-Relevant-Now-3f-"></a>
<h2>Why is MCP Relevant Now?</h2>

<p>The growing interest in <strong>AI agents</strong>, <strong>function-calling APIs</strong>, and <strong>model interoperability</strong> has created a pressing need for something like MCP. Some trends driving MCP adoption include:</p>

<style>
  .trend-impact-table {
    width: 100%;
    border-collapse: collapse;
    font-family: Arial, sans-serif;
  }
  .trend-impact-table th,
  .trend-impact-table td {
    text-align: left;
    padding: 12px;
    border-bottom: 1px solid #ccc;
    vertical-align: top;
  }
  .trend-impact-table th {
    background-color: #f2f2f2;
    width: 25%;
  }
</style>




<table class="trend-impact-table">
  <thead>
    <tr>
      <th>Trend</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Agentic Workflows</td>
      <td>Models need shared context to collaborate efficiently (e.g., ReAct, AutoGPT, BabyAGI).</td>
    </tr>
    <tr>
      <td>LLM Orchestration Frameworks</td>
      <td>Tools like LangChain, Semantic Kernel, and OpenDevin push for context-aware memory and model chaining.</td>
    </tr>
    <tr>
      <td>Open Model Ecosystems</td>
      <td>Efforts like Hugging Face&#8217;s Inference Endpoints, vLLM, and Modal want to standardize inference behavior.</td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation (RAG)</td>
      <td>Persistent context and metadata handling are vital for grounded reasoning.</td>
    </tr>
  </tbody>
</table>


<p>Leading companies like <strong>OpenAI (via ChatGPT APIs)</strong>, <strong>Anthropic (via Claude‚Äôs memory)</strong>, and <strong>Mistral</strong> are integrating ideas from MCP implicitly, if not through standardized APIs.</p>

<a name="Core-Concepts-of-MCP"></a>
<h2>Core Concepts of MCP</h2>

<p>An MCP server typically supports the following concepts:</p>

<a name="L-3c-strong-3e-Model-Context-3c--2f-strong-3e-"></a>
<h3><strong>Model Context</strong></h3>

<pre><code class="json">{
  "session_id": "abc-123",
  "user_id": "user-456",
  "context": {
    "history": [
      { "role": "user", "content": "Generate a project plan." },
      { "role": "assistant", "content": "Sure, here's a draft..." }
    ],
    "task": "project_planning",
    "dependencies": ["retrieval_plugin", "summarizer_model"]
  }
}
</code></pre>

<a name="L-3c-strong-3e-Model-Invocation-with-Context-3c--2f-strong-3e-"></a>
<h3><strong>Model Invocation with Context</strong></h3>

<pre><code class="json">{
  "model": "gpt-4",
  "input": "What are the next steps?",
  "context_ref": "abc-123",
  "metadata": {
    "requested_capability": "planning.summarize"
  }
}
</code></pre>

<a name="L-3c-strong-3e-Chained-Outputs-and-Shared-State-3c--2f-strong-3e-"></a>
<h3><strong>Chained Outputs and Shared State</strong></h3>

<p>Each model contributes to a shared state, stored either in an in-memory store (like Redis) or a structured store (like Postgres + pgvector for embeddings).</p>

<a name="Building-a-Basic-MCP-Server"></a>
<h2>Building a Basic MCP Server</h2>

<p>Let‚Äôs outline what a minimal MCP-compatible server might look like using <strong>FastAPI</strong> and <strong>Redis</strong>.</p>

<a name="Basic-Server-with-Context-Store"></a>
<h3>Basic Server with Context Store</h3>

<pre><code class="python">from fastapi import FastAPI, Request
import redis
import uuid
import json

app = FastAPI()
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

@app.post("/invoke")
async def invoke_model(request: Request):
    payload = await request.json()
    context_ref = payload.get("context_ref")
    input_text = payload["input"]
    model = payload["model"]

    # Load context
    context = json.loads(r.get(context_ref)) if context_ref else {}
    history = context.get("history", [])

    # Simulate model response
    history.append({"role": "user", "content": input_text})
    response = f"Simulated response to: {input_text}"
    history.append({"role": "assistant", "content": response})

    # Save updated context
    new_context_ref = context_ref or str(uuid.uuid4())
    r.set(new_context_ref, json.dumps({"history": history}))

    return {"output": response, "context_ref": new_context_ref}
</code></pre>

<a name="Add-Capability-Metadata"></a>
<h3>Add Capability Metadata</h3>

<p>Enhance the server to log requested capabilities and dependency resolution (e.g., invoking tools or submodels).</p>

<pre><code class="python">capability = payload.get("metadata", {}).get("requested_capability")
log_event(user_id, session_id, model, capability)
</code></pre>

<hr />

<a name="MCP-vs-Alternatives"></a>
<h2>MCP vs Alternatives</h2>

<p>MCP aims to serve as the <strong>underlying protocol</strong>, while frameworks like LangChain act as <strong>developer tooling on top</strong>.</p>

<style>
  .comparison-table {
    width: 100%;
    border-collapse: collapse;
    font-family: Arial, sans-serif;
  }
  .comparison-table th,
  .comparison-table td {
    text-align: center;
    padding: 12px;
    border-bottom: 1px solid #ccc;
    width: 20%;
  }
  .comparison-table th {
    background-color: #f2f2f2;
  }
</style>




<table class="comparison-table">
  <thead>
    <tr>
      <th>Feature</th>
      <th>MCP</th>
      <th>LangChain</th>
      <th>Semantic Kernel</th>
      <th>ChatML (OpenAI)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Context Persistence</td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>Partial</td>
    </tr>
    <tr>
      <td>Model-Agnostic</td>
      <td>‚úÖ</td>
      <td>‚ùå<br><small>(Python-specific)</small></td>
      <td>‚úÖ</td>
      <td>‚ùå</td>
    </tr>
    <tr>
      <td>Stateful Memory</td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>Partial</td>
    </tr>
    <tr>
      <td>Chaining Support</td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>‚ùå</td>
    </tr>
    <tr>
      <td>Explicit Protocol</td>
      <td>‚úÖ</td>
      <td>‚ùå</td>
      <td>‚ùå</td>
      <td>‚úÖ<br><small>(format only)</small></td>
    </tr>
  </tbody>
</table>


<a name="Adoption-and-Ecosystem-Signals"></a>
<h2>Adoption and Ecosystem Signals</h2>

<ul>
<li><strong>LangChain and LlamaIndex</strong>: Moving towards standardizing memory interfaces with composable context.</li>
<li><strong>OpenAI‚Äôs Assistant API</strong>: Explicitly supports persistent threads, similar to MCP session_id and shared memory.</li>
<li><strong>Anthropic&rsquo;s Memory Plans</strong>: Incorporates long-term memory slots, resembling MCP‚Äôs context model.</li>
<li><strong>Meta‚Äôs Multi-Agent Research (2024)</strong>: Proposes architectures that are context-routing centric ‚Äî aligning with MCP‚Äôs goals.</li>
</ul>


<a name="Challenges-and-Future-Directions"></a>
<h2>Challenges and Future Directions</h2>

<a name="Technical-Challenges"></a>
<h3>Technical Challenges</h3>

<ul>
<li>Efficient context storage and retrieval at scale.</li>
<li>Dynamic resolution of capabilities and tool invocation.</li>
<li>Real-time chaining with latency constraints.</li>
</ul>


<a name="What-e2--80--99-s-Next-3f-"></a>
<h3>What‚Äôs Next?</h3>

<ul>
<li><strong>Open spec for MCP</strong>: Standardization akin to OpenAPI or GraphQL.</li>
<li><strong>Plugin Interop</strong>: Tool APIs that conform to context-aware interfaces.</li>
<li><strong>LLMOps Integration</strong>: Tracking usage, debugging flows, and observability in agentic systems.</li>
</ul>


<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The <strong>Model Context Protocol</strong> is a foundational building block for the next wave of <strong>AI-native applications</strong>. It abstracts and manages the complexity of context, model chaining, and agent collaboration ‚Äî enabling AI systems that behave less like stateless endpoints and more like intelligent software agents.</p>

<p>As the AI ecosystem matures, MCP (whether explicitly named or not) will become central to orchestrating rich, multi-turn, multi-model AI systems.</p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[High-Flyer: Pioneering AI in Finance]]></title>
        <link href="https://rishijeet.github.io/blog/high-flyer-pioneering-ai-in-finance/"/>
        <updated>2025-03-30T20:13:12+05:30</updated>
        <id>https://rishijeet.github.io/blog/high-flyer-pioneering-ai-in-finance</id>
        <content type="html"><![CDATA[<p>In the rapidly evolving landscape of artificial intelligence (AI), China&rsquo;s DeepSeek has emerged as a formidable contender, challenging established players and redefining industry standards. This ascent is deeply intertwined with High-Flyer, an AI-driven quantitative hedge fund whose strategic investments and visionary leadership have propelled DeepSeek to the forefront of AI innovation.</p>

<p><img src="/images/2025/deepseek.jpg" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p>Founded in February 2016 by Liang Wenfeng, High-Flyer‚Äîofficially known as Hangzhou Huanfang Technology Ltd Co.‚Äîquickly distinguished itself in the financial sector by leveraging AI models for investment decisions. By late 2017, AI systems managed the majority of High-Flyer&rsquo;s trading activities, solidifying its reputation as a leader in AI-driven stock trading. The firm&rsquo;s portfolio burgeoned to an impressive 100 billion yuan (approximately $13.79 billion), underscoring the efficacy of its AI-centric strategies.</p>

<!--more-->


<p><img src="/images/2025/high_flyer.png" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Strategic-Investments-in-Computing-Power"></a>
<h2>Strategic Investments in Computing Power</h2>

<p>Anticipating the critical role of computational resources in AI advancement, Liang Wenfeng initiated substantial investments in high-performance hardware. Between 2020 and 2021, High-Flyer constructed two AI supercomputing clusters comprising Nvidia&rsquo;s A100 graphics processing units (GPUs). The first cluster, operational in 2020, integrated 1,100 A100 chips at a cost of 200 million yuan.
The subsequent year saw the completion of a second, more expansive cluster with approximately 10,000 A100 chips, representing a 1 billion yuan investment. These strategic acquisitions occurred prior to the U.S. government&rsquo;s 2022 restrictions on exporting advanced chips to China, positioning High-Flyer advantageously amid tightening technological embargoes.</p>

<p><img src="/images/2025/quant_china.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="Transition-to-Artificial-General-Intelligence--28-AGI-29-"></a>
<h2>Transition to Artificial General Intelligence (AGI)</h2>

<p>In 2023, High-Flyer announced a pivotal shift towards pursuing artificial general intelligence (AGI), aiming to develop autonomous systems capable of outperforming humans in most economically valuable tasks. This initiative led to the establishment of DeepSeek as an independent research entity dedicated to exploring the essence of AGI. Liang Wenfeng assumed a leadership role in DeepSeek, guiding its strategic direction and research endeavors.</p>

<p><img src="/images/2025/hedge_fund_mania.webp" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<p><img src="/images/2025/high_flyer_return.avif" height="300" width="900" alt="Alt text" /><em>Source: Internet</em></p>

<a name="DeepSeek-27-s-Breakthroughs-and-Industry-Impact"></a>
<h2>DeepSeek&rsquo;s Breakthroughs and Industry Impact</h2>

<p>DeepSeek&rsquo;s trajectory has been marked by a series of groundbreaking AI models:</p>

<ul>
<li><p><strong>DeepSeek-V2 (May 2024):</strong> This chatbot model gained widespread acclaim in China for its cost-efficiency and superior performance, outperforming offerings from major tech companies such as ByteDance, Tencent, Baidu, and Alibaba. Its release triggered a price war, compelling competitors to significantly reduce prices on their AI models.</p></li>
<li><p><strong>DeepSeek-V3 (December 2024):</strong> Building upon its predecessor, DeepSeek-V3 further enhanced capabilities and solidified DeepSeek&rsquo;s dominance in the AI sector.</p></li>
<li><p><strong>DeepSeek-R1 (January 2025):</strong> This reasoning model and its associated chatbot application marked DeepSeek&rsquo;s entry into the international market, challenging the prevailing assumption of U.S. dominance in AI.</p></li>
</ul>


<p>The sophistication of DeepSeek&rsquo;s models has garnered praise from Silicon Valley competitors, a first for a Chinese AI model. Notably, DeepSeek claims to have achieved these advancements using a fraction of the computing power deployed by leading U.S. firms, a revelation that contributed to a global selloff of tech shares.</p>

<a name="Navigating-Challenges-and-Future-Outlook"></a>
<h2>Navigating Challenges and Future Outlook</h2>

<p>Despite its rapid ascent, DeepSeek faces challenges, particularly concerning access to advanced computing hardware due to export restrictions. Liang Wenfeng has expressed concerns about the embargo on high-end chips, acknowledging it as a significant hurdle. Nevertheless, DeepSeek continues to innovate, relying on existing resources, efficiency improvements, and exploring domestic alternatives.</p>

<p>DeepSeek&rsquo;s success has also influenced China&rsquo;s financial markets. The Chinese stock market experienced a resurgence in investor interest, with equity issuance doubling in the first quarter of 2025 compared to the previous year, reaching $16.8 billion. This optimism is partly driven by the emergence of DeepSeek, which offers AI products at lower costs, encouraging global investors to reconsider China&rsquo;s potential despite ongoing trade tensions.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>DeepSeek&rsquo;s meteoric rise, underpinned by High-Flyer&rsquo;s strategic foresight and investment in AI, exemplifies China&rsquo;s growing prowess in the global AI landscape. By prioritizing research over immediate commercial profit and fostering a meritocratic environment, DeepSeek has not only achieved remarkable technological advancements but also challenged existing paradigms, signaling a shift towards a more diversified and competitive global AI ecosystem.</p>
]]></content>
    </entry>
    
</feed>
