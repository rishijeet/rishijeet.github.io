<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Rishijeet Mishra]]></title>
  <link href="https://rishijeet.github.io/atom.xml" rel="self"/>
  <link href="https://rishijeet.github.io/"/>
  <updated>2024-01-14T23:40:10+05:30</updated>
  <id>https://rishijeet.github.io/</id>
  <author>
    <name><![CDATA[Rishijeet Mishra]]></name>
    <email><![CDATA[rishijeet@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Enhancing Natural Language Processing with Retrieval-Augmented Generation]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/enhancing-natural-language-processing-with-retrieval-augmented-generation/"/>
    <updated>2024-01-13T20:34:07+05:30</updated>
    <id>https://rishijeet.github.io/blog/enhancing-natural-language-processing-with-retrieval-augmented-generation</id>
    <content type="html"><![CDATA[<p>Natural Language Processing (NLP) has witnessed remarkable advancements in recent years, with the advent of sophisticated language models like GPT-3 (Generative Pre-trained Transformer 3). However, one of the challenges that still persists in NLP is the generation of coherent and contextually relevant content. Retrieval-Augmented Generation (RAG) emerges as a powerful solution to address this issue, combining the strengths of both retrieval-based and generation-based approaches.</p>

<a name="Understanding.Retrieval-Augmented.Generation"></a>
<h1>Understanding Retrieval-Augmented Generation</h1>

<p>Retrieval-Augmented Generation is a hybrid approach that integrates the benefits of information retrieval systems with generative models. Let&rsquo;s delve into the mathematical formulations of the key components of RAG.</p>

<p><img src="https://rishijeet.github.io/images/rag_new.png" height="300" width="900" alt="Alt text" /> Figure: Overview of our approach. We combine a pre-trained retriever <em>(Query Encoder + Document Index)</em> with a pre-trained seq2seq model <em>(Generator)</em> and fine-tune end-to-end. For query \(x\), we use Maximum Inner Product Search (MIPS) to find the top-\(K\) documents \(z_i\). For the final prediction \(y\), we treat \(z\) as a latent variable and marginalize over seq2seq predictions given different documents. <em><a href="https://arxiv.org/pdf/2005.11401.pdf">Source: arxiv.org</a></em></p>

<!-- more -->


<a name="L1..Generative.Model"></a>
<h2>1. Generative Model</h2>

<p>The generative model in RAG is often based on a pre-trained transformer architecture, such as GPT-3. The core functionality involves generating text given a context. Mathematically, the generative model can be represented as:</p>

<p>\[ P_{\text{gen}}(Y|X) \]</p>

<p>where \( Y \) is the generated text and \( X \) is the input context. This probability distribution captures the likelihood of generating \( Y \) given \( X \).</p>

<a name="L2..Retrieval.Model"></a>
<h2>2. Retrieval Model</h2>

<p>The retrieval model is responsible for fetching relevant information from a knowledge base. This can be achieved through techniques like dense retrieval using embeddings. The retrieval model computes the similarity between the input query and the documents in the knowledge base. Mathematically, this can be expressed as:</p>

<p>\[ \text{argmax}_{d \in \text{KnowledgeBase}} \text{similarity}(Q, \text{Embed}(d)) \]</p>

<p>where \( Q \) is the query, \( \text{KnowledgeBase} \) is the set of documents, \( d \) represents a document, \( \text{Embed}(\cdot) \) denotes the embedding function, and \( \text{similarity}(\cdot) \) measures the similarity between the query and document embeddings.</p>

<a name="L3..Indexing.Mechanism"></a>
<h2>3. Indexing Mechanism</h2>

<p>Indexing mechanisms play a crucial role in efficiently retrieving information. Commonly, techniques like approximate nearest neighbors are employed. Mathematically, indexing involves mapping documents to a space such that retrieval operations are expedited. This can be represented as:</p>

<p>\[ \text{Index}(d) \rightarrow \text{Embed}(d) \]</p>

<p>where \( \text{Index}(\cdot) \) denotes the indexing function mapping documents to their embeddings.</p>

<a name="L4..Context-Aware.Integration"></a>
<h2>4. Context-Aware Integration</h2>

<p>To integrate retrieved information into the generative process while maintaining context, the retrieval model output needs to be combined with the generative model&rsquo;s output. A simple formulation for context-aware integration can be:</p>

<p>\[ P_{\text{final}}(Y|X, Q) = \alpha \cdot P_{\text{gen}}(Y|X) + (1-\alpha) \cdot P_{\text{ret}}(Y|Q) \]</p>

<p>where \( P_{\text{final}}(Y|X, Q) \) is the final probability distribution of generating \( Y \) given \( X \) and \( Q \), \( \alpha \) is a hyperparameter controlling the balance between generative and retrieval components, and \( P_{\text{ret}}(Y|Q) \) is the probability distribution of generating \( Y \) given the retrieved information \( Q \).</p>

<a name="Advantages.of.Retrieval-Augmented.Generation"></a>
<h1>Advantages of Retrieval-Augmented Generation</h1>

<ol>
<li><p><strong>Improved Relevance:</strong>
By integrating information retrieval, RAG ensures that the generated content is contextually relevant and grounded in factual accuracy. This is particularly beneficial in applications where precision and relevance are critical, such as question-answering systems.</p></li>
<li><p><strong>Addressing Data Sparsity:</strong>
In scenarios where training data is limited, RAG can leverage external knowledge bases to compensate for the lack of specific information. This makes the model more robust and capable of handling a broader range of topics.</p></li>
<li><p><strong>Contextual Enrichment:</strong>
The retrieval-augmented approach allows for the enrichment of generated content by pulling in relevant details from a diverse set of sources. This not only enhances the quality of the generated text but also broadens the scope of information covered.</p></li>
<li><p><strong>Reduced Ambiguity:</strong>
Integrating retrieval mechanisms helps in disambiguating the meaning of ambiguous terms or phrases by pulling in contextually appropriate information from the knowledge base.</p></li>
</ol>


<a name="Applications.of.Retrieval-Augmented.Generation"></a>
<h1>Applications of Retrieval-Augmented Generation</h1>

<ol>
<li><p><strong>Question Answering Systems:</strong>
RAG is particularly effective in question-answering systems where precise and contextually relevant answers are essential. The retrieval model can fetch information from a knowledge base to support or augment the generative model&rsquo;s response.</p></li>
<li><p><strong>Content Creation:</strong>
In content creation tasks, such as article writing or summarization, RAG can enhance the coherence and factual accuracy of the generated content by pulling in information from external sources.</p></li>
<li><p><strong>Dialog Systems:</strong>
Conversational agents can benefit from RAG by providing more informative and contextually relevant responses. The retrieval model aids in quickly accessing relevant information to support the generative model&rsquo;s output during a conversation.</p></li>
</ol>


<a name="Challenges.and.Future.Directions"></a>
<h1>Challenges and Future Directions</h1>

<p>While Retrieval-Augmented Generation shows great promise, it is not without its challenges. Fine-tuning the balance between the generative and retrieval components, handling diverse knowledge bases, and addressing potential biases in retrieved information are areas that require further research. Additionally, exploring ways to dynamically update the knowledge base during the generative process could open new possibilities for real-time applications.</p>

<p>In conclusion, Retrieval-Augmented Generation represents a significant step forward in enhancing the capabilities of natural language processing systems. By seamlessly integrating the strengths of generative and retrieval models, RAG holds the potential to revolutionize various NLP applications, making them more accurate, contextually aware, and capable of handling a wide range of tasks. As research in this field continues to progress, we can expect even more sophisticated and versatile language models that leverage the best of both worlds.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The AI Horizon: Unveiling the Titans - Gemini, Llama2, Olympus, Ajax, and Orca 2]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/the-ai-horizon-unveiling-the-titans-gemini/"/>
    <updated>2023-12-23T22:49:43+05:30</updated>
    <id>https://rishijeet.github.io/blog/the-ai-horizon-unveiling-the-titans-gemini</id>
    <content type="html"><![CDATA[<a name="Introduction"></a>
<h2>Introduction</h2>

<p>Artificial Intelligence (AI) has witnessed remarkable advancements in recent years, with various tech giants investing heavily in developing large language models (LLMs) to enhance natural language understanding and generation. This article delves into the technical details of Google&rsquo;s Gemini, Meta&rsquo;s Llama2, Amazon&rsquo;s Olympus, Microsoft&rsquo;s Orca 2, and Apple&rsquo;s Ajax.</p>

<a name="Google.Gemini"></a>
<h2>Google Gemini</h2>

<p>Google&rsquo;s Gemini, introduced by Demis Hassabis, CEO and Co-Founder of Google DeepMind, represents a significant leap in AI capabilities. Gemini is a multimodal AI model designed to seamlessly understand and operate across different types of information, including text, code, audio, image, and video.</p>

<p>Gemini is optimized for three different sizes:</p>

<ul>
<li><strong>Gemini Ultra:</strong> The largest and most capable model for highly complex tasks.</li>
<li><strong>Gemini Pro:</strong> The best model for scaling across a wide range of tasks.</li>
<li><strong>Gemini Nano:</strong> The most efficient model for on-device tasks.</li>
</ul>


<p>Gemini Ultra outperforms state-of-the-art results on various benchmarks, including massive multitask language understanding (MMLU) and multimodal benchmarks. With its native multimodality, Gemini excels in complex reasoning tasks, image understanding, and advanced coding across multiple programming languages.</p>

<p>The model is trained using Google&rsquo;s AI-optimized infrastructure, including Tensor Processing Units (TPUs) v4 and v5e. The announcement also introduces Cloud TPU v5p, the most powerful TPU system to date, designed to accelerate the development of large-scale generative AI models.</p>

<p>Gemini reflects Google&rsquo;s commitment to responsibility and safety, incorporating comprehensive safety evaluations, including bias and toxicity assessments. The model&rsquo;s availability spans various Google products and platforms, with plans for further integration and expansion.</p>

<a name="Meta.Llama2"></a>
<h2>Meta Llama2</h2>

<p>Meta&rsquo;s Llama2 is an open-source large language model (LLM) designed as a response to models like GPT from OpenAI and Google&rsquo;s AI models. Noteworthy for its open availability for research and commercial purposes, Llama2 is poised to make a significant impact in the AI space.</p>

<p>Functioning similarly to other LLMs like GPT-3 and PaLM 2, Llama2 uses a transformer architecture and employs techniques such as pretraining and fine-tuning. It is available in different sizes, with variations like Llama 2 7B Chat, Llama 2 13B Chat, and Llama 2 70B Chat, each optimized for specific use cases.</p>

<!-- more -->


<p>Llama2 was trained on 2 trillion tokens from publicly available sources, including Common Crawl, Wikipedia, and Project Gutenberg. The model undergoes training strategies, including reinforcement learning with human feedback (RLHF), to optimize safety and appropriateness of responses.</p>

<p>What sets Llama2 apart is its open nature, allowing users to access the research paper detailing its creation, download the model, and run it on various platforms. By providing transparency and openness, Meta aims to empower other companies to develop AI applications with more control.</p>

<a name="Amazon.Olympus"></a>
<h2>Amazon Olympus</h2>

<p>Amazon, in its pursuit of AI excellence, is working on an ambitious large language model (LLM) codenamed &ldquo;Olympus.&rdquo; With a staggering 2 trillion parameters, Olympus aims to rival leading models from OpenAI and Alphabet. Led by Rohit Prasad, former head of Alexa, the team behind Olympus brings together expertise from Alexa AI and Amazon&rsquo;s science team.</p>

<p>Amazon&rsquo;s strategy involves training homegrown models to make its offerings more appealing on Amazon Web Services (AWS), catering to enterprise clients seeking top-performing models. While Amazon has trained smaller models like Titan and collaborated with AI startups such as Anthropic and AI21 Labs, there&rsquo;s no specific timeline for the release of Olympus.</p>

<p>Large language models (LLMs) are crucial for AI tools that learn from extensive datasets to generate human-like responses. Despite the increased costs associated with training larger models, Amazon is committed to investing in LLMs and generative AI.</p>

<a name="Apple.Ajax"></a>
<h2>Apple Ajax</h2>

<p>Apple&rsquo;s investment in artificial intelligence is evident through its Foundational Models unit, focusing on conversational AI. Headed by John Giannandrea, Apple&rsquo;s head of AI, this unit is dedicated to improving Siri and developing AI models across multiple teams.</p>

<p>Apple is working on advanced LLMs, including Ajax GPT, trained on over 200 billion parameters, surpassing the capabilities of OpenAI&rsquo;s GPT-3.5. The models have applications ranging from customer interaction in AppleCare to automating multistep tasks with Siri.</p>

<p>In addition to conversational AI, Apple has Visual Intelligence and Multimodal AI units developing image generation models and models capable of recognizing and producing images, video, and text simultaneously.</p>

<p>Apple&rsquo;s commitment to AI innovation is reflected in its pursuit of powerful models and diverse AI applications, ensuring advancements in Siri and other AI-powered features.</p>

<a name="Microsoft.s.Orca.2"></a>
<h2>Microsoft&rsquo;s Orca 2</h2>

<p>Microsoft&rsquo;s Orca 2 employs a teacher-student training scheme, where a larger LLM acts as a teacher for a smaller one, aiming to improve the performance of the student model. The training involves teaching the student various reasoning techniques and selecting the most effective strategy for specific tasks.</p>

<p>Orca 2 outperformed baseline models, including Llama 2 and ChatGPT, on reasoning benchmarks. The model&rsquo;s performance is evaluated on tasks such as language understanding, text completion, and summarization. Microsoft&rsquo;s innovative training methodology involves &ldquo;Cautious Reasoning,&rdquo; where prompts eliciting specific problem-solving strategies are used during teacher training, and the prompts are erased during student training.</p>

<p>The comparison with other LLMs, including GPT-4 and Llama 2, demonstrates Orca 2&rsquo;s competitive performance. Microsoft&rsquo;s approach aims to address the challenges of hosting large LLMs and emphasizes the effectiveness of smaller models when fine-tuned.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The landscape of large language models continues to evolve, with major tech players pushing the boundaries of AI capabilities. From Google&rsquo;s Gemini to Meta&rsquo;s Llama2, Amazon&rsquo;s Olympus, Apple&rsquo;s Ajax, and Microsoft&rsquo;s Orca 2, each model brings unique features and applications. The open nature of Llama2 and the innovative training schemes of Orca 2 showcase the diverse approaches in AI research. As these models shape the future of AI applications, transparency, responsibility, and safety remain central to their development and deployment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vector Database: Transforming Data Storage and Retrieval in the AI Era]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/vector-database-transforming-data-storage-and-retrieval-in-the-ai-era/"/>
    <updated>2023-11-05T22:09:04+05:30</updated>
    <id>https://rishijeet.github.io/blog/vector-database-transforming-data-storage-and-retrieval-in-the-ai-era</id>
    <content type="html"><![CDATA[<p>The AI revolution has ushered in a new era of innovation, promising breakthroughs across various industries. However, with these advancements come unique challenges, particularly in handling and processing data efficiently. One of the key data types that have gained prominence in AI applications is vector embeddings. Vector databases play a pivotal role in managing and optimizing the retrieval of these embeddings. In this article, we will explore the architecture of vector databases and their crucial role in AI applications.</p>

<a name="What.is.a.Vector.Database."></a>
<h2>What is a Vector Database?</h2>

<p>A vector database is a specialized database designed to index and store vector embeddings for efficient retrieval and similarity search. These databases offer not only CRUD (Create, Read, Update, Delete) operations but also advanced capabilities like metadata filtering and horizontal scaling. They are essential for AI applications that rely on vector embeddings to understand patterns, relationships, and underlying structures in data.</p>

<p><img src="https://rishijeet.github.io/images/vector_db2.png" height="300" width="900" alt="Alt text" /><em>Source: Elastic</em></p>

<a name="Vector.Embeddings"></a>
<h3>Vector Embeddings</h3>

<p>Vector embeddings are data representations generated by AI models, such as large language models. They encapsulate semantic information critical for AI to understand and perform complex tasks effectively. These embeddings have multiple attributes or features, making their management a unique challenge.</p>

<p>Traditional scalar-based databases struggle to handle the complexity and scale of vector data, hindering real-time analysis and insights extraction. Vector databases are tailored to address these limitations, providing the performance, scalability, and flexibility needed for extracting valuable insights from vector embeddings.</p>

<!-- more -->


<a name="Vector.Database.Architecture"></a>
<h2>Vector Database Architecture</h2>

<p>Traditional databases store scalar data in rows and columns, whereas vector databases operate on vectors. They differ in the way data is optimized and queried.</p>

<p>In traditional databases, queries typically seek exact matches between query values and database records. In vector databases, similarity metrics are applied to find vectors that are most similar to the query. Vector databases employ a combination of algorithms to enable Approximate Nearest Neighbor (ANN) searches, which optimize retrieval speed.</p>

<a name="Vector.Database.Pipeline"></a>
<h3>Vector Database Pipeline</h3>

<p>A typical vector database pipeline consists of the following stages:</p>

<p><img src="https://rishijeet.github.io/images/vector_db_pipeline.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<ol>
<li><p><strong>Indexing</strong>: The database indexes vectors using algorithms like PQ (Product Quantization), LSH (Locality-Sensitive Hashing), or HNSW (Hierarchical Navigable Small World). This step maps vectors to data structures for faster searching.</p></li>
<li><p><strong>Querying</strong>: The database compares the indexed query vector to the indexed vectors in the dataset to find the nearest neighbors, applying a similarity metric used by the index.</p></li>
<li><p><strong>Post Processing</strong>: In some cases, the database retrieves the final nearest neighbors from the dataset and may post-process them, such as re-ranking them using a different similarity measure.</p></li>
</ol>


<a name="Algorithms.for.Vector.Indexing"></a>
<h3>Algorithms for Vector Indexing</h3>

<p>Vector databases rely on various algorithms to create efficient indexes for high-dimensional vector embeddings. These algorithms are designed to transform the original vector data into a compressed form, optimizing the query process for faster retrieval.</p>

<a name="Random.Projection"></a>
<h4>Random Projection</h4>

<p>Random projection is a technique that aims to project high-dimensional vectors into a lower-dimensional space using a random projection matrix. Here&rsquo;s how it works:</p>

<ul>
<li><p><strong>Projection Matrix Creation</strong>: A matrix of random numbers is created with the target lower-dimensional value. This matrix is then used to calculate the dot product of input vectors, resulting in a projected matrix that has fewer dimensions but still preserves vector similarity.</p></li>
<li><p><strong>Query Process</strong>: When a query is executed, the same projection matrix is used to project the query vector into the lower-dimensional space. The projected query vector is then compared to the projected vectors in the database to find the nearest neighbors. The reduced dimensionality of the data speeds up the search process.</p></li>
</ul>


<p>It&rsquo;s essential to note that random projection is an approximate method, and the quality of the projection depends on the properties of the projection matrix. Generating a truly random projection matrix can be computationally expensive, especially for large datasets.
<img src="https://rishijeet.github.io/images/vector_rp.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<a name="Product.Quantization"></a>
<h4>Product Quantization</h4>

<p>Product quantization (PQ) is a lossy compression technique tailored for high-dimensional vectors, such as vector embeddings. The process involves splitting, training, encoding, and querying:</p>

<ul>
<li><p><strong>Splitting</strong>: Vectors are divided into segments.</p></li>
<li><p><strong>Training</strong>: A &ldquo;codebook&rdquo; is created for each segment, representing potential codes for vectors. The codebook is established by performing k-means clustering on each segment, resulting in center points that serve as codes.</p></li>
<li><p><strong>Encoding</strong>: Each vector segment is assigned a specific code from the codebook, typically the nearest value. Multiple PQ codes can represent a segment.</p></li>
<li><p><strong>Query Process</strong>: During querying, vectors are broken down into sub-vectors and quantized using the codebook. The indexed codes are then used to find the nearest vectors to the query vector.</p></li>
</ul>


<p>The number of representative vectors in the codebook involves a trade-off between representation accuracy and computational cost. A larger codebook improves accuracy but increases computational expenses.
<img src="https://rishijeet.github.io/images/vector_pq.png" height="300" width="900" alt="Alt text" /><em>Source: Towards Data Science</em></p>

<a name="Locality-Sensitive.Hashing..LSH."></a>
<h4>Locality-Sensitive Hashing (LSH)</h4>

<p>Locality-sensitive hashing (LSH) is optimized for approximate nearest-neighbor search. LSH maps similar vectors into &ldquo;buckets&rdquo; using a set of hashing functions:</p>

<ul>
<li><p><strong>Indexing</strong>: Similar vectors are grouped into hash tables using the hashing functions.</p></li>
<li><p><strong>Query Process</strong>: To find the nearest neighbors for a query vector, the same hashing functions are used to map the query vector to a specific table. The query vector is then compared with the vectors in that table to find the closest matches. This method accelerates searching by reducing the number of vectors to consider.</p></li>
</ul>


<p>LSH is an approximate method, and the quality of the approximation depends on the properties of the hash functions. Using more hash functions improves approximation quality but can be computationally expensive, especially for large datasets.
<img src="https://rishijeet.github.io/images/vector_lsh.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<a name="Hierarchical.Navigable.Small.World..HNSW."></a>
<h4>Hierarchical Navigable Small World (HNSW)</h4>

<p>HNSW creates a hierarchical, tree-like structure where each node represents a set of vectors, and edges indicate similarity between vectors. The algorithm follows these steps:</p>

<ul>
<li><p><strong>Node Creation</strong>: A set of nodes is established, each containing a small number of vectors. Nodes can be created randomly or by clustering vectors using algorithms like k-means.</p></li>
<li><p><strong>Edge Formation</strong>: The algorithm examines the vectors within each node and establishes edges between the node and those nodes containing the most similar vectors.</p></li>
<li><p><strong>Query Process</strong>: When querying an HNSW index, the algorithm navigates the hierarchical structure, visiting nodes that are likely to contain the closest vectors to the query vector.</p></li>
</ul>


<p><img src="https://rishijeet.github.io/images/vector_hnsw.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<a name="Similarity.Measures"></a>
<h3>Similarity Measures</h3>

<p>The choice of similarity measure plays a crucial role in vector database performance. Common similarity measures include:</p>

<ul>
<li><p><strong>Cosine Similarity</strong>: Measures the cosine of the angle between two vectors, with a range from -1 to 1. It signifies the degree of similarity between vectors.</p></li>
<li><p><strong>Euclidean Distance</strong>: Measures the straight-line distance between vectors in a vector space, with a range from 0 to infinity.</p></li>
<li><p><strong>Dot Product</strong>: Measures the product of the magnitudes of two vectors and the cosine of the angle between them, with a range from -∞ to ∞.</p></li>
</ul>


<p>The selection of the appropriate similarity measure depends on the specific use case and requirements.</p>

<a name="Filtering"></a>
<h3>Filtering</h3>

<p>Each vector stored in the database includes associated metadata. Vector databases can filter query results based on metadata queries, typically maintaining both vector and metadata indexes. The filtering process can occur before or after the vector search, with trade-offs in terms of efficiency.</p>

<ul>
<li><p><strong>Pre-filtering</strong>: Filters are applied before the vector search. While reducing the search space, it may exclude relevant results not meeting metadata filter criteria and add computational overhead.</p></li>
<li><p><strong>Post-filtering</strong>: Filters are applied after the vector search. This ensures all relevant results are considered but may introduce additional processing overhead.</p></li>
</ul>


<p><img src="https://rishijeet.github.io/images/vector_filter.png" height="300" width="900" alt="Alt text" /><em>Source: Pinecone</em></p>

<p>Optimizing the filtering process involves techniques like advanced indexing for metadata and parallel processing to balance performance and accuracy.</p>

<a name="Vector.Database.vs..Vector.Index"></a>
<h3>Vector Database vs. Vector Index</h3>

<p>While standalone vector indices like FAISS (Facebook AI Similarity Search) can enhance the search and retrieval of vector embeddings, they lack essential database features. Vector databases offer several advantages over standalone vector indices, including:</p>

<ol>
<li><p><strong>Data Management</strong>: Vector databases provide traditional database features for easy data management, such as insertion, deletion, and updating. This simplifies vector data management compared to standalone vector indices like FAISS, which require additional integration with storage solutions.</p></li>
<li><p><strong>Metadata Storage and Filtering</strong>: Vector databases can store metadata associated with each vector entry. Users can query the database using additional metadata filters for more granular queries.</p></li>
<li><p><strong>Scalability</strong>: Vector databases are designed for scalability, supporting the growth of data volumes and user demands. They excel in distributed and parallel processing, while standalone vector indices may require custom solutions for similar scalability.</p></li>
<li><p><strong>Real-time Updates</strong>: Vector databases often support real-time data updates, allowing dynamic changes to the data. Standalone vector indexes may require time-consuming and computationally expensive full re-indexing to incorporate new data.</p></li>
<li><p><strong>Backups and Collections</strong>: Vector databases handle data backup operations, and users can selectively choose specific indexes to back up in the form of &ldquo;collections.&rdquo; This feature ensures data resilience and retrievability.</p></li>
<li><p><strong>Ecosystem Integration</strong>: Vector databases can seamlessly integrate with other components of a data processing ecosystem, streamlining data management workflows. This integration extends to AI-related tools, fostering a comprehensive ecosystem for AI applications.</p></li>
<li><p><strong>Data Security and Access Control</strong>: Vector databases typically include built-in data security features and access control mechanisms to safeguard sensitive information. These security measures may not be available in standalone vector index solutions.</p></li>
</ol>


<p>In summary, vector databases are purpose-built for managing vector embeddings, addressing the limitations of standalone vector indices and offering a more effective and streamlined data management experience.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>In the age of AI, efficient data processing and retrieval are paramount for applications relying on vector embeddings. Vector databases are purpose-built to handle these complex data types, offering advanced capabilities for storage, retrieval, and analysis. Understanding the architecture and capabilities of vector databases empowers organizations to unlock the full potential of their AI applications, gaining a competitive edge in the rapidly evolving AI landscape.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Innovative GenAI Applications with the GenAI Stack: Unleashing the Power of Docker]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/building-innovative-genai-applications-with-the-genai-stack-unleashing-the-power-of-docker/"/>
    <updated>2023-11-04T22:49:05+05:30</updated>
    <id>https://rishijeet.github.io/blog/building-innovative-genai-applications-with-the-genai-stack-unleashing-the-power-of-docker</id>
    <content type="html"><![CDATA[<p>In the fast-evolving landscape of artificial intelligence, Generative AI (GenAI) is at the forefront, opening up exciting opportunities for developers and businesses. One of the most significant challenges in GenAI development is creating a robust, efficient, and scalable infrastructure that harnesses the power of AI models. To address this challenge, the GenAI Stack has emerged as a game-changer, combining cutting-edge technologies like Docker, LangChain, Neo4j, and Ollama. In this article, we will delve into the intricacies of these technologies and explore how they work together to build innovative GenAI applications.</p>

<a name="Understanding.the.GenAI.Stack"></a>
<h2>Understanding the GenAI Stack</h2>

<p>Before we dive into the technical details, let&rsquo;s establish a clear understanding of what the GenAI Stack is and what it aims to achieve.</p>

<p>The GenAI Stack is a comprehensive environment designed to facilitate the development and deployment of GenAI applications. It provides a seamless integration of various components, including a management tool for local Large Language Models (LLMs), a database for grounding, and GenAI apps powered by LangChain. Here&rsquo;s a breakdown of these components and their roles:</p>

<ol>
<li><p><strong>Docker:</strong> Docker is a containerization platform that allows developers to package applications and their dependencies into containers. These containers are lightweight, portable, and provide consistent runtime environments, making them an ideal choice for deploying GenAI applications.</p></li>
<li><p><strong>LangChain:</strong> LangChain is a powerful tool that orchestrates GenAI applications. It&rsquo;s the brains behind the application logic and ensures that the various components of the GenAI Stack work harmoniously together. LangChain simplifies the process of building and orchestrating GenAI applications.</p></li>
<li><p><strong>Neo4j:</strong> Neo4j is a highly versatile graph database that serves as the backbone of GenAI applications. It provides a robust foundation for building knowledge graph-based applications. Neo4j&rsquo;s graph database capabilities are instrumental in managing and querying complex relationships and data structures.</p></li>
<li><p><strong>Ollama:</strong> Ollama represents the core of the GenAI Stack. It is a local LLM container that brings the power of large language models to your GenAI applications. Ollama enables you to run LLMs on your infrastructure or even on your local machine, providing more control and flexibility over your GenAI models.</p></li>
</ol>


<!-- more -->


<a name="Docker:.The.Containerization.Revolution"></a>
<h2>Docker: The Containerization Revolution</h2>

<p>Docker has revolutionized application deployment and management. It introduces the concept of containerization, allowing developers to bundle their applications and dependencies into containers. These containers are isolated and share the host operating system&rsquo;s kernel, making them lightweight and efficient. Docker&rsquo;s advantages include:</p>

<ul>
<li><p><strong>Portability:</strong> Containers can run on any platform that supports Docker, ensuring consistency across different environments.</p></li>
<li><p><strong>Scalability:</strong> Docker&rsquo;s container orchestration tools, such as Kubernetes, make it easy to scale applications horizontally.</p></li>
<li><p><strong>Resource Efficiency:</strong> Containers consume fewer resources compared to traditional virtual machines, allowing for better resource utilization.</p></li>
</ul>


<p><img src="https://rishijeet.github.io/images/docker.png" height="300" width="900" alt="Alt text" /><em>Source: Whizlabs</em></p>

<p>In the GenAI Stack, Docker is the foundation that ensures all components work seamlessly together, providing a consistent and controlled environment for GenAI applications.</p>

<a name="LangChain:.Orchestrating.GenAI.Applications"></a>
<h2>LangChain: Orchestrating GenAI Applications</h2>

<p>LangChain is the orchestrator of GenAI applications within the GenAI Stack. It is designed to simplify the process of building, managing, and deploying GenAI applications. Key features of LangChain include:</p>

<ul>
<li><p><strong>Application Logic:</strong> LangChain houses the application logic in Python, allowing developers to create GenAI apps easily.</p></li>
<li><p><strong>User Interface:</strong> LangChain leverages Streamlit for creating user interfaces, enabling developers to build interactive and user-friendly applications.</p></li>
<li><p><strong>Docker Integration:</strong> LangChain seamlessly integrates with Docker, facilitating containerization and deployment of GenAI apps.</p></li>
<li><p><strong>Development Environment:</strong> LangChain provides a development environment that supports rapid feedback loops, making it easier for developers to iterate on their applications.</p></li>
</ul>


<p><img src="https://rishijeet.github.io/images/langchain.png" height="300" width="900" alt="Alt text" /><em>Source: Packt</em></p>

<p>LangChain is the bridge that connects various components of the GenAI Stack, ensuring that they work together cohesively to bring GenAI applications to life.</p>

<a name="Neo4j:.Powering.Knowledge.Graph-Based.Applications"></a>
<h2>Neo4j: Powering Knowledge Graph-Based Applications</h2>

<p>Knowledge graphs have become a pivotal component in GenAI applications. Neo4j, a graph database, plays a crucial role in managing the intricate relationships and data structures that underpin these applications. Key attributes of Neo4j include:</p>

<ul>
<li><p><strong>Graph Database:</strong> Neo4j stores and manages data in a graph format, making it ideal for applications that require intricate data relationships.</p></li>
<li><p><strong>Querying Capabilities:</strong> Neo4j provides powerful querying capabilities, allowing developers to retrieve and manipulate data in a flexible and efficient manner.</p></li>
<li><p><strong>Scalability:</strong> Neo4j can scale horizontally to accommodate growing data and application demands.</p></li>
</ul>


<p><img src="https://rishijeet.github.io/images/neo4j.svg" height="300" width="900" alt="Alt text" /><em>Source: Neo4j</em></p>

<p>In GenAI applications, Neo4j serves as the foundation for creating knowledge graph-based applications. It allows developers to model and query complex relationships, ultimately enhancing the accuracy and relevance of GenAI responses.</p>

<a name="Ollama:.The.Power.of.Local.LLMs"></a>
<h2>Ollama: The Power of Local LLMs</h2>

<p>Large Language Models (LLMs) are at the heart of GenAI applications. Ollama, an integral part of the GenAI Stack, brings LLMs to the local environment, offering more control and flexibility. Key advantages of Ollama include:</p>

<ul>
<li><p><strong>Open Source:</strong> Ollama is an open-source project, enabling developers to run LLMs without depending on external providers.</p></li>
<li><p><strong>Data Control:</strong> Ollama allows developers to have complete control over data flows, storage, and sharing.</p></li>
<li><p><strong>Local Deployment:</strong> Developers can run Ollama on their infrastructure or even on a local machine, making it a versatile choice for GenAI development.</p></li>
</ul>


<p><img src="https://rishijeet.github.io/images/ollama.png" height="300" width="900" alt="Alt text" /><em>Source: Ollama</em></p>

<p>Ollama represents a significant step forward in GenAI development by providing a seamless solution for setting up and running local LLMs, removing dependencies on external providers, and offering more control over the data flow.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The GenAI Stack powered by Docker, LangChain, Neo4j, and Ollama is a formidable combination for building innovative GenAI applications. It simplifies the development process, provides the infrastructure for knowledge graph-based applications, and empowers developers with local LLM capabilities. With these technologies at your disposal, you can create GenAI applications that are accurate, relevant, and highly customizable.</p>

<p>As the GenAI landscape continues to evolve, the GenAI Stack is a beacon of innovation, enabling developers to unlock the full potential of artificial intelligence. Whether you&rsquo;re building chatbots, support agents, or knowledge retrieval systems, the GenAI Stack has the tools you need to make your GenAI applications shine.</p>

<p>It&rsquo;s time to explore the possibilities, experiment with GenAI, and build the next generation of intelligent applications using the GenAI Stack. The future of GenAI is here, and it&rsquo;s waiting for your creative ideas and innovations to transform it.</p>

<p><em>Disclaimer: The images and URLs in this blog are for illustrative purposes only and may not represent real services or websites.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Webhook vs. WebSocket: Choosing the Right Communication Mechanism for Your Application]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/webhook-vs-websocket-choosing-the-right-communication-mechanism-for-your-application/"/>
    <updated>2023-11-04T20:56:03+05:30</updated>
    <id>https://rishijeet.github.io/blog/webhook-vs-websocket-choosing-the-right-communication-mechanism-for-your-application</id>
    <content type="html"><![CDATA[<p>In today&rsquo;s digital age, communication between applications is crucial, and it&rsquo;s the APIs (Application Programming Interfaces) that act as the mediators. APIs provide a standardized way for software modules, applications, and devices to exchange data and instructions. However, not all communication needs can be met by APIs alone. In this article, we&rsquo;ll explore three different communication mechanisms: API, WebHook, and WebSocket, and help you understand when to use each one.</p>

<a name="Understanding.API.Interfaces"></a>
<h2>Understanding API Interfaces</h2>

<p>APIs are the backbone of modern application development. They guide machines, devices, and applications on how to interact with each other, just like human language allows us to express our thoughts. APIs define the rules and methods for data exchange between the client-side application and the server-side infrastructure.</p>

<p>There are three primary types of APIs:</p>

<ol>
<li><strong>Private API:</strong> Restricted to authorized personnel within an organization.</li>
<li><strong>Public API:</strong> Accessible to anyone without restrictions.</li>
<li><strong>Partner API:</strong> Used to enable business partnerships and third-party integrations.</li>
</ol>


<p>APIs are essential for ensuring secure and efficient communication between various components of an application. Poor API security can lead to data corruption and pose a significant risk to the entire application.</p>

<a name="WebHook:.Reverse.API.for.Event-Driven.Communication"></a>
<h2>WebHook: Reverse API for Event-Driven Communication</h2>

<p>WebHooks can be thought of as reverse APIs since they operate in the opposite direction. While APIs allow clients to request data from the server, WebHooks enable servers to push information to other servers or applications. They are often referred to as server-to-server push notifications.</p>

<p>WebHooks are highly versatile and are ideal for handling integrations between different solutions or applications. They are typically used to notify an application or web app about specific events, such as receiving a message, processing a payment, or any other update.</p>

<a name="WebSocket:.Real-Time..Bidirectional.Communication"></a>
<h2>WebSocket: Real-Time, Bidirectional Communication</h2>

<p>WebSocket is a communication protocol that enables full-duplex, bidirectional communication over a single TCP connection. Unlike HTTP-based APIs, WebSocket maintains a continuous, open connection, making it suitable for real-time applications. It is considered a stateful protocol because the communication remains active until one of the parties terminates it, and it employs a 3-way handshake for connection establishment.</p>

<p>WebSocket is perfect for applications that demand real-time communication, as it allows information exchange at any time and from anywhere. It is particularly useful for collaborative tools, data visualization applications, and chat applications, where immediate and bidirectional communication is essential.</p>

<!-- more -->


<a name="When.to.Use.API.Interfaces..WebHooks..and.WebSockets"></a>
<h2>When to Use API Interfaces, WebHooks, and WebSockets</h2>

<p>Choosing the right communication mechanism depends on the specific needs of your application:</p>

<p><strong>API Interfaces:</strong>
- Ideal for applications that require an easy interface for end-users.
- Suitable for CRUD operations from mobile and web apps, data transfer using XML or JSON, and frequent data changes.
- Useful for applications that demand instant responses to user requests (e.g., live chat applications, messenger apps, IoT devices, and wearable devices).
<img src="https://rishijeet.github.io/images/API%20Interface.jpg" height="300" width="900" alt="Alt text" /><em>Source: wallarm</em></p>

<p><strong>WebSockets:</strong>
- Perfect for applications that require real-time communication.
- Promotes bidirectional communication and maintains an open connection.
- Suited for collaborative and chat-centric applications (e.g., modern browsers, data visualization tools, and chat applications).
<img src="https://rishijeet.github.io/images/websockets.jpg" height="300" width="900" alt="Alt text" /><em>Source: mirrorfly</em></p>

<p><strong>WebHooks:</strong>
- Best for making backend calls and handling one-way event-driven communication.
- Ideal when your application needs to fetch data from a third-party application.
- Preferred for applications deployed on the cloud, where open communication is not necessary (e.g., Discord Bots).
<img src="https://rishijeet.github.io/images/webhooks.jpg" height="300" width="900" alt="Alt text" /><em>Source: wallarm</em></p>

<p>In conclusion, the effectiveness of an application often hinges on how well server-client communication is implemented. APIs, WebHooks, and WebSockets are three essential ways to facilitate this communication. APIs are the foundation, while WebSockets excel in event-driven scenarios, and WebHooks are best for one-way event-infused communication. Choose the communication mechanism that aligns with your application&rsquo;s requirements for optimal functionality and user experience.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Revolutionizing AI Inference: Lightmatter's Envise Chip]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/revolutionizing-ai-inference-lightmatters-envise-chip/"/>
    <updated>2023-06-18T22:24:50+05:30</updated>
    <id>https://rishijeet.github.io/blog/revolutionizing-ai-inference-lightmatters-envise-chip</id>
    <content type="html"><![CDATA[<p>Artificial Intelligence (AI) is rapidly transforming various industries, from autonomous driving and robotics to healthcare and customer service. As the demand for AI applications grows, so does the need for more powerful and energy-efficient processors. In this context, Lightmatter, a company at the forefront of photonic processors, has developed the Envise chip—an innovative solution that promises unprecedented performance and energy efficiency in AI inference.</p>

<a name="Unleashing.Unprecedented.Power.and.Efficiency"></a>
<h5>Unleashing Unprecedented Power and Efficiency</h5>

<p>The Envise chip is a game-changer in the world of AI inference. It features 16 Envise Chips in a 4-U server configuration, consuming only 3kW of power. This remarkable power efficiency enables the chip to run the largest neural networks developed to date with exceptional performance. In fact, Lightmatter claims that the Envise chip delivers three times higher instructions per second (IPS) than the Nvidia DGX-A100, while achieving eight times the IPS per watt on BERT-Base SQuAD. These numbers are staggering and highlight the potential of the Envise chip to redefine AI inference capabilities.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<a name="Unmatched.Specifications"></a>
<h5>Unmatched Specifications</h5>

<p>The Envise chip boasts several cutting-edge features that contribute to its remarkable performance. Its on-chip activation and weight storage eliminate the need to transfer data to external memory, enabling state-of-the-art neural network execution within the processor itself. Additionally, the chip utilizes a standards-based host and interconnect interface, offering seamless integration into existing systems. The inclusion of RISC cores per Envise processor provides generic off-load capabilities, enhancing the chip&rsquo;s versatility. Its ultra-high-performance out-of-order super-scalar processing architecture further optimizes computation efficiency.</p>

<!-- more -->


<p><img src="https://rishijeet.github.io/images/Photonics.jpg" height="400" width="900" alt="Alt text" /><em>Source: Lightmatter</em></p>

<a name="Applications.Across.Industries"></a>
<h5>Applications Across Industries</h5>

<p>The applications of the Envise chip are vast and span various industries. In the automotive sector, it can power autonomous driving systems, improving safety and efficiency on the road. In robotics, the chip enables advanced vision and control capabilities, empowering robots to perform complex tasks with precision. In e-commerce and advertising, it facilitates accurate product recommendations, enhancing customer experiences. The chip finds utility in healthcare for pharmaceutical research, pathology analysis, and cancer detection. Moreover, it enhances customer service through digital assistants and chatbots. Its potential also extends to signal processing and natural language processing, enabling tasks such as digital signal analysis, text-to-speech, and language translation.</p>

<a name="A.Sustainable.and.Economical.Solution"></a>
<h5>A Sustainable and Economical Solution</h5>

<p>Lightmatter&rsquo;s Envise chip not only delivers superior performance but also addresses the pressing concern of power consumption in data centers. With ICT energy consumption projected to increase significantly in the coming years, solutions that offer high compute capabilities with reduced energy consumption are crucial for sustainable and cost-effective data centers. The Envise chip&rsquo;s photonic architecture leverages silicon photonics, consuming significantly less energy than traditional CMOS solutions. By reducing power demands, the chip helps mitigate the environmental impact and supports the growth of energy-efficient data centers.</p>

<a name="Looking.Ahead"></a>
<h5>Looking Ahead</h5>

<p>The Envise chip represents a significant milestone in the advancement of AI inference technology. With its impressive performance, energy efficiency, and versatile applications, it has the potential to reshape industries and accelerate AI adoption. Lightmatter&rsquo;s dedication to bringing its product to market, as demonstrated by its recent funding and strategic board appointments, underscores its commitment to revolutionizing the AI landscape.</p>

<p><img src="https://rishijeet.github.io/images/Hot-Chips-32-Lightmatter-Software.jpg" height="300" width="900" alt="Alt text" /><em>Source: Lightmatter</em></p>

<p>Lightmatter&rsquo;s Envise chip stands as a testament to the rapid evolution of AI inference processors. Its exceptional performance, energy efficiency, and broad applications make it a force to be reckoned with in the AI industry. As more companies explore photonic processors and accelerators, the development of specialized computing devices like the Envise chip paves the way for a future where AI capabilities are maximized while minimizing energy consumption. The Envise chip is poised to transform industries and contribute to the realization of sustainable and economical data centers.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><em>Source Lightmatter</em><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AI Deep Learning: Unleashing the Power of Neural Networks]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/ai-deep-learning-unleashing-the-power-of-neural-networks/"/>
    <updated>2023-05-23T23:35:46+05:30</updated>
    <id>https://rishijeet.github.io/blog/ai-deep-learning-unleashing-the-power-of-neural-networks</id>
    <content type="html"><![CDATA[<p>Artificial intelligence (AI) and its subset, deep learning, have revolutionized numerous industries, from healthcare to autonomous vehicles. Deep learning, an approach within AI, has garnered significant attention for its ability to process vast amounts of data and extract complex patterns. In this advanced tech article, we will delve into the core concepts and techniques of deep learning, exploring its architecture, training process, and real-world applications.</p>

<a name="The.Basics.of.Deep.Learning"></a>
<h5>The Basics of Deep Learning</h5>

<ul>
<li><p>Neural Networks: Deep learning relies on neural networks, inspired by the human brain&rsquo;s structure and functioning. Neural networks consist of interconnected layers of artificial neurons, with each neuron performing a weighted computation and applying an activation function.</p></li>
<li><p>Deep Neural Networks (DNNs): DNNs are neural networks with multiple hidden layers, enabling them to learn hierarchical representations of data. These layers enable deep learning models to capture intricate patterns and relationships within complex datasets.</p></li>
</ul>


<a name="Deep.Learning.Architectures"></a>
<h5>Deep Learning Architectures</h5>

<a name="Convolutional.Neural.Networks..CNNs."></a>
<h6>Convolutional Neural Networks (CNNs)</h6>

<p>CNNs are designed for image and video analysis. They employ convolutional layers to extract local features from the input, pooling layers for downsampling, and fully connected layers for classification.</p>

<p>Convolutional Neural Networks (CNNs) Example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Define the CNN model</span>
</span><span class='line'><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">))</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;softmax&#39;</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Compile and train the model</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;adam&#39;</span><span class="p">,</span>
</span><span class='line'>              <span class="n">loss</span><span class="o">=</span><span class="s">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
</span><span class='line'>              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;accuracy&#39;</span><span class="p">])</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<!--more-->


<a name="Recurrent.Neural.Networks..RNNs."></a>
<h6>Recurrent Neural Networks (RNNs)</h6>

<p>RNNs are suitable for sequential data, such as text or time-series data. They utilize recurrent connections to capture temporal dependencies and process variable-length sequences.</p>

<p>Recurrent Neural Networks (RNNs) Example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Define the RNN model</span>
</span><span class='line'><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;softmax&#39;</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Compile and train the model</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;adam&#39;</span><span class="p">,</span>
</span><span class='line'>              <span class="n">loss</span><span class="o">=</span><span class="s">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
</span><span class='line'>              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;accuracy&#39;</span><span class="p">])</span>
</span><span class='line'><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_sequences</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_sequences</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<a name="Generative.Adversarial.Networks..GANs."></a>
<h6>Generative Adversarial Networks (GANs)</h6>

<p>GANs consist of a generator and a discriminator, engaged in a competitive training process. GANs generate new data samples that closely resemble the training data, making them useful for tasks like image generation and data synthesis.</p>

<p>Generative Adversarial Networks (GANs) Example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Define the generator model</span>
</span><span class='line'><span class="n">generator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span class='line'><span class="n">generator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">))</span>
</span><span class='line'><span class="n">generator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;tanh&#39;</span><span class="p">))</span>
</span><span class='line'><span class="n">generator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Define the discriminator model</span>
</span><span class='line'><span class="n">discriminator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span class='line'><span class="n">discriminator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</span><span class='line'><span class="n">discriminator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">))</span>
</span><span class='line'><span class="n">discriminator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;sigmoid&#39;</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Combine the generator and discriminator into a GAN</span>
</span><span class='line'><span class="n">gan</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">generator</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Compile and train the GAN</span>
</span><span class='line'><span class="n">discriminator</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">&#39;binary_crossentropy&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">discriminator</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</span><span class='line'><span class="n">gan</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">&#39;binary_crossentropy&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">gan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note: These code snippets provide a basic structure for each model and may require additional adjustments based on your specific use case and dataset. Make sure to import the necessary libraries, preprocess your data, and customize the models accordingly.</p>

<a name="Training.Deep.Learning.Models"></a>
<h5>Training Deep Learning Models</h5>

<ol type="a">
<li><p>Backpropagation: Backpropagation is a key algorithm used to train deep learning models. It calculates the gradients of the model&rsquo;s parameters with respect to a loss function, allowing the network to update its weights and improve its performance.</p></li>
<li><p>Optimization Algorithms: Various optimization algorithms, such as stochastic gradient descent (SGD), Adam, and RMSprop, help find the optimal set of weights for deep learning models. These algorithms aim to minimize the loss function and improve model accuracy.</p></li>
</ol>


<a name="Conclusion"></a>
<h5>Conclusion</h5>

<p>Deep learning is at the forefront of AI advancements, enabling machines to learn complex patterns and make accurate predictions. With neural networks as their foundation, deep learning models like CNNs, RNNs, and GANs have transformed various domains, including computer vision, natural language processing, and autonomous systems. Understanding the architecture, training process, and real-world applications of deep learning empowers developers and researchers to harness its immense potential. As deep learning continues to evolve, it holds the key to solving increasingly complex problems and driving innovation across industries.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What's new in Java 20?]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/whats-new-in-java-20/"/>
    <updated>2023-05-23T23:03:47+05:30</updated>
    <id>https://rishijeet.github.io/blog/whats-new-in-java-20</id>
    <content type="html"><![CDATA[<p>Java, being one of the most widely used programming languages, continues to evolve with each new release, bringing enhancements, features, and improvements to the development community. In this tech article, we will explore the exciting new features introduced in Java 20, highlighting the advancements that developers can leverage to build robust, efficient, and modern applications.</p>

<a name="Improved.Pattern.Matching.for.instanceof"></a>
<h5>Improved Pattern Matching for instanceof</h5>

<p>Java 20 introduces further improvements to pattern matching for the instanceof operator, building upon the enhancements introduced in previous versions. Developers can now use patterns in switch statements with instanceof, simplifying code and reducing the need for explicit casting.</p>

<a name="Records"></a>
<h5>Records</h5>

<p>Java 20 introduces the records feature, which provides a concise syntax for defining immutable data classes. Records eliminate the need for boilerplate code by automatically generating constructors, accessors, and other methods. They promote readability, immutability, and ease of use when working with data-centric classes.</p>

<a name="Sealed.Classes"></a>
<h5>Sealed Classes</h5>

<p>Sealed classes offer enhanced control over class inheritance and improve code maintainability. Java 20 introduces sealed classes that allow developers to define a limited set of subclasses that can extend them. This feature helps enforce encapsulation, restrict inheritance, and make code more predictable.</p>

<!--more-->


<a name="Vector.API..Incubator."></a>
<h5>Vector API (Incubator)</h5>

<p>The Vector API, introduced as an incubating feature in Java 20, provides a platform-independent way to express vector computations that can leverage hardware-specific vector units. This API allows developers to write high-performance, portable code for vector operations, enabling efficient use of hardware capabilities.</p>

<a name="Foreign.Function..amp..Memory.API..Incubator."></a>
<h5>Foreign Function &amp; Memory API (Incubator)</h5>

<p>Java 20 introduces the Foreign Function &amp; Memory API as an incubating feature. This API allows Java programs to interoperate with native code, enabling developers to call native functions and access native memory directly. It simplifies integration with existing native libraries and provides more flexibility in Java programming.</p>

<a name="Enhanced.Garbage.Collection"></a>
<h5>Enhanced Garbage Collection</h5>

<p>Java 20 brings advancements in garbage collection algorithms, including the Concurrent Mark and Sweep (CMS) algorithm and the G1 garbage collector. These improvements focus on reducing pause times, improving memory management, and optimizing garbage collection performance. Developers can expect more predictable behavior and enhanced application performance.</p>

<a name="Performance.and.Security.Enhancements"></a>
<h5>Performance and Security Enhancements</h5>

<p>Java 20 includes various performance and security enhancements. These include improved startup times, optimized code execution, reduced memory footprint, enhanced security features, and updates to cryptographic algorithms. These improvements contribute to better application performance, security, and overall user experience.</p>

<a name="Conclusion"></a>
<h3>Conclusion</h3>

<p>Java 20 introduces several exciting features and improvements that enhance developer productivity, code readability, performance, and security. The advancements in pattern matching, sealed classes, records, and the Vector API empower developers to write cleaner, more concise code and leverage hardware capabilities efficiently. The incubating features, such as the Foreign Function &amp; Memory API, offer new possibilities for integration with native code. Additionally, the enhanced garbage collection algorithms and performance optimizations contribute to better application performance.</p>

<p>As a Java developer, it is essential to stay updated with the latest features and best practices introduced in Java 20. By leveraging these advancements, developers can build robust, efficient, and secure applications that meet the demands of modern software development.</p>

<a name="References"></a>
<h3>References</h3>

<ul>
<li>OpenJDK. &ldquo;JEPs in JDK 20.&rdquo; <a href="https://openjdk.java.net/projects/jdk/20/">https://openjdk.java.net/projects/jdk/20/</a></li>
<li>Oracle. &ldquo;Java 20 Release Notes.&rdquo; <a href="https://www.oracle.com/java/technologies/javase/20-relnotes.html">https://www.oracle.com/java/technologies/javase/20-relnotes.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning and AI Revolutionizing the e-Trading Market]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/machine-learning-and-ai-revolutionizing-the-e-trading-market/"/>
    <updated>2023-05-23T21:44:08+05:30</updated>
    <id>https://rishijeet.github.io/blog/machine-learning-and-ai-revolutionizing-the-e-trading-market</id>
    <content type="html"><![CDATA[<p>The world of electronic trading (e-Trading) has undergone a profound transformation with the emergence of machine learning and artificial intelligence (AI). These technologies have revolutionized how financial markets operate, empowering traders with advanced tools and insights to make more informed decisions. In this blog, we will explore the significant impact of machine learning and AI on the e-Trading market, highlighting their transformative potential and the benefits they bring to traders and investors.</p>

<a name="Enhanced.Data.Analysis.and.Decision-Making"></a>
<h5>Enhanced Data Analysis and Decision-Making</h5>

<p>Machine learning algorithms excel at analyzing vast amounts of financial data, identifying patterns, and extracting valuable insights. By processing market data in real-time, AI-powered systems can recognize complex patterns and relationships that might not be apparent to human traders. This enables more accurate predictions and informed decision-making, empowering traders to seize opportunities and mitigate risks effectively.</p>

<a name="Algorithmic.Trading.and.Execution"></a>
<h5>Algorithmic Trading and Execution</h5>

<p>One of the prominent applications of machine learning and AI in e-Trading is algorithmic trading. AI algorithms can automatically execute trades based on predefined rules, market conditions, and predictive models. These algorithms leverage historical and real-time data, continuously learning and adapting to changing market dynamics. Algorithmic trading not only improves execution speed but also reduces human errors and emotions, leading to more efficient and precise trading strategies.</p>

<a name="Risk.Management.and.Fraud.Detection"></a>
<h5>Risk Management and Fraud Detection</h5>

<p>Machine learning algorithms play a crucial role in risk management within e-Trading. By analyzing historical data and real-time market indicators, AI models can identify potential risks and deviations from normal trading patterns. This allows traders to implement risk mitigation strategies and protect their portfolios. Additionally, AI-powered systems can detect and prevent fraudulent activities, such as market manipulation or insider trading, ensuring a fair and transparent trading environment.</p>

<!--more-->


<a name="Sentiment.Analysis.and.News.Impact"></a>
<h5>Sentiment Analysis and News Impact</h5>

<p>The ability to analyze and interpret market sentiment and news impact is vital in e-Trading. Machine learning and AI algorithms can process vast amounts of textual and sentiment data from news articles, social media, and other sources. By gauging market sentiment and assessing the impact of news events, traders can make more informed decisions and adjust their strategies accordingly. This enhances their ability to capitalize on market movements driven by news and sentiment shifts.</p>

<a name="Market.Prediction.and.Forecasting"></a>
<h5>Market Prediction and Forecasting</h5>

<p>Machine learning models, including neural networks and deep learning algorithms, have demonstrated remarkable capabilities in market prediction and forecasting. These models learn from historical data, capturing complex patterns and relationships, and generate predictions about future market movements. Traders can leverage these predictive insights to identify potential entry and exit points, optimize trading strategies, and improve overall performance in the e-Trading market.</p>

<a name="High-Frequency.Trading..HFT."></a>
<h5>High-Frequency Trading (HFT)</h5>

<p>High-frequency trading has seen a significant impact from machine learning and AI. HFT algorithms leverage powerful computational capabilities to analyze and execute trades at lightning speed, taking advantage of small price discrepancies and fleeting market opportunities. Machine learning techniques enable HFT systems to adapt to changing market conditions, optimize trade execution, and generate profits in highly competitive and fast-paced trading environments.</p>

<a name="Conclusion"></a>
<h3>Conclusion</h3>

<p>Machine learning and AI have revolutionized the e-Trading market, empowering traders with advanced tools, insights, and automation capabilities. From enhanced data analysis and algorithmic trading to risk management and market prediction, these technologies have transformed how financial markets operate. As machine learning and AI continue to evolve, we can expect further advancements in e-Trading, driving efficiency, accuracy, and profitability for traders and investors.</p>

<p>It&rsquo;s important to note that while AI brings significant benefits, human expertise remains crucial in interpreting AI-generated insights, ensuring regulatory compliance, and making strategic decisions. Combining the power of AI with human judgment and experience will lead to optimal results in the dynamic and complex world of e-Trading.</p>

<a name="References"></a>
<h3>References</h3>

<ul>
<li>Prendergast, K. (2018). &ldquo;Machine Learning in Algorithmic Trading.&rdquo; <a href="https://www.cognizant.com/whitepapers/machine-learning-in-algorithmic-trading-codex4391.pdf">https://www.cognizant.com/whitepapers/machine-learning-in-algorithmic-trading-codex4391.pdf</a></li>
<li>Zhang, H. (2018). &ldquo;Artificial Intelligence in Finance: Applications and Possibilities.&rdquo; <a href="https://www.cognizant.com/whitepapers/artificial-intelligence-in-finance-applications-and-possibilities-codex4583.pdf">https://www.cognizant.com/whitepapers/artificial-intelligence-in-finance-applications-and-possibilities-codex4583.pdf</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unleashing the Power of AI Transformer: Revolutionizing Artificial Intelligence]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/unleashing-the-power-of-ai-transformer-revolutionizing-artificial-intelligence/"/>
    <updated>2023-05-22T18:57:12+05:30</updated>
    <id>https://rishijeet.github.io/blog/unleashing-the-power-of-ai-transformer-revolutionizing-artificial-intelligence</id>
    <content type="html"><![CDATA[<p>In recent years, the field of artificial intelligence (AI) has witnessed a groundbreaking advancement with the introduction of the AI Transformer model. Inspired by the Transformer architecture, which gained fame for its effectiveness in natural language processing tasks, the AI Transformer has emerged as a powerful tool that revolutionizes various domains, including language translation, image recognition, and speech synthesis. In this blog, we will explore the capabilities and impact of the AI Transformer model, shedding light on its remarkable contributions to the world of AI.</p>

<a name="Understanding.the.Transformer.Architecture"></a>
<h5>Understanding the Transformer Architecture</h5>

<p>The Transformer architecture, initially introduced for machine translation tasks, reshaped the landscape of AI. Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), the Transformer model leverages a self-attention mechanism, enabling it to capture global dependencies in the input data efficiently. This architecture eliminates the need for sequential processing and allows for parallelization, resulting in faster and more accurate predictions.</p>

<a name="Language.Translation.Advancements"></a>
<h5>Language Translation Advancements</h5>

<p>One of the key applications of the AI Transformer is language translation. With its ability to handle long-range dependencies and capture contextual information effectively, the AI Transformer has significantly improved the quality of machine translation systems. The model&rsquo;s attention mechanism enables it to attend to relevant parts of the input text, producing more accurate and coherent translations across different languages. This breakthrough has bridged communication gaps and fostered cross-cultural understanding on a global scale.</p>

<a name="Image.Recognition.and.Computer.Vision"></a>
<h5>Image Recognition and Computer Vision</h5>

<p>The impact of the AI Transformer extends beyond natural language processing. In the realm of computer vision, the model has demonstrated remarkable performance in image recognition tasks. By leveraging the self-attention mechanism, the AI Transformer can analyze and interpret complex visual data, leading to more accurate object detection, image segmentation, and scene understanding. This has paved the way for advancements in autonomous vehicles, robotics, medical imaging, and various other industries reliant on computer vision technologies.</p>

<!-- more -->


<a name="Speech.Synthesis.and.Natural.Language.Generation"></a>
<h5>Speech Synthesis and Natural Language Generation</h5>

<p>Another domain where the AI Transformer has left an indelible mark is speech synthesis and natural language generation. By leveraging its ability to learn dependencies and patterns in sequential data, the AI Transformer can generate human-like speech and produce coherent and contextually relevant text. This has found applications in voice assistants, audiobooks, accessibility technologies, and more, enhancing the overall user experience and accessibility of information.</p>

<a name="Challenges.and.Future.Directions"></a>
<h5>Challenges and Future Directions</h5>

<p>While the AI Transformer has achieved remarkable success, there are still challenges to overcome. The model&rsquo;s immense computational requirements and memory constraints can pose difficulties for real-time and resource-limited applications. Researchers are continuously exploring techniques to optimize and compress the AI Transformer, enabling its deployment on edge devices and enhancing its efficiency.</p>

<p>The future of the AI Transformer holds tremendous promise. As advancements continue, we can expect the model to tackle more complex tasks, push the boundaries of AI capabilities, and facilitate breakthroughs in areas such as drug discovery, personalized medicine, recommendation systems, and intelligent virtual assistants.</p>

<a name="Conclusion"></a>
<h5>Conclusion</h5>

<p>The AI Transformer has emerged as a game-changer in the field of artificial intelligence. Its ability to capture long-range dependencies and understand context has revolutionized language translation, image recognition, speech synthesis, and natural language generation. As we delve deeper into the potential of the AI Transformer, we can anticipate transformative advancements across various domains, propelling us toward a future where AI seamlessly integrates into our daily lives.</p>

<p>Through continued research and development, the AI Transformer will undoubtedly contribute to the evolution of AI, driving innovation and enhancing the way we interact with technology. Brace yourself for a future where the power of the AI Transformer shapes the world as we know it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Dynamic Web Applications with React and Firebase]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/building-dynamic-web-applications-with-react-and-firebase/"/>
    <updated>2023-05-22T13:22:00+05:30</updated>
    <id>https://rishijeet.github.io/blog/building-dynamic-web-applications-with-react-and-firebase</id>
    <content type="html"><![CDATA[<p>In recent years, React has emerged as a popular JavaScript library for building user interfaces, while Firebase has become a powerful platform for developing and deploying web applications. When combined, React and Firebase provide developers with a robust toolkit for creating dynamic and real-time web applications. In this blog, we will explore the integration of React and Firebase, highlighting the benefits and demonstrating how they work seamlessly together.</p>

<a name="An.Overview.of.React.and.Firebase"></a>
<h4>An Overview of React and Firebase</h4>

<p>React is a JavaScript library developed by Facebook, designed to build reusable and interactive user interfaces. It follows a component-based architecture that allows developers to create modular and reusable UI elements, making it easier to manage complex web applications.</p>

<p>Firebase, on the other hand, is a comprehensive platform developed by Google for building web and mobile applications. It offers a wide range of services, including real-time database, authentication, hosting, cloud functions, and more. Firebase simplifies the backend infrastructure, enabling developers to focus on the frontend development and user experience.</p>

<a name="Setting.Up.a.React.App.with.Firebase"></a>
<h4>Setting Up a React App with Firebase</h4>

<p>To get started with React and Firebase, you&rsquo;ll need to set up a React project and integrate Firebase into it. This involves creating a new React app using tools like Create React App and configuring Firebase SDKs and authentication methods. Once set up, you can leverage Firebase services within your React components, making API calls, handling user authentication, and storing data.</p>

<!-- more -->


<a name="Real-time.Data.Synchronization.with.Firebase.Realtime.Database"></a>
<h4>Real-time Data Synchronization with Firebase Realtime Database</h4>

<p>Firebase Realtime Database is a NoSQL cloud database that allows you to store and synchronize data in real-time across multiple clients. With React, you can seamlessly integrate Firebase Realtime Database to create real-time collaborative applications, chat applications, live dashboards, and more. React&rsquo;s state management and Firebase&rsquo;s real-time synchronization capabilities work together to provide a smooth and responsive user experience.</p>

<a name="User.Authentication.with.Firebase.Authentication"></a>
<h4>User Authentication with Firebase Authentication</h4>

<p>Firebase Authentication provides an easy-to-use authentication system that allows users to sign up, sign in, and manage their accounts securely. React can leverage Firebase Authentication to implement features like user registration, login, password reset, and social media authentication within your application. React&rsquo;s component-driven approach can help create a seamless and intuitive user interface for handling authentication flows.</p>

<a name="Deploying.React.Apps.with.Firebase.Hosting"></a>
<h4>Deploying React Apps with Firebase Hosting</h4>

<p>Once your React app is ready, Firebase Hosting enables you to deploy your application with ease. It provides a fast and secure hosting solution for your static assets, including HTML, CSS, JavaScript, and other resources. Firebase Hosting also supports features like custom domains, SSL certificates, and automatic deployments, simplifying the process of launching your React app to the web.</p>

<a name="Cloud.Functions.and.Serverless.Computing"></a>
<h4>Cloud Functions and Serverless Computing</h4>

<p>Firebase Cloud Functions allows you to extend the functionality of your React app by running server-side code in a serverless environment. With Cloud Functions, you can perform server-side operations, process data, trigger events, and integrate with other services. React components can interact with Cloud Functions to offload heavy computation or perform backend tasks asynchronously.</p>

<a name="Conclusion"></a>
<h3>Conclusion</h3>

<p>React and Firebase provide a powerful combination for building dynamic and real-time web applications. React&rsquo;s component-driven architecture, along with Firebase&rsquo;s suite of services for data storage, authentication, hosting, and more, enables developers to create modern, responsive, and feature-rich web applications with ease. By leveraging the capabilities of React and Firebase, developers can focus on delivering exceptional user experiences while minimizing the complexity of backend infrastructure development. Whether you&rsquo;re building a small project or a large-scale application, the React-Firebase duo offers a comprehensive toolkit to bring your ideas to life.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Web 3.0 - How does it impacts IOT ?]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/web-3-dot-0-how-does-it-impacts-iot/"/>
    <updated>2023-05-22T13:13:54+05:30</updated>
    <id>https://rishijeet.github.io/blog/web-3-dot-0-how-does-it-impacts-iot</id>
    <content type="html"><![CDATA[<p>Web 3.0 has a significant impact on the Internet of Things (IoT) by enhancing its capabilities, security, and interoperability. Here are some key ways in which Web 3.0 transforms the IoT landscape:</p>

<ul>
<li><p>Decentralized Data Management: Web 3.0 leverages blockchain technology to create decentralized data marketplaces and storage solutions. This allows IoT devices to securely and autonomously store and exchange data without relying on centralized servers or intermediaries. Decentralization increases data privacy and reduces the risk of data breaches.</p></li>
<li><p>Enhanced Security and Privacy: With Web 3.0, IoT devices can leverage blockchain&rsquo;s cryptographic techniques to establish secure communication channels and ensure data integrity. By eliminating single points of failure and relying on consensus mechanisms, Web 3.0 provides a more robust security framework for IoT devices, mitigating risks associated with hacking, tampering, and unauthorized access.</p></li>
<li><p>Interoperability and Standardization: Web 3.0 fosters interoperability among diverse IoT devices and platforms. By utilizing decentralized protocols and standards, Web 3.0 enables seamless communication and data exchange between different IoT devices and ecosystems. This interoperability unlocks new possibilities for cross-domain collaborations, creating a more connected and efficient IoT ecosystem.</p></li>
</ul>


<!-- more -->


<ul>
<li><p>Autonomous Device-to-Device Transactions: Web 3.0, powered by smart contracts, enables autonomous transactions between IoT devices. Devices can negotiate and execute agreements based on predefined conditions, eliminating the need for intermediaries. For example, smart contracts can enable devices to autonomously manage energy consumption, negotiate pricing, or perform self-maintenance tasks, enhancing the efficiency and automation of IoT networks.</p></li>
<li><p>Monetization of IoT Data: Web 3.0 introduces decentralized data marketplaces, where IoT device owners can securely and transparently monetize their data. Through blockchain-based platforms, device-generated data can be tokenized, allowing users to retain ownership and control over their data while selling or sharing it with interested parties. This opens up new revenue streams and incentives for IoT device owners.</p></li>
<li><p>Edge Computing and Reduced Latency: Web 3.0 leverages edge computing to process and analyze IoT data at the network edge, closer to the devices generating the data. This reduces latency and enables real-time decision-making, making IoT applications more responsive and efficient. Edge computing also reduces the reliance on centralized cloud servers, improving scalability and reliability in IoT deployments.</p></li>
<li><p>Trust and Accountability: Web 3.0&rsquo;s transparent and immutable nature, powered by blockchain, enhances trust and accountability in the IoT ecosystem. Device behavior and data transactions can be recorded on the blockchain, creating an auditable and tamper-proof record. This fosters trust among stakeholders and enables better accountability in areas such as supply chain management, autonomous vehicles, and healthcare.</p></li>
</ul>


<p>In conclusion, Web 3.0 revolutionizes the Internet of Things by providing a decentralized, secure, and interoperable framework. It enhances data privacy, enables autonomous transactions, fosters trust, and unlocks new opportunities for monetization and collaboration. Web 3.0&rsquo;s integration with IoT promises a more connected, efficient, and trustworthy future for smart devices and applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Web 3.0 - Decentralizing Internet]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/web-3-dot-0-decentralizing-internet/"/>
    <updated>2022-10-29T22:04:22+05:30</updated>
    <id>https://rishijeet.github.io/blog/web-3-dot-0-decentralizing-internet</id>
    <content type="html"><![CDATA[<p>The Internet has become an integral part of our lives, transforming the way we communicate, work, and access information. From the early days of Web 1.0, where static websites provided basic information, to the dynamic and interactive Web 2.0 era that brought social media, online collaboration, and user-generated content, the Internet has continuously evolved. Now, a new paradigm shift is on the horizon - Web 3.0. In this blog, we will explore the exciting possibilities and potential of Web 3.0, a decentralized and user-centric vision of the future internet.</p>

<ul>
<li>Defining Web 3.0: Decentralization and Interoperability</li>
</ul>


<p>Web 3.0, often referred to as the &ldquo;Decentralized Web,&rdquo; represents a departure from the centralized systems that dominate the current internet landscape. It is built on principles of decentralization, interoperability, and enhanced user control. Unlike Web 2.0, which relies heavily on centralized platforms and intermediaries, Web 3.0 envisions a distributed network where users have greater ownership and control over their data and digital identities.</p>

<ul>
<li>Blockchain Technology: The Backbone of Web 3.0</li>
</ul>


<p>At the core of Web 3.0 lies blockchain technology, the decentralized ledger system that underpins cryptocurrencies like Bitcoin and Ethereum. Blockchain provides a transparent, tamper-proof, and secure way to record and verify digital transactions. With Web 3.0, blockchain expands its scope beyond financial applications, enabling the development of decentralized applications (dApps), smart contracts, and decentralized autonomous organizations (DAOs).</p>

<!-- more -->


<ul>
<li>Decentralized Applications (dApps): Empowering Users</li>
</ul>


<p>Web 3.0 empowers users by placing them at the center of the internet experience. Decentralized applications (dApps) are one of the cornerstones of Web 3.0, offering users greater control, privacy, and ownership of their data. dApps leverage blockchain&rsquo;s decentralized nature to eliminate middlemen, reduce censorship, and enable direct peer-to-peer interactions. These applications span various domains, including finance, healthcare, supply chain management, and social media.</p>

<ul>
<li>Smart Contracts: Trust and Automation</li>
</ul>


<p>Smart contracts, powered by blockchain technology, are self-executing agreements that automatically enforce the terms and conditions encoded within them. Web 3.0 leverages smart contracts to facilitate trust and automation in a wide range of transactions. These contracts eliminate the need for intermediaries, reducing costs, increasing efficiency, and ensuring transparency. Smart contracts have the potential to revolutionize industries such as real estate, insurance, and intellectual property rights.</p>

<ul>
<li>Enhanced Privacy and Security</li>
</ul>


<p>Web 3.0 addresses growing concerns about data privacy and security. By leveraging decentralized systems and cryptographic techniques, Web 3.0 offers enhanced privacy protection. Users have greater control over their personal information, deciding when and how it is shared. Moreover, the decentralized nature of Web 3.0 reduces the risk of data breaches and single points of failure, enhancing overall security and resilience.</p>

<ul>
<li>The Internet of Things (IoT) in Web 3.0</li>
</ul>


<p>Web 3.0 integrates the Internet of Things (IoT) into its fabric, enabling seamless connectivity and communication between devices.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rest API with Go &amp; Gorilla Mux]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/rest-api-with-go-and-gorilla-mux/"/>
    <updated>2021-02-20T23:12:49+05:30</updated>
    <id>https://rishijeet.github.io/blog/rest-api-with-go-and-gorilla-mux</id>
    <content type="html"><![CDATA[<p>Gorilla is a web toolkit for the Go programming language. The gorilla/mux implements a request router and dispatcher for matching incomings requests to the respective handlers.</p>

<p>One of the cool feature it has is that the registered URLs can be built or reversed which helps maintaining the references to resources and nested routes are only tested if the parent route matches. This is useful to define groups of routes that share common conditions like a host, a path prefix or other repeated attributes.</p>

<!-- more -->


<p>The routes can be declared as mentioned below</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='go'><span class='line'><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>    <span class="nx">r</span> <span class="o">:=</span> <span class="nx">mux</span><span class="p">.</span><span class="nx">NewRouter</span><span class="p">()</span>
</span><span class='line'>    <span class="nx">r</span><span class="p">.</span><span class="nx">HandleFunc</span><span class="p">(</span><span class="s">&quot;/&quot;</span><span class="p">,</span> <span class="nx">HomeHandler</span><span class="p">)</span>
</span><span class='line'>    <span class="nx">r</span><span class="p">.</span><span class="nx">HandleFunc</span><span class="p">(</span><span class="s">&quot;/users&quot;</span><span class="p">,</span> <span class="nx">UsersHandler</span><span class="p">)</span>
</span><span class='line'>    <span class="nx">r</span><span class="p">.</span><span class="nx">HandleFunc</span><span class="p">(</span><span class="s">&quot;/blogs&quot;</span><span class="p">,</span> <span class="nx">BlogHandler</span><span class="p">)</span>
</span><span class='line'>    <span class="nx">http</span><span class="p">.</span><span class="nx">Handle</span><span class="p">(</span><span class="s">&quot;/&quot;</span><span class="p">,</span> <span class="nx">r</span><span class="p">)</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The basic http server code</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='go'><span class='line'><span class="c1">// Author: Rishijeet Mishra</span>
</span><span class='line'><span class="kn">package</span> <span class="nx">main</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="p">(</span>
</span><span class='line'>    <span class="s">&quot;net/http&quot;</span>
</span><span class='line'>    <span class="s">&quot;encoding/json&quot;</span>
</span><span class='line'>    <span class="s">&quot;log&quot;</span>
</span><span class='line'>    <span class="s">&quot;github.com/gorilla/mux&quot;</span>
</span><span class='line'><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>    <span class="nx">router</span> <span class="o">:=</span> <span class="nx">mux</span><span class="p">.</span><span class="nx">NewRouter</span><span class="p">()</span>
</span><span class='line'>    <span class="c1">// Routes consist of a path and a handler function.</span>
</span><span class='line'>    <span class="nx">router</span><span class="p">.</span><span class="nx">HandleFunc</span><span class="p">(</span><span class="s">&quot;/myhandler&quot;</span><span class="p">,</span> <span class="nx">MyHandler</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Bind to a port and pass our router in</span>
</span><span class='line'>    <span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">http</span><span class="p">.</span><span class="nx">ListenAndServe</span><span class="p">(</span><span class="s">&quot;:5000&quot;</span><span class="p">,</span> <span class="nx">router</span><span class="p">))</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">func</span> <span class="nx">MyHandler</span><span class="p">(</span><span class="nx">w</span> <span class="nx">http</span><span class="p">.</span><span class="nx">ResponseWriter</span><span class="p">,</span> <span class="nx">r</span> <span class="o">*</span><span class="nx">http</span><span class="p">.</span><span class="nx">Request</span><span class="p">){</span>
</span><span class='line'>    <span class="c1">//w.Write([]byte(&quot;The page to be rendered&quot;))</span>
</span><span class='line'>    <span class="nx">w</span><span class="p">.</span><span class="nx">Header</span><span class="p">().</span><span class="nx">Set</span><span class="p">(</span><span class="s">&quot;Content-Type&quot;</span><span class="p">,</span> <span class="s">&quot;application/json&quot;</span><span class="p">)</span>
</span><span class='line'>    <span class="nx">json</span><span class="p">.</span><span class="nx">NewEncoder</span><span class="p">(</span><span class="nx">w</span><span class="p">).</span><span class="nx">Encode</span><span class="p">(</span><span class="kd">struct</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="nx">key</span> <span class="kt">string</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="s">&quot;value&quot;</span>
</span><span class='line'>    <span class="p">}</span> <span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s add the POST method</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='go'><span class='line'><span class="c1">// Add the route</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="nx">r</span><span class="p">.</span><span class="nx">HandleFunc</span><span class="p">(</span><span class="s">&quot;/add/{item}&quot;</span><span class="p">,</span> <span class="nx">addItem</span><span class="p">).</span><span class="nx">Methods</span><span class="p">(</span><span class="s">&quot;POST&quot;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">//Add the func</span>
</span><span class='line'>
</span><span class='line'><span class="kd">func</span> <span class="nx">addItem</span><span class="p">(</span><span class="nx">w</span> <span class="nx">http</span><span class="p">.</span><span class="nx">ResponseWriter</span><span class="p">,</span> <span class="nx">r</span> <span class="o">*</span><span class="nx">http</span><span class="p">.</span><span class="nx">Request</span><span class="p">){</span>
</span><span class='line'>    <span class="nx">itemholder</span>  <span class="o">:=</span> <span class="nx">mux</span><span class="p">.</span><span class="nx">Vars</span><span class="p">(</span><span class="nx">r</span><span class="p">)[</span><span class="err">&#39;</span><span class="nx">item</span><span class="err">&#39;</span><span class="p">]</span>
</span><span class='line'>    <span class="nx">data</span> <span class="p">=</span> <span class="nb">append</span><span class="p">(</span> <span class="nx">data</span><span class="p">,</span> <span class="nx">itemholder</span><span class="p">)</span>
</span><span class='line'>    <span class="nx">json</span><span class="p">.</span><span class="nx">NewEncoder</span><span class="p">(</span><span class="nx">w</span><span class="p">).</span><span class="nx">Encode</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s add the GET</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='go'><span class='line'><span class="c1">// The struct for grouping</span>
</span><span class='line'><span class="kd">type</span> <span class="nx">Item</span> <span class="kd">struct</span> <span class="p">{</span>
</span><span class='line'>    <span class="nx">Data</span> <span class="kt">string</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">var</span> <span class="nx">data</span> <span class="p">[]</span><span class="nx">Item</span> <span class="p">=</span> <span class="p">[]</span><span class="nx">Item</span><span class="p">{}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="kd">func</span> <span class="nx">addItem</span><span class="p">(</span><span class="nx">w</span> <span class="nx">http</span><span class="p">.</span><span class="nx">ResponseWriter</span><span class="p">,</span> <span class="nx">r</span> <span class="o">*</span><span class="nx">http</span><span class="p">.</span><span class="nx">Request</span><span class="p">){</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">var</span> <span class="nx">latestItem</span> <span class="nx">Item</span>
</span><span class='line'>    <span class="nx">json</span><span class="p">.</span><span class="nx">NewDecoder</span><span class="p">(</span><span class="nx">r</span><span class="p">.</span><span class="nx">Body</span><span class="p">).</span><span class="nx">Decode</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">latestItem</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="nx">data</span> <span class="p">=</span> <span class="nb">append</span><span class="p">(</span> <span class="nx">data</span><span class="p">,</span> <span class="nx">latestItem</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// output the json</span>
</span><span class='line'>    <span class="nx">w</span><span class="p">.</span><span class="nx">Header</span><span class="p">().</span><span class="nx">Set</span><span class="p">(</span><span class="s">&quot;Content-Type&quot;</span><span class="p">,</span> <span class="s">&quot;application/json&quot;</span><span class="p">)</span>
</span><span class='line'>    <span class="nx">json</span><span class="p">.</span><span class="nx">NewEncoder</span><span class="p">(</span><span class="nx">w</span><span class="p">).</span><span class="nx">Encode</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Mux supports the addition of middlewares to a Router, which are executed in the order they are added if a match is found, including its subrouters. Middlewares are (typically) small pieces of code which take one request, do something with it, and pass it down to another middleware or the final handler.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='batch'><span class='line'>type MiddlewareFunc func(http.Handler) http.Handler
</span><span class='line'>
</span><span class='line'><span class="n">//Add</span> the middleware to the route
</span><span class='line'>r :<span class="o">=</span> mux.NewRouter()
</span><span class='line'>r.HandleFunc(<span class="s2">&quot;/&quot;</span><span class="p">,</span> handler)
</span><span class='line'>r.Use(loggingMiddleware)
</span></code></pre></td></tr></table></div></figure>


<p>Click for more details on the <a href="https://github.com/gorilla/mux">gorilla/mux</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tuning Apache Kafka’s performance]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/tuning-apache-kafkas-performance/"/>
    <updated>2019-07-11T11:42:32+05:30</updated>
    <id>https://rishijeet.github.io/blog/tuning-apache-kafkas-performance</id>
    <content type="html"><![CDATA[<p>Well, Apache Kafka is one of the best pub-sub messaging system used widely across several technology’s based industries. Originated at LinkedIn and was open sourced in early 2011.</p>

<p>Ok, so what so special about <strong>Apache Kafka</strong> ? Here are the few things Kafka is meant to handle.</p>

<ul>
<li>High throughput to support large volume event feeds.</li>
<li>Real time processing of enormous amount of data.</li>
<li>Support large data backlogs to handle periodic ingestion from offline systems.</li>
<li>Support low - latency delivery of the messages compared to other messaging systems</li>
<li>High Availability, Fault Tolerance.</li>
</ul>


<p>So what else are you looking ?</p>

<!--more-->


<p>Now if you know about Apache Kafka a bit, here are few things we can fine tune to make it better in terms of performance. Let’s categories the system into the following aspects and see what could be done in each space.</p>

<ul>
<li>Producers</li>
<li>Brokers</li>
<li>Consumers</li>
</ul>


<a name="Producers"></a>
<h2>Producers</h2>

<a name="Asynchronous"></a>
<h3>Asynchronous</h3>

<p>Now think, how long you want to wait for the ack on the message sent to the broker ? Answer to this question will change the speed of handling the messages in the Kafka.</p>

<p>request.required.acks is the property of the producer.</p>

<p>Possible values for this are:</p>

<ul>
<li><code>0</code> = producer never waits for the ack from the broker. This will give you “Least durability and least latency”.</li>
<li><code>1</code> = producer gets ack from the master replica. This will give you “some durability and less latency”.</li>
<li><code>-1</code> = producer gets ack from the all the replicas. This will give you “most durability and most latency”.</li>
</ul>


<a name="Batching"></a>
<h3>Batching</h3>

<p>How about batching the messages ? Let’s use the asynchronous producers.</p>

<p><code>producer.type=1</code> to make the producers run async.</p>

<p>You can get the “callback” for the messages here to know their status. Now batch your messages to the brokers in different threads, this will improve the throughput. Some configuration to handle the messages in this scenario are:</p>

<ul>
<li><code>queue.buffer.max.ms</code> - Duration of the batch window.</li>
<li><code>batch.num.messages</code> - Number of messages to be sent in a batch.</li>
</ul>


<a name="Compression"></a>
<h3>Compression</h3>

<p>Use the compression property to reduces the I/O on the machine. We might also want to think of the CPU load when it decompresses the message object back. So, maintain a balance between the two. compression.codec - Values are none, gzip and snappy</p>

<p>For presumably large messages say - 10G , you might want to pass the file location of the share drive in the maessage rather than the payload itself. This would be tremendously faster.</p>

<a name="Timeout"></a>
<h3>Timeout</h3>

<p>Don’t wait for the message unnecessarily unless its is really really required. Have a “timeout”</p>

<p><code>request.timeout.ms</code> - The time until the broker waits before sending error back to the client.
Amount of time to block before dropping the messages when running in async mode ( default = indefinitely )</p>

<a name="Brokers"></a>
<h2>Brokers</h2>

<a name="Partition"></a>
<h3>Partition</h3>

<p>Plan to have as number of partitions = number of consumers. This will increase the concurrency, the more the partitions the more the concurrency. Remember, more the partitions more the latency too. Also, recommended to have one partition per physical disk to ensure I/O is not the bottleneck while writing the logs.</p>

<p>Use “kafka-reassign-partitions.sh” to ensure partition is not overloaded.</p>

<p>Some of the configurations worth mentioning here are:</p>

<ul>
<li><code>num.io.threads</code> - The number of I/O threads server uses to execute the requests.</li>
<li><code>num.partitions</code> - Number of partitions per topic</li>
<li><code>log.flush.interval,messages</code> - The number of messages written to the log partition before we force an fsync on the log.</li>
</ul>


<a name="Consumers"></a>
<h2>Consumers</h2>

<p>The max number of consumers for the topic is equal to number of partitions. Have enough partitions to handle all the consumers in your Kafka’s ecosystem.</p>

<p>Consumer in the same consumer group split the partitions among themselves. Adding more consumers to a group can enhance performance.</p>

<p>Performance is not affected by adding more consumer groups</p>

<p><code>replica.high.watermark.checkpoint.interval.ms</code> can affect the throughput. When reading from partition, you can mark the last point where you read the information. If you set checkpoint watermark for every event, you will have high durability but hit on the performance. Rather, set it to check the offset for every x number of messages wherein you have margin of safety and will less impact your throughput.</p>

<a name="Timeout"></a>
<h3>Timeout</h3>

<p>Choose the timeouts and onward pipeline properly. Also, refer to Apache Kafka doc for setting fetch size, time, auto-commit etc.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Blockchain - Really worth in elections ?]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/blockchain-really-worth-in-elections/"/>
    <updated>2017-05-12T10:37:14+05:30</updated>
    <id>https://rishijeet.github.io/blog/blockchain-really-worth-in-elections</id>
    <content type="html"><![CDATA[<p>Issues with <strong>EVM ( Electronic Voting Machine )</strong> have been the talk of the town for quite a while in India nowadays. Political parties have been taking about going back to voting mechanism using ballot boxes, wherein there would be very less possibility of non-legitimate voting. EVMs are prone to hacking, untrusted votes, digital errors as explained by these political parties.</p>

<p>I ask you political parties this, why ballot boxes or directly &ldquo;why papers&rdquo; ? Why cut trees to achieve your goal considering the fact that, we are living in the - digital era with brilliant minds across the country to make things possible using technology.</p>

<p>The recent government (people too) has been keen towards its programme of Digital India.</p>

<!--more-->


<p>As the website says -</p>

<p><em>The Digital India programme is a flagship programme of the Government of India with a vision to transform India into a digitally empowered society and knowledge economy.</em></p>

<p>So why paper for casting votes ?</p>

<p>The question is - Can the blockchain technology solve this problem ?</p>

<p>The answer to the question is - Yes</p>

<p>Adopting to the Blockchain technology will not only ensure, that the vote casted is trusted but will also eradicate the underlying issues of hacking, third party dependency, voting transactions, fraud &amp; other securities hacks.</p>

<p>Importantly it will reduce the cost of elections drastically and the government could utilise its funds in other major projects. Note, the Loksabha election 2014 costed around <strong>3,500 crore</strong>.</p>

<p><img src="https://rishijeet.github.io/images/myimages/voting_places.png" alt="Alt text" /></p>

<p>Simple mechanism to understand this from blockchain perspective.</p>

<ul>
<li>Voting could be casted from any place on a particular day. This could be translated as <strong>NODES</strong> in blockchain.</li>
<li>Voting done at nodes could be validated by a elected nodes or electoral offices. This translates to <strong>MINERS</strong> in blockchain.</li>
<li>Elected nodes or electoral offices would update their transactions. This translates to <strong>SYNCHRONISATION</strong> ledger in blockchain.</li>
<li>Transactions are up to date at any electoral offices and results could be declared instantly on the same day if required.</li>
</ul>


<p>Government should just ensure the infrastructure is developed for this and is prioritise.</p>

<p>Let&rsquo;s all participate in making India a - <strong>Digital India</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fancybox2 to Fancybox3 for image gallery]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/fancybox2-to-fancybox3-for-image-gallery/"/>
    <updated>2017-05-12T09:58:37+05:30</updated>
    <id>https://rishijeet.github.io/blog/fancybox2-to-fancybox3-for-image-gallery</id>
    <content type="html"><![CDATA[<p>I recently migrated my image gallery to Fancybox3 from Fancybox2. Fancybox3 has some of the advantages over its previous version.
The important one I like is the fact that you have to code less.</p>

<ul>
<li>Import the javascript</li>
<li>Wrap the images with class and the work is done.</li>
<li>It supports the image security too. <i class="em em---1"></i>
<!-- more --></li>
</ul>


<p>Personally, I like the masonry style more and working on moving towards its soon. Still exploring the feasibility.
The details on the masonry grid could be found <a href="https://masonry.desandro.com/">here</a>.</p>

<p>Did you notice the masonry grid used in <a href="http://iam.beyonce.com/tagged/my_work">Beyonce&rsquo;s Website</a> ? <i class="em em-blush"></i></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributed Computing - Quorum]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/distributed-computing/"/>
    <updated>2016-09-04T18:40:02+05:30</updated>
    <id>https://rishijeet.github.io/blog/distributed-computing</id>
    <content type="html"><![CDATA[<p>In a distributed database system, a transaction could be executing its operations at multiple sites. Since atomicity requires every distributed transaction to be atomic, the transaction must have the same fate (commit or abort) at every site. In case of network partitioning, sites are partitioned and the partitions may not be able to communicate with each other. This is where a quorum-based technique comes in. The fundamental idea is that a transaction is executed if the majority of sites vote to execute it.</p>

<a name="Quorum.Consensus.Protocol"></a>
<h4>Quorum Consensus Protocol</h4>

<p>This is one of the distributed lock manager based concurrency control protocol in distributed database systems. It works as follows;</p>

<ul>
<li>The protocol assigns each site that have a replica with a weight.</li>
<li>For any data item, the protocol assigns a read quorum Qr and write quorum Qw. Here, Qr and Qw are two integers (sum of weights of some sites). And, these two integers are chosen according to the following conditions put together;</li>
</ul>


<p><code>Qr + Qw &gt; S - rule which avoids read-write conflict. (i.e, two transactions cannot read and write concurrently)</code></p>

<p><code>2 * Qw &gt; S - rule which avoids write-write conflict. (i.e, two transactions cannot write concurrently)</code></p>

<p>Here, S is the total weight of all sites in which the data item replicated.</p>

<!-- more -->


<a name="How.do.we.perform.read.and.write.on.replicas."></a>
<h4>How do we perform read and write on replicas?</h4>

<ul>
<li>A transaction that needs a data item for reading purpose has to lock enough sites. ie, it has lock sites with the sum of their weight >= Qr. Read quorum must always intersect with write quorum.</li>
<li>A transaction that needs a data item for writing purpose has to lock enough sites. ie, it has lock sites with the sum of their weight >= Qw.</li>
</ul>


<a name="How.does.it.work."></a>
<h4>How does it work?</h4>

<p>Let us assume a fully replicated distributed database with four sites S1, S2, S3, and S4.</p>

<ol>
<li><p>According to the protocol, we need to assign a weight to every site. (This weight can be chosen on many factors like the availability of the site, latency etc.). For simplicity, let us assume the weight as 1 for all sites.</p></li>
<li><p>Let us choose the values for Qr and Qw as 2 and 3. Our total weight S is 4. And according to the conditions, our Qr and Qw values are correct;
<br>
<code>Qr + Qw &gt; S =&gt; 2 + 3 &gt; 4               True</code>
<br>
<code>2 * Qw  &gt; S =&gt; 2 * 3 &gt; 4               True</code>
<br></p></li>
<li>Now, a transaction which needs a read lock on a data item has to lock 2 sites. A transaction which needs a write lock on data item has to lock 3 sites.</li>
</ol>


<a name="Case.1.code..sup1...code."></a>
<h6>Case 1<code>&sup1;</code></h6>

<p>Read Quorum Qr = 2, Write Quorum Qw = 3, Site’s weight = 1, Total weight of sites S = 4</p>

<ul>
<li>Read Lock

<ol>
<li>Read request has to lock at least two replicas (2 sites in our example)</li>
<li>Any two sites can be locked</li>
</ol>
</li>
<li>Write Lock

<ol>
<li>Write request has to lock at least three replicas (3 sites in our example)</li>
</ol>
</li>
</ul>


<a name="Case.2"></a>
<h6>Case 2</h6>

<p>Read Quorum Qr = 1, Write Quorum Qw = 4, Site’s weight = 1, Total weight of sites S = 4</p>

<ul>
<li>Read Lock

<ol>
<li>Read lock requires one site</li>
</ol>
</li>
<li>Write Lock

<ol>
<li>Write lock requires 4 sites</li>
</ol>
</li>
</ul>


<p><em><code>&sup1;</code>Points and example taken from web.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Is Microsoft biased towards Linux]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/is-microsoft-biased-towards-linux/"/>
    <updated>2016-03-11T09:45:09+05:30</updated>
    <id>https://rishijeet.github.io/blog/is-microsoft-biased-towards-linux</id>
    <content type="html"><![CDATA[<p>I see news about Microsoft crafted a switch OS on Debian Linux platform,  announcing its SQL server of Linux and may be some more. Though this is a surprising news as Microsoft windows has released its best OS so far, which is more stable and fast compare to its ancestors, so what could be the reason to move towards Linux is a question. Is this related to the security aspect of the operating system or is it the open source nature of the platform and the community support it has or is it just the recent decision to move towards it ? Whatever it is, I see this as a strong move towards making its environment and platform more robust and performant.</p>

<!-- more -->


<p>According to the Microsoft&rsquo;s Blog</p>

<p><i>
These improvements, and many more, are all built into SQL Server and bring you not just a new database but a complete platform for data management, business analytics and intelligent apps – one that can be used in a consistent way across both on-premises and the cloud. In fact, over the last year we’ve been using the SQL Server 2016 code-base to run in production more than 1.4 million SQL Databases in the cloud using our Azure SQL Database as a Service offering, and this real-world experience has made SQL Server 2016 an incredibly robust and battle-hardened data platform.
</i></p>


<p>
As the executive tells the New York Times, this is all about &#8220;market expansion.&#8221; Microsoft would rather corner the server software space, which has been shifting toward Linux, than insist on a Windows-only policy out of stubborn pride. It&#8217;s tough to know if the Linux server crowd will warm up to its longtime arch-rival, but those more open-minded firms are now free to integrate Microsoft without making a wholesale switch.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memory Management in Python]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/memory-management-in-python/"/>
    <updated>2015-04-22T09:54:01+05:30</updated>
    <id>https://rishijeet.github.io/blog/memory-management-in-python</id>
    <content type="html"><![CDATA[<p>I came across the interesting write up somewhere on website on memory management in Python. Here are some data facts
which I liked,</p>

<p>Python allocates memory transparently, manages objects using a reference count system, and frees memory when an object’s reference count falls to zero. In theory, it’s swell. In practice, you need to know a few things about Python memory management to get a memory-efficient program running. One of the things you should know, or at least get a good feel about, is the sizes of basic Python objects. Another thing is how Python manages its memory internally.</p>

<p>So let us begin with the size of basic objects. In Python, there’s not a lot of primitive data types: there are ints, longs (an unlimited precision version of ints), floats (which are doubles), tuples, strings, lists, dictionaries, and classes.</p>

<!-- more -->


<p>What is the size of int? A programmer with a C or C++ background will probably guess that the size of a machine-specific int is something like 32 bits, maybe 64; and that therefore it occupies at most 8 bytes. But is that so in Python?</p>

<p>Let us first write a function that shows the sizes of objects (recursively if necessary):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">sys</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">show_sizeof</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">print</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">&quot;</span> <span class="o">*</span> <span class="n">level</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">__class__</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s">&#39;__iter__&#39;</span><span class="p">):</span>
</span><span class='line'>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s">&#39;items&#39;</span><span class="p">):</span>
</span><span class='line'>            <span class="k">for</span> <span class="n">xx</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class='line'>                <span class="n">show_sizeof</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">level</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span class='line'>        <span class="k">else</span><span class="p">:</span>
</span><span class='line'>            <span class="k">for</span> <span class="n">xx</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
</span><span class='line'>                <span class="n">show_sizeof</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">level</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>We can now use the function to inspect the sizes of the different basic data types:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">63</span><span class="p">)</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="mi">102947298469128649161972364837164</span><span class="p">)</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="mi">918659326943756134897561304875610348756384756193485761304875613948576297485698417</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#If you have a 32-bit 2.7x Python, you’ll see:</span>
</span><span class='line'>
</span><span class='line'><span class="mi">8</span> <span class="bp">None</span>
</span><span class='line'><span class="mi">12</span> <span class="mi">3</span>
</span><span class='line'><span class="mi">22</span> <span class="mi">9223372036854775808</span>
</span><span class='line'><span class="mi">28</span> <span class="mi">102947298469128649161972364837164</span>
</span><span class='line'><span class="mi">48</span> <span class="mi">918659326943756134897561304875610348756384756193485761304875613948576297485698417</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c">#and if you have a 64-bit 2.7x Python, you’ll see:</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="mi">16</span> <span class="bp">None</span>
</span><span class='line'><span class="mi">24</span> <span class="mi">3</span>
</span><span class='line'><span class="mi">36</span> <span class="mi">9223372036854775808</span>
</span><span class='line'><span class="mi">40</span> <span class="mi">102947298469128649161972364837164</span>
</span><span class='line'><span class="mi">60</span> <span class="mi">918659326943756134897561304875610348756384756193485761304875613948576297485698417</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let us focus on the 64-bit version (mainly because that’s what we need the most often in our case). None takes 16 bytes. int takes 24 bytes, three times as much memory as a C int64_t, despite being some kind of “machine-friendly” integer. Long integers (unbounded precision), used to represent integers larger than 263-1, have a minimum size of 36 bytes. Then it grows linearly in the logarithm of the integer represented.</p>

<p>Python’s floats are implementation-specific but seem to be C doubles. However, they do not eat up only 8 bytes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="mf">3.14159265358979323846264338327950288</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Output</span>
</span><span class='line'>
</span><span class='line'><span class="mi">16</span> <span class="mf">3.14159265359</span>
</span><span class='line'>
</span><span class='line'><span class="c">#on a 32-bit platform and</span>
</span><span class='line'>
</span><span class='line'><span class="mi">24</span> <span class="mf">3.14159265359</span>
</span><span class='line'><span class="c">#on a 64-bit platform.</span>
</span></code></pre></td></tr></table></div></figure>


<p>That’s again, three times the size a C programmer would expect. Now, what about strings?</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">(</span><span class="s">&quot;My hovercraft is full of eels&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>outputs, on a 32 bit platform:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="mi">21</span>
</span><span class='line'><span class="mi">50</span> <span class="n">My</span> <span class="n">hovercraft</span> <span class="ow">is</span> <span class="n">full</span> <span class="n">of</span> <span class="n">eels</span>
</span></code></pre></td></tr></table></div></figure>


<p>and</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="mi">37</span>
</span><span class='line'><span class="mi">66</span> <span class="n">My</span> <span class="n">hovercraft</span> <span class="ow">is</span> <span class="n">full</span> <span class="n">of</span> <span class="n">eels</span>
</span></code></pre></td></tr></table></div></figure>


<p>An empty string costs 37 bytes in a 64-bit environment! Memory used by string then linearly grows in the length of the (useful) string.</p>

<p>Other structures commonly used, tuples, lists, and dictionaries are worthwhile to examine. Lists (which are implemented as array lists, not as linked lists, with everything it entails) are arrays of references to Python objects, allowing them to be heterogeneous. Let us look at our sizes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">show_sizeof</span><span class="p">([])</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="s">&quot;toaster&quot;</span><span class="p">,</span> <span class="mf">230.1</span><span class="p">])</span>
</span><span class='line'><span class="c">#outputs</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="mi">32</span> <span class="p">[]</span>
</span><span class='line'><span class="mi">44</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="s">&#39;toaster&#39;</span><span class="p">,</span> <span class="mf">230.1</span><span class="p">]</span>
</span><span class='line'><span class="c">#on a 32-bit platform and</span>
</span><span class='line'>
</span><span class='line'><span class="mi">72</span> <span class="p">[]</span>
</span><span class='line'><span class="mi">96</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="s">&#39;toaster&#39;</span><span class="p">,</span> <span class="mf">230.1</span><span class="p">]</span>
</span><span class='line'><span class="c">#on a 64-bit platform. </span>
</span></code></pre></td></tr></table></div></figure>


<p>An empty list eats up 72 bytes. The size of an empty, 64-bit C++ std::list() is only 16 bytes,
 4-5 times less. What about tuples? (and dictionaries?):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">show_sizeof</span><span class="p">({})</span>
</span><span class='line'><span class="n">show_sizeof</span><span class="p">({</span><span class="s">&#39;a&#39;</span><span class="p">:</span><span class="mi">213</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">:</span><span class="mi">2131</span><span class="p">})</span>
</span></code></pre></td></tr></table></div></figure>


<p>outputs, on a 32-bit box</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="mi">136</span> <span class="p">{}</span>
</span><span class='line'> <span class="mi">136</span> <span class="p">{</span><span class="s">&#39;a&#39;</span><span class="p">:</span> <span class="mi">213</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">:</span> <span class="mi">2131</span><span class="p">}</span>
</span><span class='line'>        <span class="mi">32</span> <span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">213</span><span class="p">)</span>
</span><span class='line'>                <span class="mi">22</span> <span class="n">a</span>
</span><span class='line'>                <span class="mi">12</span> <span class="mi">213</span>
</span><span class='line'>        <span class="mi">32</span> <span class="p">(</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="mi">2131</span><span class="p">)</span>
</span><span class='line'>                <span class="mi">22</span> <span class="n">b</span>
</span><span class='line'>                <span class="mi">12</span> <span class="mi">2131</span>
</span><span class='line'><span class="ow">and</span>
</span><span class='line'>
</span><span class='line'><span class="mi">280</span> <span class="p">{}</span>
</span><span class='line'> <span class="mi">280</span> <span class="p">{</span><span class="s">&#39;a&#39;</span><span class="p">:</span> <span class="mi">213</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">:</span> <span class="mi">2131</span><span class="p">}</span>
</span><span class='line'>        <span class="mi">72</span> <span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">213</span><span class="p">)</span>
</span><span class='line'>                <span class="mi">38</span> <span class="n">a</span>
</span><span class='line'>                <span class="mi">24</span> <span class="mi">213</span>
</span><span class='line'>        <span class="mi">72</span> <span class="p">(</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="mi">2131</span><span class="p">)</span>
</span><span class='line'>                <span class="mi">38</span> <span class="n">b</span>
</span><span class='line'>                <span class="mi">24</span> <span class="mi">2131</span>
</span><span class='line'>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>for a 64-bit box.</p>

<p>This last example is particularly interesting because it “doesn’t add up.” If we look at individual key/value pairs, they take 72 bytes (while their components take 38+24=62 bytes, leaving 10 bytes for the pair itself), but the dictionary takes 280 bytes (rather than a strict minimum of 144=72×2 bytes). The dictionary is supposed to be an efficient data structure for search and the two likely implementations will use more space that strictly necessary. If it’s some kind of tree, then we should pay the cost of internal nodes that contain a key and two pointers to children nodes; if it’s a hash table, then we must have some room with free entries to ensure good performance.</p>

<p>The (somewhat) equivalent <code>std::map</code> C++ structure takes 48 bytes when created (that is,
empty). An empty C++ string takes 8 bytes (then allocated size grows linearly the size of the string). An integer takes 4 bytes (32 bits).</p>

<p>Why does all this matter? It seems that whether an empty string takes 8 bytes or 37 doesn’t change anything much
. That’s true. That’s true until you need to scale. Then, you need to be really careful about how many objects you
create to limit the quantity of memory your program uses. It is a problem in real-life applications. However, to devise a really good strategy about memory management, we must not only consider the sizes of objects, but how many and in which order they are created. It turns out to be very important for Python programs. One key element to understand is how Python allocates its memory internally, which we will discuss next.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware Player and Hyper-V are not compatible]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/vmware-player-and-hyper-v-are-not-compatible/"/>
    <updated>2015-04-12T11:28:34+05:30</updated>
    <id>https://rishijeet.github.io/blog/vmware-player-and-hyper-v-are-not-compatible</id>
    <content type="html"><![CDATA[<p>I run my VMs using vmware player for multiple operating system like Ubuntu, CentOS, Fedora, Suse,
Mint Linux. One fine day I noticed this error &ldquo;VMware Player and Hyper-V are not compatible&rdquo; from the vmplayer while
starting Ubuntu. This was bit surprising for me as I had run the same vm couple of times.</p>

<p>I realized that disabling the hyper-V could fix this problem, but I was still curious,
as of why could this start all of sudden?</p>

<!-- more -->


<p>The fix was simple as I said, to disable hyper-V</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='batch'><span class='line'>bcdedit <span class="n">/set</span> hypervisorlaunchtype <span class="k">off</span>
</span></code></pre></td></tr></table></div></figure>


<p>Rebooting the window&rsquo;s machine after running above command from command prompt with admin privilege fixes the problem.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jekyll Simply So Simple]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/jekyll-simply-so-simple/"/>
    <updated>2014-09-15T22:36:38+05:30</updated>
    <id>https://rishijeet.github.io/blog/jekyll-simply-so-simple</id>
    <content type="html"><![CDATA[<p><img src="https://rishijeet.github.io/images/jekyll.png" height="100" width="100" alt="Alt text" /></p>

<p>Jekyll one of the fast and simple static html page generator is really easy to start of with. I am so addicted to it now. Still exploring it more, there are so many features available and installing it
is as easy as it can be.</p>

<!-- more -->


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="o">~</span> <span class="err">$</span> <span class="n">gem</span> <span class="n">install</span> <span class="n">jekyll</span>
</span><span class='line'><span class="o">~</span> <span class="err">$</span> <span class="n">jekyll</span> <span class="kp">new</span> <span class="n">my</span><span class="o">-</span><span class="n">awesome</span><span class="o">-</span><span class="n">site</span>
</span><span class='line'><span class="o">~</span> <span class="err">$</span> <span class="n">cd</span> <span class="n">my</span><span class="o">-</span><span class="n">awesome</span><span class="o">-</span><span class="n">site</span>
</span><span class='line'><span class="o">~</span><span class="sr">/my-awesome-site $ jekyll serve</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PyCon India 2014]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/pycon-india-2014/"/>
    <updated>2014-09-15T21:57:01+05:30</updated>
    <id>https://rishijeet.github.io/blog/pycon-india-2014</id>
    <content type="html"><![CDATA[<p><img src="https://rishijeet.github.io/images/pycon.png" height="100" width="100" alt="Alt text" /> Yes, PyCon India 2014 is happening in Bangalore. <a href="http://in.pycon.org/2014/schedule.html#schedule_conference">Interesting topics</a> are on there for you on day 2.
Workshops are also been conducted as a part of day 1 program. I have attended the one in year 2012 and some of the topics were really good to know and worth attending.
The topics which focus on large data processing, performance, high scalable application in python, complex data structure would be worth to attend.</p>

<!-- more -->


<p>So as I see the topics on the PyCon India Website, if not changed, my favorite topic would be,<br/>
<a href="http://in.pycon.org/funnel/2014/108-python-spark-lightning-fast-cluster-computing">Python + Spark: Lightning Fast Cluster Computing</a><br/>
<a href="http://in.pycon.org/funnel/2014/227-django-design-patterns">Django Design Patterns</a><br/>
<a href="http://in.pycon.org/funnel/2014/150-new-scientific-plotting-in-python">New Scientific Plotting in Python</a><br/>
<a href="http://in.pycon.org/funnel/2014/165-faster-data-processing-in-python">Faster data processing in Python</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Convert from epoch to human readable date]]></title>
    <summary><![CDATA[]]></summary>
    <link href="https://rishijeet.github.io/blog/convert-from-epoch-to-human-readable-date/"/>
    <updated>2014-09-12T23:50:56+05:30</updated>
    <id>https://rishijeet.github.io/blog/convert-from-epoch-to-human-readable-date</id>
    <content type="html"><![CDATA[<p>I was stuck with an issue of converting the epoch time to human readable format, in my case
the epoch time was in milli sec, and I was getting all sort of <code>python ValueError: (22, 'Invalid argument')</code></p>

<!-- more -->


<p>The fix was simple, to convert the epoch in milli sec to exact <code>date +%s</code> format
by slicing <code>[:3]</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">time</span>
</span><span class='line'><span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">&quot;%a, </span><span class="si">%d</span><span class="s"> %b %Y %H:%M:%S +0000&quot;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">localtime</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c">#Replace time.localtime with time.gmtime for GMT time.</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
</feed>
